# Ollama Provider Fingerprinting
# Documentation: https://ollama.com/

provider: ollama
display_name: "Ollama"
type: local
base_urls:
  - http://localhost:11434
  - http://127.0.0.1:11434

# === Authentication ===
auth:
  type: none
  note: "Ollama runs locally without authentication by default"

# === Endpoints ===
endpoints:
  # Native Ollama API
  generate:
    path: "/api/generate"
    method: POST
    request_type: completion
    streaming:
      default: true
      indicator:
        body_field: "stream"
        value: true
    request_extraction:
      model: "$.model"
      prompt: "$.prompt"
      system: "$.system"
      template: "$.template"
      context: "$.context"
      stream: "$.stream"
      options:
        temperature: "$.options.temperature"
        num_predict: "$.options.num_predict"
        top_p: "$.options.top_p"
        top_k: "$.options.top_k"
    response_extraction:
      model: "$.model"
      response: "$.response"
      done: "$.done"
      context: "$.context"
      usage:
        prompt_tokens: "$.prompt_eval_count"
        completion_tokens: "$.eval_count"
      timing:
        total_duration: "$.total_duration"
        load_duration: "$.load_duration"
        prompt_eval_duration: "$.prompt_eval_duration"
        eval_duration: "$.eval_duration"
        
  chat:
    path: "/api/chat"
    method: POST
    request_type: chat
    streaming:
      default: true
      indicator:
        body_field: "stream"
        value: true
    request_extraction:
      model: "$.model"
      messages: "$.messages"
      stream: "$.stream"
      tools: "$.tools"
      options:
        temperature: "$.options.temperature"
        num_predict: "$.options.num_predict"
    response_extraction:
      model: "$.model"
      message: "$.message"
      done: "$.done"
      usage:
        prompt_tokens: "$.prompt_eval_count"
        completion_tokens: "$.eval_count"
        
  embeddings:
    path: "/api/embeddings"
    method: POST
    request_type: embedding
    request_extraction:
      model: "$.model"
      prompt: "$.prompt"
    response_extraction:
      embedding: "$.embedding"
      
  embed:
    path: "/api/embed"
    method: POST
    request_type: embedding
    note: "Newer embeddings endpoint"
    request_extraction:
      model: "$.model"
      input: "$.input"
    response_extraction:
      embeddings: "$.embeddings"

  # OpenAI-compatible endpoints
  openai_chat_completions:
    path: "/v1/chat/completions"
    method: POST
    request_type: chat
    note: "OpenAI-compatible endpoint"
    compatible_with: openai
    
  openai_completions:
    path: "/v1/completions"
    method: POST
    request_type: completion
    note: "OpenAI-compatible endpoint"
    compatible_with: openai
    
  openai_embeddings:
    path: "/v1/embeddings"
    method: POST
    request_type: embedding
    note: "OpenAI-compatible endpoint"
    compatible_with: openai

  # Management endpoints (for model discovery)
  list_models:
    path: "/api/tags"
    method: GET
    request_type: management
    note: "List locally available models"
    
  show_model:
    path: "/api/show"
    method: POST
    request_type: management
    note: "Get model details"
    
  pull_model:
    path: "/api/pull"
    method: POST
    request_type: management
    note: "Download a model"

# === Models ===
# Model data is auto-generated from LiteLLM. See:
#   semconv/providers/_generated/models.yaml
#   semconv/providers/_generated/models.json
#
# Note: Ollama models are dynamic - users can pull any model from ollama.com/library
# The generated registry includes common models, but Ollama supports many more.
#
# To update model data, run:
#   python scripts/sync-models.py
#
# Model families for grouping (manually maintained):
model_families:
  llama:
    pattern: "^llama|^codellama"
    display_name: "Llama"
    
  mistral:
    pattern: "^mistral|^mixtral"
    display_name: "Mistral"
    
  qwen:
    pattern: "^qwen"
    display_name: "Qwen"
    
  phi:
    pattern: "^phi"
    display_name: "Phi"
    
  gemma:
    pattern: "^gemma"
    display_name: "Gemma"
    
  deepseek:
    pattern: "^deepseek"
    display_name: "DeepSeek"
    
  embedding:
    pattern: "embed|minilm"
    display_name: "Embeddings"

# === Fingerprinting Signals ===
fingerprints:
  high_confidence:
    - port: 11434
      path_prefix: "/api/"
      
    - port: 11434
      path_prefix: "/v1/"
      note: "OpenAI-compatible mode"
      
  medium_confidence:
    - domain_pattern: "localhost:11434"
    - domain_pattern: "127.0.0.1:11434"
    
  response_signals:
    # Native API signals
    - body_field: "$.done"
      type: boolean
    - body_field: "$.eval_count"
      type: integer
    - body_field: "$.prompt_eval_count"
      type: integer
    - body_field: "$.total_duration"
      type: integer
      
    # Model name patterns
    - body_field: "$.model"
      patterns:
        - "^llama"
        - "^mistral"
        - "^codellama"
        - "^phi"
        - "^gemma"
        - "^qwen"

# === Detection Notes ===
notes:
  - "Ollama primarily runs on localhost:11434"
  - "Can also be exposed on network via OLLAMA_HOST env var"
  - "Supports both native API and OpenAI-compatible endpoints"
  - "Models are identified by name, may include version tags (e.g., llama3:70b)"
  - "Response includes detailed timing information not available from cloud providers"

