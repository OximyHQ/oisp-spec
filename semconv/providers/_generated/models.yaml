# OISP Model Registry
# Auto-generated from LiteLLM - DO NOT EDIT MANUALLY
# Source: https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json
# Generated: 2025-12-28T02:28:12.589608+00:00
#
# To regenerate: python scripts/sync-models.py

version: '0.1'
generated_at: '2025-12-28T02:28:12.589625+00:00'
source: litellm
source_url: 'https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json'

providers:
  ai21:
    model_count: 12
    models:
      'j2-light':
        mode: completion
        max_input_tokens: 8192
        max_output_tokens: 8192
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.003
      'j2-mid':
        mode: completion
        max_input_tokens: 8192
        max_output_tokens: 8192
        input_cost_per_1k: 0.01
        output_cost_per_1k: 0.01
      'j2-ultra':
        mode: completion
        max_input_tokens: 8192
        max_output_tokens: 8192
        input_cost_per_1k: 0.015
        output_cost_per_1k: 0.015
      'jamba-1.5':
        mode: chat
        max_input_tokens: 256000
        max_output_tokens: 256000
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0004
      'jamba-1.5-large':
        mode: chat
        max_input_tokens: 256000
        max_output_tokens: 256000
        input_cost_per_1k: 0.002
        output_cost_per_1k: 0.008
      'jamba-1.5-large@001':
        mode: chat
        max_input_tokens: 256000
        max_output_tokens: 256000
        input_cost_per_1k: 0.002
        output_cost_per_1k: 0.008
      'jamba-1.5-mini':
        mode: chat
        max_input_tokens: 256000
        max_output_tokens: 256000
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0004
      'jamba-1.5-mini@001':
        mode: chat
        max_input_tokens: 256000
        max_output_tokens: 256000
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0004
      'jamba-large-1.6':
        mode: chat
        max_input_tokens: 256000
        max_output_tokens: 256000
        input_cost_per_1k: 0.002
        output_cost_per_1k: 0.008
      'jamba-large-1.7':
        mode: chat
        max_input_tokens: 256000
        max_output_tokens: 256000
        input_cost_per_1k: 0.002
        output_cost_per_1k: 0.008
      'jamba-mini-1.6':
        mode: chat
        max_input_tokens: 256000
        max_output_tokens: 256000
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0004
      'jamba-mini-1.7':
        mode: chat
        max_input_tokens: 256000
        max_output_tokens: 256000
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0004
  aiml:
    model_count: 12
    models:
      'dall-e-2':
        litellm_id: 'aiml/dall-e-2'
        mode: image
      'dall-e-3':
        litellm_id: 'aiml/dall-e-3'
        mode: image
      'flux-pro':
        litellm_id: 'aiml/flux-pro'
        mode: image
      'flux-pro/v1.1':
        litellm_id: 'aiml/flux-pro/v1.1'
        mode: image
      'flux-pro/v1.1-ultra':
        litellm_id: 'aiml/flux-pro/v1.1-ultra'
        mode: image
      'flux-realism':
        litellm_id: 'aiml/flux-realism'
        mode: image
      'flux/dev':
        litellm_id: 'aiml/flux/dev'
        mode: image
      'flux/kontext-max/text-to-image':
        litellm_id: 'aiml/flux/kontext-max/text-to-image'
        mode: image
      'flux/kontext-pro/text-to-image':
        litellm_id: 'aiml/flux/kontext-pro/text-to-image'
        mode: image
      'flux/schnell':
        litellm_id: 'aiml/flux/schnell'
        mode: image
      'google/imagen-4.0-ultra-generate-001':
        litellm_id: 'aiml/google/imagen-4.0-ultra-generate-001'
        mode: image
      'google/nano-banana-pro':
        litellm_id: 'aiml/google/nano-banana-pro'
        mode: image
  aleph_alpha:
    model_count: 6
    models:
      'luminous-base':
        mode: completion
        max_input_tokens: 2048
        input_cost_per_1k: 0.03
        output_cost_per_1k: 0.033
      'luminous-base-control':
        mode: chat
        max_input_tokens: 2048
        input_cost_per_1k: 0.0375
        output_cost_per_1k: 0.04125
      'luminous-extended':
        mode: completion
        max_input_tokens: 2048
        input_cost_per_1k: 0.045
        output_cost_per_1k: 0.0495
      'luminous-extended-control':
        mode: chat
        max_input_tokens: 2048
        input_cost_per_1k: 0.05625
        output_cost_per_1k: 0.061875
      'luminous-supreme':
        mode: completion
        max_input_tokens: 2048
        input_cost_per_1k: 0.175
        output_cost_per_1k: 0.1925
      'luminous-supreme-control':
        mode: chat
        max_input_tokens: 2048
        input_cost_per_1k: 0.21875
        output_cost_per_1k: 0.240625
  amazon_nova:
    model_count: 4
    models:
      'amazon-nova/nova-lite-v1':
        mode: chat
        max_input_tokens: 300000
        max_output_tokens: 10000
        input_cost_per_1k: 6e-05
        output_cost_per_1k: 0.00024
        capabilities: [vision, function_calling, json_mode, prompt_caching]
      'amazon-nova/nova-micro-v1':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 10000
        input_cost_per_1k: 3.5e-05
        output_cost_per_1k: 0.00014
        capabilities: [function_calling, json_mode, prompt_caching]
      'amazon-nova/nova-premier-v1':
        mode: chat
        max_input_tokens: 1000000
        max_output_tokens: 10000
        input_cost_per_1k: 0.0025
        output_cost_per_1k: 0.0125
        capabilities: [vision, function_calling, json_mode]
      'amazon-nova/nova-pro-v1':
        mode: chat
        max_input_tokens: 300000
        max_output_tokens: 10000
        input_cost_per_1k: 0.0008
        output_cost_per_1k: 0.0032
        capabilities: [vision, function_calling, json_mode, prompt_caching]
  anthropic:
    model_count: 22
    models:
      'claude-3-5-haiku-20241022':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 8192
        input_cost_per_1k: 0.0008
        output_cost_per_1k: 0.004
        capabilities: [vision, function_calling, json_mode, prompt_caching, web_search]
        deprecated: true
        deprecation_date: '2025-10-01'
      'claude-3-5-haiku-latest':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 8192
        input_cost_per_1k: 0.001
        output_cost_per_1k: 0.005
        capabilities: [vision, function_calling, json_mode, prompt_caching, web_search]
        deprecated: true
        deprecation_date: '2025-10-01'
      'claude-3-5-sonnet-20240620':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 8192
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.015
        capabilities: [vision, function_calling, json_mode, prompt_caching]
        deprecated: true
        deprecation_date: '2025-06-01'
      'claude-3-5-sonnet-20241022':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 8192
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.015
        capabilities: [vision, function_calling, json_mode, prompt_caching, web_search]
        deprecated: true
        deprecation_date: '2025-10-01'
      'claude-3-5-sonnet-latest':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 8192
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.015
        capabilities: [vision, function_calling, json_mode, prompt_caching, web_search]
        deprecated: true
        deprecation_date: '2025-06-01'
      'claude-3-7-sonnet-20250219':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 64000
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.015
        capabilities: [vision, function_calling, json_mode, prompt_caching, reasoning, web_search]
        deprecated: true
        deprecation_date: '2026-02-19'
      'claude-3-7-sonnet-latest':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 64000
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.015
        capabilities: [vision, function_calling, json_mode, prompt_caching, reasoning]
        deprecated: true
        deprecation_date: '2025-06-01'
      'claude-3-haiku-20240307':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 4096
        input_cost_per_1k: 0.00025
        output_cost_per_1k: 0.00125
        capabilities: [vision, function_calling, json_mode, prompt_caching]
      'claude-3-opus-20240229':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 4096
        input_cost_per_1k: 0.015
        output_cost_per_1k: 0.075
        capabilities: [vision, function_calling, json_mode, prompt_caching]
        deprecated: true
        deprecation_date: '2026-05-01'
      'claude-3-opus-latest':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 4096
        input_cost_per_1k: 0.015
        output_cost_per_1k: 0.075
        capabilities: [vision, function_calling, json_mode, prompt_caching]
        deprecated: true
        deprecation_date: '2025-03-01'
      'claude-4-opus-20250514':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 32000
        input_cost_per_1k: 0.015
        output_cost_per_1k: 0.075
        capabilities: [vision, function_calling, json_mode, prompt_caching, reasoning]
      'claude-4-sonnet-20250514':
        mode: chat
        max_input_tokens: 1000000
        max_output_tokens: 64000
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.015
        capabilities: [vision, function_calling, json_mode, prompt_caching, reasoning]
      'claude-haiku-4-5':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 64000
        input_cost_per_1k: 0.001
        output_cost_per_1k: 0.005
        capabilities: [vision, function_calling, json_mode, prompt_caching, reasoning]
      'claude-haiku-4-5-20251001':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 64000
        input_cost_per_1k: 0.001
        output_cost_per_1k: 0.005
        capabilities: [vision, function_calling, json_mode, prompt_caching, reasoning]
      'claude-opus-4-1':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 32000
        input_cost_per_1k: 0.015
        output_cost_per_1k: 0.075
        capabilities: [vision, function_calling, json_mode, prompt_caching, reasoning]
      'claude-opus-4-1-20250805':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 32000
        input_cost_per_1k: 0.015
        output_cost_per_1k: 0.075
        capabilities: [vision, function_calling, json_mode, prompt_caching, reasoning]
        deprecated: true
        deprecation_date: '2026-08-05'
      'claude-opus-4-20250514':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 32000
        input_cost_per_1k: 0.015
        output_cost_per_1k: 0.075
        capabilities: [vision, function_calling, json_mode, prompt_caching, reasoning]
        deprecated: true
        deprecation_date: '2026-05-14'
      'claude-opus-4-5':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 64000
        input_cost_per_1k: 0.005
        output_cost_per_1k: 0.025
        capabilities: [vision, function_calling, json_mode, prompt_caching, reasoning]
      'claude-opus-4-5-20251101':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 64000
        input_cost_per_1k: 0.005
        output_cost_per_1k: 0.025
        capabilities: [vision, function_calling, json_mode, prompt_caching, reasoning]
      'claude-sonnet-4-20250514':
        mode: chat
        max_input_tokens: 1000000
        max_output_tokens: 64000
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.015
        capabilities: [vision, function_calling, json_mode, prompt_caching, reasoning]
        deprecated: true
        deprecation_date: '2026-05-14'
      'claude-sonnet-4-5':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 64000
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.015
        capabilities: [vision, function_calling, json_mode, prompt_caching, reasoning]
      'claude-sonnet-4-5-20250929':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 64000
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.015
        capabilities: [vision, function_calling, json_mode, prompt_caching, reasoning, web_search]
  anyscale:
    model_count: 12
    models:
      'HuggingFaceH4/zephyr-7b-beta':
        litellm_id: 'anyscale/HuggingFaceH4/zephyr-7b-beta'
        mode: chat
        max_input_tokens: 16384
        max_output_tokens: 16384
        input_cost_per_1k: 0.00015
        output_cost_per_1k: 0.00015
      'codellama/CodeLlama-34b-Instruct-hf':
        litellm_id: 'anyscale/codellama/CodeLlama-34b-Instruct-hf'
        mode: chat
        max_input_tokens: 4096
        max_output_tokens: 4096
        input_cost_per_1k: 0.001
        output_cost_per_1k: 0.001
      'codellama/CodeLlama-70b-Instruct-hf':
        litellm_id: 'anyscale/codellama/CodeLlama-70b-Instruct-hf'
        mode: chat
        max_input_tokens: 4096
        max_output_tokens: 4096
        input_cost_per_1k: 0.001
        output_cost_per_1k: 0.001
      'google/gemma-7b-it':
        litellm_id: 'anyscale/google/gemma-7b-it'
        mode: chat
        max_input_tokens: 8192
        max_output_tokens: 8192
        input_cost_per_1k: 0.00015
        output_cost_per_1k: 0.00015
      'meta-llama/Llama-2-13b-chat-hf':
        litellm_id: 'anyscale/meta-llama/Llama-2-13b-chat-hf'
        mode: chat
        max_input_tokens: 4096
        max_output_tokens: 4096
        input_cost_per_1k: 0.00025
        output_cost_per_1k: 0.00025
      'meta-llama/Llama-2-70b-chat-hf':
        litellm_id: 'anyscale/meta-llama/Llama-2-70b-chat-hf'
        mode: chat
        max_input_tokens: 4096
        max_output_tokens: 4096
        input_cost_per_1k: 0.001
        output_cost_per_1k: 0.001
      'meta-llama/Llama-2-7b-chat-hf':
        litellm_id: 'anyscale/meta-llama/Llama-2-7b-chat-hf'
        mode: chat
        max_input_tokens: 4096
        max_output_tokens: 4096
        input_cost_per_1k: 0.00015
        output_cost_per_1k: 0.00015
      'meta-llama/Meta-Llama-3-70B-Instruct':
        litellm_id: 'anyscale/meta-llama/Meta-Llama-3-70B-Instruct'
        mode: chat
        max_input_tokens: 8192
        max_output_tokens: 8192
        input_cost_per_1k: 0.001
        output_cost_per_1k: 0.001
      'meta-llama/Meta-Llama-3-8B-Instruct':
        litellm_id: 'anyscale/meta-llama/Meta-Llama-3-8B-Instruct'
        mode: chat
        max_input_tokens: 8192
        max_output_tokens: 8192
        input_cost_per_1k: 0.00015
        output_cost_per_1k: 0.00015
      'mistralai/Mistral-7B-Instruct-v0.1':
        litellm_id: 'anyscale/mistralai/Mistral-7B-Instruct-v0.1'
        mode: chat
        max_input_tokens: 16384
        max_output_tokens: 16384
        input_cost_per_1k: 0.00015
        output_cost_per_1k: 0.00015
        capabilities: [function_calling]
      'mistralai/Mixtral-8x22B-Instruct-v0.1':
        litellm_id: 'anyscale/mistralai/Mixtral-8x22B-Instruct-v0.1'
        mode: chat
        max_input_tokens: 65536
        max_output_tokens: 65536
        input_cost_per_1k: 0.0009
        output_cost_per_1k: 0.0009
        capabilities: [function_calling]
      'mistralai/Mixtral-8x7B-Instruct-v0.1':
        litellm_id: 'anyscale/mistralai/Mixtral-8x7B-Instruct-v0.1'
        mode: chat
        max_input_tokens: 16384
        max_output_tokens: 16384
        input_cost_per_1k: 0.00015
        output_cost_per_1k: 0.00015
        capabilities: [function_calling]
  assemblyai:
    model_count: 2
    models:
      'best':
        litellm_id: 'assemblyai/best'
        mode: audio_transcription
      'nano':
        litellm_id: 'assemblyai/nano'
        mode: audio_transcription
  aws_bedrock:
    model_count: 192
    models:
      '*/1-month-commitment/cohere.command-light-text-v14':
        litellm_id: 'bedrock/*/1-month-commitment/cohere.command-light-text-v14'
        mode: chat
        max_input_tokens: 4096
        max_output_tokens: 4096
      '*/1-month-commitment/cohere.command-text-v14':
        litellm_id: 'bedrock/*/1-month-commitment/cohere.command-text-v14'
        mode: chat
        max_input_tokens: 4096
        max_output_tokens: 4096
      '*/6-month-commitment/cohere.command-light-text-v14':
        litellm_id: 'bedrock/*/6-month-commitment/cohere.command-light-text-v14'
        mode: chat
        max_input_tokens: 4096
        max_output_tokens: 4096
      '*/6-month-commitment/cohere.command-text-v14':
        litellm_id: 'bedrock/*/6-month-commitment/cohere.command-text-v14'
        mode: chat
        max_input_tokens: 4096
        max_output_tokens: 4096
      'ai21.j2-mid-v1':
        mode: chat
        max_input_tokens: 8191
        max_output_tokens: 8191
        input_cost_per_1k: 0.0125
        output_cost_per_1k: 0.0125
      'ai21.j2-ultra-v1':
        mode: chat
        max_input_tokens: 8191
        max_output_tokens: 8191
        input_cost_per_1k: 0.0188
        output_cost_per_1k: 0.0188
      'ai21.jamba-1-5-large-v1:0':
        mode: chat
        max_input_tokens: 256000
        max_output_tokens: 256000
        input_cost_per_1k: 0.002
        output_cost_per_1k: 0.008
      'ai21.jamba-1-5-mini-v1:0':
        mode: chat
        max_input_tokens: 256000
        max_output_tokens: 256000
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0004
      'ai21.jamba-instruct-v1:0':
        mode: chat
        max_input_tokens: 70000
        max_output_tokens: 4096
        input_cost_per_1k: 0.0005
        output_cost_per_1k: 0.0007
        capabilities: [system_messages]
      'amazon.nova-canvas-v1:0':
        mode: image
        max_input_tokens: 2600
      'amazon.rerank-v1:0':
        mode: rerank
        max_input_tokens: 32000
        max_output_tokens: 32000
      'amazon.titan-embed-image-v1':
        mode: embedding
        max_input_tokens: 128
        input_cost_per_1k: 0.0008
      'amazon.titan-embed-text-v1':
        mode: embedding
        max_input_tokens: 8192
        input_cost_per_1k: 0.0001
      'amazon.titan-embed-text-v2:0':
        mode: embedding
        max_input_tokens: 8192
        input_cost_per_1k: 0.0002
      'amazon.titan-image-generator-v1':
        mode: image
      'amazon.titan-image-generator-v2':
        mode: image
      'amazon.titan-image-generator-v2:0':
        mode: image
      'amazon.titan-text-express-v1':
        mode: chat
        max_input_tokens: 42000
        max_output_tokens: 8000
        input_cost_per_1k: 0.0013
        output_cost_per_1k: 0.0017
      'amazon.titan-text-lite-v1':
        mode: chat
        max_input_tokens: 42000
        max_output_tokens: 4000
        input_cost_per_1k: 0.0003
        output_cost_per_1k: 0.0004
      'amazon.titan-text-premier-v1:0':
        mode: chat
        max_input_tokens: 42000
        max_output_tokens: 32000
        input_cost_per_1k: 0.0005
        output_cost_per_1k: 0.0015
      'anthropic.claude-3-5-haiku-20241022-v1:0':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 8192
        input_cost_per_1k: 0.0008
        output_cost_per_1k: 0.004
        capabilities: [function_calling, json_mode, prompt_caching]
      'anthropic.claude-3-5-sonnet-20240620-v1:0':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 4096
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.015
        capabilities: [vision, function_calling, json_mode]
      'anthropic.claude-3-5-sonnet-20241022-v2:0':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 8192
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.015
        capabilities: [vision, function_calling, json_mode, prompt_caching]
      'anthropic.claude-3-7-sonnet-20240620-v1:0':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 8192
        input_cost_per_1k: 0.0036
        output_cost_per_1k: 0.018
        capabilities: [vision, function_calling, json_mode, prompt_caching, reasoning]
      'anthropic.claude-3-haiku-20240307-v1:0':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 4096
        input_cost_per_1k: 0.00025
        output_cost_per_1k: 0.00125
        capabilities: [vision, function_calling, json_mode]
      'anthropic.claude-3-opus-20240229-v1:0':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 4096
        input_cost_per_1k: 0.015
        output_cost_per_1k: 0.075
        capabilities: [vision, function_calling, json_mode]
      'anthropic.claude-3-sonnet-20240229-v1:0':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 4096
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.015
        capabilities: [vision, function_calling, json_mode]
      'anthropic.claude-instant-v1':
        mode: chat
        max_input_tokens: 100000
        max_output_tokens: 8191
        input_cost_per_1k: 0.0008
        output_cost_per_1k: 0.0024
      'anthropic.claude-v1':
        mode: chat
        max_input_tokens: 100000
        max_output_tokens: 8191
        input_cost_per_1k: 0.008
        output_cost_per_1k: 0.024
      'anthropic.claude-v2:1':
        mode: chat
        max_input_tokens: 100000
        max_output_tokens: 8191
        input_cost_per_1k: 0.008
        output_cost_per_1k: 0.024
      'ap-northeast-1/1-month-commitment/anthropic.claude-instant-v1':
        litellm_id: 'bedrock/ap-northeast-1/1-month-commitment/anthropic.claude-instant-v1'
        mode: chat
        max_input_tokens: 100000
        max_output_tokens: 8191
      'ap-northeast-1/1-month-commitment/anthropic.claude-v1':
        litellm_id: 'bedrock/ap-northeast-1/1-month-commitment/anthropic.claude-v1'
        mode: chat
        max_input_tokens: 100000
        max_output_tokens: 8191
      'ap-northeast-1/1-month-commitment/anthropic.claude-v2:1':
        litellm_id: 'bedrock/ap-northeast-1/1-month-commitment/anthropic.claude-v2:1'
        mode: chat
        max_input_tokens: 100000
        max_output_tokens: 8191
      'ap-northeast-1/6-month-commitment/anthropic.claude-instant-v1':
        litellm_id: 'bedrock/ap-northeast-1/6-month-commitment/anthropic.claude-instant-v1'
        mode: chat
        max_input_tokens: 100000
        max_output_tokens: 8191
      'ap-northeast-1/6-month-commitment/anthropic.claude-v1':
        litellm_id: 'bedrock/ap-northeast-1/6-month-commitment/anthropic.claude-v1'
        mode: chat
        max_input_tokens: 100000
        max_output_tokens: 8191
      'ap-northeast-1/6-month-commitment/anthropic.claude-v2:1':
        litellm_id: 'bedrock/ap-northeast-1/6-month-commitment/anthropic.claude-v2:1'
        mode: chat
        max_input_tokens: 100000
        max_output_tokens: 8191
      'ap-northeast-1/anthropic.claude-instant-v1':
        litellm_id: 'bedrock/ap-northeast-1/anthropic.claude-instant-v1'
        mode: chat
        max_input_tokens: 100000
        max_output_tokens: 8191
        input_cost_per_1k: 0.00223
        output_cost_per_1k: 0.00755
      'ap-northeast-1/anthropic.claude-v1':
        litellm_id: 'bedrock/ap-northeast-1/anthropic.claude-v1'
        mode: chat
        max_input_tokens: 100000
        max_output_tokens: 8191
        input_cost_per_1k: 0.008
        output_cost_per_1k: 0.024
      'ap-northeast-1/anthropic.claude-v2:1':
        litellm_id: 'bedrock/ap-northeast-1/anthropic.claude-v2:1'
        mode: chat
        max_input_tokens: 100000
        max_output_tokens: 8191
        input_cost_per_1k: 0.008
        output_cost_per_1k: 0.024
      'ap-south-1/meta.llama3-70b-instruct-v1:0':
        litellm_id: 'bedrock/ap-south-1/meta.llama3-70b-instruct-v1:0'
        mode: chat
        max_input_tokens: 8192
        max_output_tokens: 8192
        input_cost_per_1k: 0.00318
        output_cost_per_1k: 0.0042
      'ap-south-1/meta.llama3-8b-instruct-v1:0':
        litellm_id: 'bedrock/ap-south-1/meta.llama3-8b-instruct-v1:0'
        mode: chat
        max_input_tokens: 8192
        max_output_tokens: 8192
        input_cost_per_1k: 0.00036
        output_cost_per_1k: 0.00072
      'apac.anthropic.claude-3-5-sonnet-20240620-v1:0':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 4096
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.015
        capabilities: [vision, function_calling, json_mode]
      'apac.anthropic.claude-3-5-sonnet-20241022-v2:0':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 8192
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.015
        capabilities: [vision, function_calling, json_mode, prompt_caching]
      'apac.anthropic.claude-3-haiku-20240307-v1:0':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 4096
        input_cost_per_1k: 0.00025
        output_cost_per_1k: 0.00125
        capabilities: [vision, function_calling, json_mode]
      'apac.anthropic.claude-3-sonnet-20240229-v1:0':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 4096
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.015
        capabilities: [vision, function_calling, json_mode]
      'ca-central-1/meta.llama3-70b-instruct-v1:0':
        litellm_id: 'bedrock/ca-central-1/meta.llama3-70b-instruct-v1:0'
        mode: chat
        max_input_tokens: 8192
        max_output_tokens: 8192
        input_cost_per_1k: 0.00305
        output_cost_per_1k: 0.00403
      'ca-central-1/meta.llama3-8b-instruct-v1:0':
        litellm_id: 'bedrock/ca-central-1/meta.llama3-8b-instruct-v1:0'
        mode: chat
        max_input_tokens: 8192
        max_output_tokens: 8192
        input_cost_per_1k: 0.00035
        output_cost_per_1k: 0.00069
      'claude-sonnet-4-5-20250929-v1:0':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 64000
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.015
        capabilities: [vision, function_calling, json_mode, prompt_caching, reasoning]
      'cohere.command-light-text-v14':
        mode: chat
        max_input_tokens: 4096
        max_output_tokens: 4096
        input_cost_per_1k: 0.0003
        output_cost_per_1k: 0.0006
      'cohere.command-r-plus-v1:0':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 4096
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.015
      'cohere.command-r-v1:0':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 4096
        input_cost_per_1k: 0.0005
        output_cost_per_1k: 0.0015
      'cohere.command-text-v14':
        mode: chat
        max_input_tokens: 4096
        max_output_tokens: 4096
        input_cost_per_1k: 0.0015
        output_cost_per_1k: 0.002
      'cohere.embed-english-v3':
        mode: embedding
        max_input_tokens: 512
        input_cost_per_1k: 0.0001
      'cohere.embed-multilingual-v3':
        mode: embedding
        max_input_tokens: 512
        input_cost_per_1k: 0.0001
      'cohere.embed-v4:0':
        mode: embedding
        max_input_tokens: 128000
        input_cost_per_1k: 0.00012
      'cohere.rerank-v3-5:0':
        mode: rerank
        max_input_tokens: 32000
        max_output_tokens: 32000
      'eu-central-1/1-month-commitment/anthropic.claude-instant-v1':
        litellm_id: 'bedrock/eu-central-1/1-month-commitment/anthropic.claude-instant-v1'
        mode: chat
        max_input_tokens: 100000
        max_output_tokens: 8191
      'eu-central-1/1-month-commitment/anthropic.claude-v1':
        litellm_id: 'bedrock/eu-central-1/1-month-commitment/anthropic.claude-v1'
        mode: chat
        max_input_tokens: 100000
        max_output_tokens: 8191
      'eu-central-1/1-month-commitment/anthropic.claude-v2:1':
        litellm_id: 'bedrock/eu-central-1/1-month-commitment/anthropic.claude-v2:1'
        mode: chat
        max_input_tokens: 100000
        max_output_tokens: 8191
      'eu-central-1/6-month-commitment/anthropic.claude-instant-v1':
        litellm_id: 'bedrock/eu-central-1/6-month-commitment/anthropic.claude-instant-v1'
        mode: chat
        max_input_tokens: 100000
        max_output_tokens: 8191
      'eu-central-1/6-month-commitment/anthropic.claude-v1':
        litellm_id: 'bedrock/eu-central-1/6-month-commitment/anthropic.claude-v1'
        mode: chat
        max_input_tokens: 100000
        max_output_tokens: 8191
      'eu-central-1/6-month-commitment/anthropic.claude-v2:1':
        litellm_id: 'bedrock/eu-central-1/6-month-commitment/anthropic.claude-v2:1'
        mode: chat
        max_input_tokens: 100000
        max_output_tokens: 8191
      'eu-central-1/anthropic.claude-instant-v1':
        litellm_id: 'bedrock/eu-central-1/anthropic.claude-instant-v1'
        mode: chat
        max_input_tokens: 100000
        max_output_tokens: 8191
        input_cost_per_1k: 0.00248
        output_cost_per_1k: 0.00838
      'eu-central-1/anthropic.claude-v1':
        litellm_id: 'bedrock/eu-central-1/anthropic.claude-v1'
        mode: chat
        max_input_tokens: 100000
        max_output_tokens: 8191
        input_cost_per_1k: 0.008
        output_cost_per_1k: 0.024
      'eu-central-1/anthropic.claude-v2:1':
        litellm_id: 'bedrock/eu-central-1/anthropic.claude-v2:1'
        mode: chat
        max_input_tokens: 100000
        max_output_tokens: 8191
        input_cost_per_1k: 0.008
        output_cost_per_1k: 0.024
      'eu-west-1/meta.llama3-70b-instruct-v1:0':
        litellm_id: 'bedrock/eu-west-1/meta.llama3-70b-instruct-v1:0'
        mode: chat
        max_input_tokens: 8192
        max_output_tokens: 8192
        input_cost_per_1k: 0.00286
        output_cost_per_1k: 0.00378
      'eu-west-1/meta.llama3-8b-instruct-v1:0':
        litellm_id: 'bedrock/eu-west-1/meta.llama3-8b-instruct-v1:0'
        mode: chat
        max_input_tokens: 8192
        max_output_tokens: 8192
        input_cost_per_1k: 0.00032
        output_cost_per_1k: 0.00065
      'eu-west-2/meta.llama3-70b-instruct-v1:0':
        litellm_id: 'bedrock/eu-west-2/meta.llama3-70b-instruct-v1:0'
        mode: chat
        max_input_tokens: 8192
        max_output_tokens: 8192
        input_cost_per_1k: 0.00345
        output_cost_per_1k: 0.00455
      'eu-west-2/meta.llama3-8b-instruct-v1:0':
        litellm_id: 'bedrock/eu-west-2/meta.llama3-8b-instruct-v1:0'
        mode: chat
        max_input_tokens: 8192
        max_output_tokens: 8192
        input_cost_per_1k: 0.00039
        output_cost_per_1k: 0.00078
      'eu-west-3/mistral.mistral-7b-instruct-v0:2':
        litellm_id: 'bedrock/eu-west-3/mistral.mistral-7b-instruct-v0:2'
        mode: chat
        max_input_tokens: 32000
        max_output_tokens: 8191
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.00026
      'eu-west-3/mistral.mistral-large-2402-v1:0':
        litellm_id: 'bedrock/eu-west-3/mistral.mistral-large-2402-v1:0'
        mode: chat
        max_input_tokens: 32000
        max_output_tokens: 8191
        input_cost_per_1k: 0.0104
        output_cost_per_1k: 0.0312
        capabilities: [function_calling]
      'eu-west-3/mistral.mixtral-8x7b-instruct-v0:1':
        litellm_id: 'bedrock/eu-west-3/mistral.mixtral-8x7b-instruct-v0:1'
        mode: chat
        max_input_tokens: 32000
        max_output_tokens: 8191
        input_cost_per_1k: 0.00059
        output_cost_per_1k: 0.00091
      'eu.anthropic.claude-3-5-haiku-20241022-v1:0':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 8192
        input_cost_per_1k: 0.00025
        output_cost_per_1k: 0.00125
        capabilities: [function_calling, json_mode, prompt_caching]
      'eu.anthropic.claude-3-5-sonnet-20240620-v1:0':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 4096
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.015
        capabilities: [vision, function_calling, json_mode]
      'eu.anthropic.claude-3-5-sonnet-20241022-v2:0':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 8192
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.015
        capabilities: [vision, function_calling, json_mode, prompt_caching]
      'eu.anthropic.claude-3-7-sonnet-20250219-v1:0':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 8192
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.015
        capabilities: [vision, function_calling, json_mode, prompt_caching, reasoning]
      'eu.anthropic.claude-3-haiku-20240307-v1:0':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 4096
        input_cost_per_1k: 0.00025
        output_cost_per_1k: 0.00125
        capabilities: [vision, function_calling, json_mode]
      'eu.anthropic.claude-3-opus-20240229-v1:0':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 4096
        input_cost_per_1k: 0.015
        output_cost_per_1k: 0.075
        capabilities: [vision, function_calling, json_mode]
      'eu.anthropic.claude-3-sonnet-20240229-v1:0':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 4096
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.015
        capabilities: [vision, function_calling, json_mode]
      'eu.meta.llama3-2-1b-instruct-v1:0':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 4096
        input_cost_per_1k: 0.00013
        output_cost_per_1k: 0.00013
        capabilities: [function_calling]
      'eu.meta.llama3-2-3b-instruct-v1:0':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 4096
        input_cost_per_1k: 0.00019
        output_cost_per_1k: 0.00019
        capabilities: [function_calling]
      'eu.twelvelabs.marengo-embed-2-7-v1:0':
        mode: embedding
        max_input_tokens: 77
        input_cost_per_1k: 0.07
      'eu.twelvelabs.pegasus-1-2-v1:0':
        mode: chat
        output_cost_per_1k: 0.0075
      'invoke/anthropic.claude-3-5-sonnet-20240620-v1:0':
        litellm_id: 'bedrock/invoke/anthropic.claude-3-5-sonnet-20240620-v1:0'
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 4096
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.015
        capabilities: [vision, function_calling, json_mode]
      'max-x-max/50-steps/stability.stable-diffusion-xl-v0':
        mode: image
        max_input_tokens: 77
      'max-x-max/max-steps/stability.stable-diffusion-xl-v0':
        mode: image
        max_input_tokens: 77
      'meta.llama2-13b-chat-v1':
        mode: chat
        max_input_tokens: 4096
        max_output_tokens: 4096
        input_cost_per_1k: 0.00075
        output_cost_per_1k: 0.001
      'meta.llama2-70b-chat-v1':
        mode: chat
        max_input_tokens: 4096
        max_output_tokens: 4096
        input_cost_per_1k: 0.00195
        output_cost_per_1k: 0.00256
      'meta.llama3-1-405b-instruct-v1:0':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 4096
        input_cost_per_1k: 0.00532
        output_cost_per_1k: 0.016
        capabilities: [function_calling]
      'meta.llama3-1-70b-instruct-v1:0':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 2048
        input_cost_per_1k: 0.00099
        output_cost_per_1k: 0.00099
        capabilities: [function_calling]
      'meta.llama3-1-8b-instruct-v1:0':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 2048
        input_cost_per_1k: 0.00022
        output_cost_per_1k: 0.00022
        capabilities: [function_calling]
      'meta.llama3-2-11b-instruct-v1:0':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 4096
        input_cost_per_1k: 0.00035
        output_cost_per_1k: 0.00035
        capabilities: [vision, function_calling]
      'meta.llama3-2-1b-instruct-v1:0':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 4096
        input_cost_per_1k: 0.0001
        output_cost_per_1k: 0.0001
        capabilities: [function_calling]
      'meta.llama3-2-3b-instruct-v1:0':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 4096
        input_cost_per_1k: 0.00015
        output_cost_per_1k: 0.00015
        capabilities: [function_calling]
      'meta.llama3-2-90b-instruct-v1:0':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 4096
        input_cost_per_1k: 0.002
        output_cost_per_1k: 0.002
        capabilities: [vision, function_calling]
      'meta.llama3-70b-instruct-v1:0':
        mode: chat
        max_input_tokens: 8192
        max_output_tokens: 8192
        input_cost_per_1k: 0.00265
        output_cost_per_1k: 0.0035
      'meta.llama3-8b-instruct-v1:0':
        mode: chat
        max_input_tokens: 8192
        max_output_tokens: 8192
        input_cost_per_1k: 0.0003
        output_cost_per_1k: 0.0006
      'mistral.mistral-7b-instruct-v0:2':
        mode: chat
        max_input_tokens: 32000
        max_output_tokens: 8191
        input_cost_per_1k: 0.00015
        output_cost_per_1k: 0.0002
      'mistral.mistral-large-2402-v1:0':
        mode: chat
        max_input_tokens: 32000
        max_output_tokens: 8191
        input_cost_per_1k: 0.008
        output_cost_per_1k: 0.024
        capabilities: [function_calling]
      'mistral.mistral-large-2407-v1:0':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 8191
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.009
        capabilities: [function_calling]
      'mistral.mistral-small-2402-v1:0':
        mode: chat
        max_input_tokens: 32000
        max_output_tokens: 8191
        input_cost_per_1k: 0.001
        output_cost_per_1k: 0.003
        capabilities: [function_calling]
      'mistral.mixtral-8x7b-instruct-v0:1':
        mode: chat
        max_input_tokens: 32000
        max_output_tokens: 8191
        input_cost_per_1k: 0.00045
        output_cost_per_1k: 0.0007
      'sa-east-1/meta.llama3-70b-instruct-v1:0':
        litellm_id: 'bedrock/sa-east-1/meta.llama3-70b-instruct-v1:0'
        mode: chat
        max_input_tokens: 8192
        max_output_tokens: 8192
        input_cost_per_1k: 0.00445
        output_cost_per_1k: 0.00588
      'sa-east-1/meta.llama3-8b-instruct-v1:0':
        litellm_id: 'bedrock/sa-east-1/meta.llama3-8b-instruct-v1:0'
        mode: chat
        max_input_tokens: 8192
        max_output_tokens: 8192
        input_cost_per_1k: 0.0005
        output_cost_per_1k: 0.00101
      'stability.sd3-5-large-v1:0':
        mode: image
        max_input_tokens: 77
      'stability.sd3-large-v1:0':
        mode: image
        max_input_tokens: 77
      'stability.stable-conservative-upscale-v1:0':
        mode: image_edit
        max_input_tokens: 77
      'stability.stable-creative-upscale-v1:0':
        mode: image_edit
        max_input_tokens: 77
      'stability.stable-fast-upscale-v1:0':
        mode: image_edit
        max_input_tokens: 77
      'stability.stable-image-control-sketch-v1:0':
        mode: image_edit
        max_input_tokens: 77
      'stability.stable-image-control-structure-v1:0':
        mode: image_edit
        max_input_tokens: 77
      'stability.stable-image-core-v1:0':
        mode: image
        max_input_tokens: 77
      'stability.stable-image-core-v1:1':
        mode: image
        max_input_tokens: 77
      'stability.stable-image-erase-object-v1:0':
        mode: image_edit
        max_input_tokens: 77
      'stability.stable-image-inpaint-v1:0':
        mode: image_edit
        max_input_tokens: 77
      'stability.stable-image-remove-background-v1:0':
        mode: image_edit
        max_input_tokens: 77
      'stability.stable-image-search-recolor-v1:0':
        mode: image_edit
        max_input_tokens: 77
      'stability.stable-image-search-replace-v1:0':
        mode: image_edit
        max_input_tokens: 77
      'stability.stable-image-style-guide-v1:0':
        mode: image_edit
        max_input_tokens: 77
      'stability.stable-image-ultra-v1:0':
        mode: image
        max_input_tokens: 77
      'stability.stable-image-ultra-v1:1':
        mode: image
        max_input_tokens: 77
      'stability.stable-outpaint-v1:0':
        mode: image_edit
        max_input_tokens: 77
      'stability.stable-style-transfer-v1:0':
        mode: image_edit
        max_input_tokens: 77
      'twelvelabs.marengo-embed-2-7-v1:0':
        mode: embedding
        max_input_tokens: 77
        input_cost_per_1k: 0.07
      'twelvelabs.pegasus-1-2-v1:0':
        mode: chat
        output_cost_per_1k: 0.0075
      'us-east-1/1-month-commitment/anthropic.claude-instant-v1':
        litellm_id: 'bedrock/us-east-1/1-month-commitment/anthropic.claude-instant-v1'
        mode: chat
        max_input_tokens: 100000
        max_output_tokens: 8191
      'us-east-1/1-month-commitment/anthropic.claude-v1':
        litellm_id: 'bedrock/us-east-1/1-month-commitment/anthropic.claude-v1'
        mode: chat
        max_input_tokens: 100000
        max_output_tokens: 8191
      'us-east-1/1-month-commitment/anthropic.claude-v2:1':
        litellm_id: 'bedrock/us-east-1/1-month-commitment/anthropic.claude-v2:1'
        mode: chat
        max_input_tokens: 100000
        max_output_tokens: 8191
      'us-east-1/6-month-commitment/anthropic.claude-instant-v1':
        litellm_id: 'bedrock/us-east-1/6-month-commitment/anthropic.claude-instant-v1'
        mode: chat
        max_input_tokens: 100000
        max_output_tokens: 8191
      'us-east-1/6-month-commitment/anthropic.claude-v1':
        litellm_id: 'bedrock/us-east-1/6-month-commitment/anthropic.claude-v1'
        mode: chat
        max_input_tokens: 100000
        max_output_tokens: 8191
      'us-east-1/6-month-commitment/anthropic.claude-v2:1':
        litellm_id: 'bedrock/us-east-1/6-month-commitment/anthropic.claude-v2:1'
        mode: chat
        max_input_tokens: 100000
        max_output_tokens: 8191
      'us-east-1/anthropic.claude-instant-v1':
        litellm_id: 'bedrock/us-east-1/anthropic.claude-instant-v1'
        mode: chat
        max_input_tokens: 100000
        max_output_tokens: 8191
        input_cost_per_1k: 0.0008
        output_cost_per_1k: 0.0024
      'us-east-1/anthropic.claude-v1':
        litellm_id: 'bedrock/us-east-1/anthropic.claude-v1'
        mode: chat
        max_input_tokens: 100000
        max_output_tokens: 8191
        input_cost_per_1k: 0.008
        output_cost_per_1k: 0.024
      'us-east-1/anthropic.claude-v2:1':
        litellm_id: 'bedrock/us-east-1/anthropic.claude-v2:1'
        mode: chat
        max_input_tokens: 100000
        max_output_tokens: 8191
        input_cost_per_1k: 0.008
        output_cost_per_1k: 0.024
      'us-east-1/meta.llama3-70b-instruct-v1:0':
        litellm_id: 'bedrock/us-east-1/meta.llama3-70b-instruct-v1:0'
        mode: chat
        max_input_tokens: 8192
        max_output_tokens: 8192
        input_cost_per_1k: 0.00265
        output_cost_per_1k: 0.0035
      'us-east-1/meta.llama3-8b-instruct-v1:0':
        litellm_id: 'bedrock/us-east-1/meta.llama3-8b-instruct-v1:0'
        mode: chat
        max_input_tokens: 8192
        max_output_tokens: 8192
        input_cost_per_1k: 0.0003
        output_cost_per_1k: 0.0006
      'us-east-1/mistral.mistral-7b-instruct-v0:2':
        litellm_id: 'bedrock/us-east-1/mistral.mistral-7b-instruct-v0:2'
        mode: chat
        max_input_tokens: 32000
        max_output_tokens: 8191
        input_cost_per_1k: 0.00015
        output_cost_per_1k: 0.0002
      'us-east-1/mistral.mistral-large-2402-v1:0':
        litellm_id: 'bedrock/us-east-1/mistral.mistral-large-2402-v1:0'
        mode: chat
        max_input_tokens: 32000
        max_output_tokens: 8191
        input_cost_per_1k: 0.008
        output_cost_per_1k: 0.024
        capabilities: [function_calling]
      'us-east-1/mistral.mixtral-8x7b-instruct-v0:1':
        litellm_id: 'bedrock/us-east-1/mistral.mixtral-8x7b-instruct-v0:1'
        mode: chat
        max_input_tokens: 32000
        max_output_tokens: 8191
        input_cost_per_1k: 0.00045
        output_cost_per_1k: 0.0007
      'us-gov-east-1/amazon.nova-pro-v1:0':
        litellm_id: 'bedrock/us-gov-east-1/amazon.nova-pro-v1:0'
        mode: chat
        max_input_tokens: 300000
        max_output_tokens: 10000
        input_cost_per_1k: 0.00096
        output_cost_per_1k: 0.00384
        capabilities: [vision, function_calling, json_mode, prompt_caching]
      'us-gov-east-1/amazon.titan-embed-text-v1':
        litellm_id: 'bedrock/us-gov-east-1/amazon.titan-embed-text-v1'
        mode: embedding
        max_input_tokens: 8192
        input_cost_per_1k: 0.0001
      'us-gov-east-1/amazon.titan-embed-text-v2:0':
        litellm_id: 'bedrock/us-gov-east-1/amazon.titan-embed-text-v2:0'
        mode: embedding
        max_input_tokens: 8192
        input_cost_per_1k: 0.0002
      'us-gov-east-1/amazon.titan-text-express-v1':
        litellm_id: 'bedrock/us-gov-east-1/amazon.titan-text-express-v1'
        mode: chat
        max_input_tokens: 42000
        max_output_tokens: 8000
        input_cost_per_1k: 0.0013
        output_cost_per_1k: 0.0017
      'us-gov-east-1/amazon.titan-text-lite-v1':
        litellm_id: 'bedrock/us-gov-east-1/amazon.titan-text-lite-v1'
        mode: chat
        max_input_tokens: 42000
        max_output_tokens: 4000
        input_cost_per_1k: 0.0003
        output_cost_per_1k: 0.0004
      'us-gov-east-1/amazon.titan-text-premier-v1:0':
        litellm_id: 'bedrock/us-gov-east-1/amazon.titan-text-premier-v1:0'
        mode: chat
        max_input_tokens: 42000
        max_output_tokens: 32000
        input_cost_per_1k: 0.0005
        output_cost_per_1k: 0.0015
      'us-gov-east-1/anthropic.claude-3-5-sonnet-20240620-v1:0':
        litellm_id: 'bedrock/us-gov-east-1/anthropic.claude-3-5-sonnet-20240620-v1:0'
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 8192
        input_cost_per_1k: 0.0036
        output_cost_per_1k: 0.018
        capabilities: [vision, function_calling, json_mode]
      'us-gov-east-1/anthropic.claude-3-haiku-20240307-v1:0':
        litellm_id: 'bedrock/us-gov-east-1/anthropic.claude-3-haiku-20240307-v1:0'
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 4096
        input_cost_per_1k: 0.0003
        output_cost_per_1k: 0.0015
        capabilities: [vision, function_calling, json_mode]
      'us-gov-east-1/claude-sonnet-4-5-20250929-v1:0':
        litellm_id: 'bedrock/us-gov-east-1/claude-sonnet-4-5-20250929-v1:0'
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 4096
        input_cost_per_1k: 0.0033
        output_cost_per_1k: 0.0165
        capabilities: [vision, function_calling, json_mode, prompt_caching, reasoning]
      'us-gov-east-1/meta.llama3-70b-instruct-v1:0':
        litellm_id: 'bedrock/us-gov-east-1/meta.llama3-70b-instruct-v1:0'
        mode: chat
        max_input_tokens: 8000
        max_output_tokens: 2048
        input_cost_per_1k: 0.00265
        output_cost_per_1k: 0.0035
      'us-gov-east-1/meta.llama3-8b-instruct-v1:0':
        litellm_id: 'bedrock/us-gov-east-1/meta.llama3-8b-instruct-v1:0'
        mode: chat
        max_input_tokens: 8000
        max_output_tokens: 2048
        input_cost_per_1k: 0.0003
        output_cost_per_1k: 0.00265
      'us-gov-west-1/amazon.nova-pro-v1:0':
        litellm_id: 'bedrock/us-gov-west-1/amazon.nova-pro-v1:0'
        mode: chat
        max_input_tokens: 300000
        max_output_tokens: 10000
        input_cost_per_1k: 0.00096
        output_cost_per_1k: 0.00384
        capabilities: [vision, function_calling, json_mode, prompt_caching]
      'us-gov-west-1/amazon.titan-embed-text-v1':
        litellm_id: 'bedrock/us-gov-west-1/amazon.titan-embed-text-v1'
        mode: embedding
        max_input_tokens: 8192
        input_cost_per_1k: 0.0001
      'us-gov-west-1/amazon.titan-embed-text-v2:0':
        litellm_id: 'bedrock/us-gov-west-1/amazon.titan-embed-text-v2:0'
        mode: embedding
        max_input_tokens: 8192
        input_cost_per_1k: 0.0002
      'us-gov-west-1/amazon.titan-text-express-v1':
        litellm_id: 'bedrock/us-gov-west-1/amazon.titan-text-express-v1'
        mode: chat
        max_input_tokens: 42000
        max_output_tokens: 8000
        input_cost_per_1k: 0.0013
        output_cost_per_1k: 0.0017
      'us-gov-west-1/amazon.titan-text-lite-v1':
        litellm_id: 'bedrock/us-gov-west-1/amazon.titan-text-lite-v1'
        mode: chat
        max_input_tokens: 42000
        max_output_tokens: 4000
        input_cost_per_1k: 0.0003
        output_cost_per_1k: 0.0004
      'us-gov-west-1/amazon.titan-text-premier-v1:0':
        litellm_id: 'bedrock/us-gov-west-1/amazon.titan-text-premier-v1:0'
        mode: chat
        max_input_tokens: 42000
        max_output_tokens: 32000
        input_cost_per_1k: 0.0005
        output_cost_per_1k: 0.0015
      'us-gov-west-1/anthropic.claude-3-5-sonnet-20240620-v1:0':
        litellm_id: 'bedrock/us-gov-west-1/anthropic.claude-3-5-sonnet-20240620-v1:0'
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 8192
        input_cost_per_1k: 0.0036
        output_cost_per_1k: 0.018
        capabilities: [vision, function_calling, json_mode]
      'us-gov-west-1/anthropic.claude-3-7-sonnet-20250219-v1:0':
        litellm_id: 'bedrock/us-gov-west-1/anthropic.claude-3-7-sonnet-20250219-v1:0'
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 8192
        input_cost_per_1k: 0.0036
        output_cost_per_1k: 0.018
        capabilities: [vision, function_calling, json_mode, prompt_caching, reasoning]
      'us-gov-west-1/anthropic.claude-3-haiku-20240307-v1:0':
        litellm_id: 'bedrock/us-gov-west-1/anthropic.claude-3-haiku-20240307-v1:0'
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 4096
        input_cost_per_1k: 0.0003
        output_cost_per_1k: 0.0015
        capabilities: [vision, function_calling, json_mode]
      'us-gov-west-1/claude-sonnet-4-5-20250929-v1:0':
        litellm_id: 'bedrock/us-gov-west-1/claude-sonnet-4-5-20250929-v1:0'
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 4096
        input_cost_per_1k: 0.0033
        output_cost_per_1k: 0.0165
        capabilities: [vision, function_calling, json_mode, prompt_caching, reasoning]
      'us-gov-west-1/meta.llama3-70b-instruct-v1:0':
        litellm_id: 'bedrock/us-gov-west-1/meta.llama3-70b-instruct-v1:0'
        mode: chat
        max_input_tokens: 8000
        max_output_tokens: 2048
        input_cost_per_1k: 0.00265
        output_cost_per_1k: 0.0035
      'us-gov-west-1/meta.llama3-8b-instruct-v1:0':
        litellm_id: 'bedrock/us-gov-west-1/meta.llama3-8b-instruct-v1:0'
        mode: chat
        max_input_tokens: 8000
        max_output_tokens: 2048
        input_cost_per_1k: 0.0003
        output_cost_per_1k: 0.00265
      'us-west-1/meta.llama3-70b-instruct-v1:0':
        litellm_id: 'bedrock/us-west-1/meta.llama3-70b-instruct-v1:0'
        mode: chat
        max_input_tokens: 8192
        max_output_tokens: 8192
        input_cost_per_1k: 0.00265
        output_cost_per_1k: 0.0035
      'us-west-1/meta.llama3-8b-instruct-v1:0':
        litellm_id: 'bedrock/us-west-1/meta.llama3-8b-instruct-v1:0'
        mode: chat
        max_input_tokens: 8192
        max_output_tokens: 8192
        input_cost_per_1k: 0.0003
        output_cost_per_1k: 0.0006
      'us-west-2/1-month-commitment/anthropic.claude-instant-v1':
        litellm_id: 'bedrock/us-west-2/1-month-commitment/anthropic.claude-instant-v1'
        mode: chat
        max_input_tokens: 100000
        max_output_tokens: 8191
      'us-west-2/1-month-commitment/anthropic.claude-v1':
        litellm_id: 'bedrock/us-west-2/1-month-commitment/anthropic.claude-v1'
        mode: chat
        max_input_tokens: 100000
        max_output_tokens: 8191
      'us-west-2/1-month-commitment/anthropic.claude-v2:1':
        litellm_id: 'bedrock/us-west-2/1-month-commitment/anthropic.claude-v2:1'
        mode: chat
        max_input_tokens: 100000
        max_output_tokens: 8191
      'us-west-2/6-month-commitment/anthropic.claude-instant-v1':
        litellm_id: 'bedrock/us-west-2/6-month-commitment/anthropic.claude-instant-v1'
        mode: chat
        max_input_tokens: 100000
        max_output_tokens: 8191
      'us-west-2/6-month-commitment/anthropic.claude-v1':
        litellm_id: 'bedrock/us-west-2/6-month-commitment/anthropic.claude-v1'
        mode: chat
        max_input_tokens: 100000
        max_output_tokens: 8191
      'us-west-2/6-month-commitment/anthropic.claude-v2:1':
        litellm_id: 'bedrock/us-west-2/6-month-commitment/anthropic.claude-v2:1'
        mode: chat
        max_input_tokens: 100000
        max_output_tokens: 8191
      'us-west-2/anthropic.claude-instant-v1':
        litellm_id: 'bedrock/us-west-2/anthropic.claude-instant-v1'
        mode: chat
        max_input_tokens: 100000
        max_output_tokens: 8191
        input_cost_per_1k: 0.0008
        output_cost_per_1k: 0.0024
      'us-west-2/anthropic.claude-v1':
        litellm_id: 'bedrock/us-west-2/anthropic.claude-v1'
        mode: chat
        max_input_tokens: 100000
        max_output_tokens: 8191
        input_cost_per_1k: 0.008
        output_cost_per_1k: 0.024
      'us-west-2/anthropic.claude-v2:1':
        litellm_id: 'bedrock/us-west-2/anthropic.claude-v2:1'
        mode: chat
        max_input_tokens: 100000
        max_output_tokens: 8191
        input_cost_per_1k: 0.008
        output_cost_per_1k: 0.024
      'us-west-2/mistral.mistral-7b-instruct-v0:2':
        litellm_id: 'bedrock/us-west-2/mistral.mistral-7b-instruct-v0:2'
        mode: chat
        max_input_tokens: 32000
        max_output_tokens: 8191
        input_cost_per_1k: 0.00015
        output_cost_per_1k: 0.0002
      'us-west-2/mistral.mistral-large-2402-v1:0':
        litellm_id: 'bedrock/us-west-2/mistral.mistral-large-2402-v1:0'
        mode: chat
        max_input_tokens: 32000
        max_output_tokens: 8191
        input_cost_per_1k: 0.008
        output_cost_per_1k: 0.024
        capabilities: [function_calling]
      'us-west-2/mistral.mixtral-8x7b-instruct-v0:1':
        litellm_id: 'bedrock/us-west-2/mistral.mixtral-8x7b-instruct-v0:1'
        mode: chat
        max_input_tokens: 32000
        max_output_tokens: 8191
        input_cost_per_1k: 0.00045
        output_cost_per_1k: 0.0007
      'us.anthropic.claude-3-5-haiku-20241022-v1:0':
        litellm_id: 'bedrock/us.anthropic.claude-3-5-haiku-20241022-v1:0'
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 8192
        input_cost_per_1k: 0.0008
        output_cost_per_1k: 0.004
        capabilities: [function_calling, json_mode, prompt_caching]
      'us.anthropic.claude-3-5-haiku-20241022-v1:0':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 8192
        input_cost_per_1k: 0.0008
        output_cost_per_1k: 0.004
        capabilities: [function_calling, json_mode, prompt_caching]
      'us.anthropic.claude-3-5-sonnet-20240620-v1:0':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 4096
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.015
        capabilities: [vision, function_calling, json_mode]
      'us.anthropic.claude-3-5-sonnet-20241022-v2:0':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 8192
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.015
        capabilities: [vision, function_calling, json_mode, prompt_caching]
      'us.anthropic.claude-3-haiku-20240307-v1:0':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 4096
        input_cost_per_1k: 0.00025
        output_cost_per_1k: 0.00125
        capabilities: [vision, function_calling, json_mode]
      'us.anthropic.claude-3-opus-20240229-v1:0':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 4096
        input_cost_per_1k: 0.015
        output_cost_per_1k: 0.075
        capabilities: [vision, function_calling, json_mode]
      'us.anthropic.claude-3-sonnet-20240229-v1:0':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 4096
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.015
        capabilities: [vision, function_calling, json_mode]
      'us.meta.llama3-1-405b-instruct-v1:0':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 4096
        input_cost_per_1k: 0.00532
        output_cost_per_1k: 0.016
        capabilities: [function_calling]
      'us.meta.llama3-1-70b-instruct-v1:0':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 2048
        input_cost_per_1k: 0.00099
        output_cost_per_1k: 0.00099
        capabilities: [function_calling]
      'us.meta.llama3-1-8b-instruct-v1:0':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 2048
        input_cost_per_1k: 0.00022
        output_cost_per_1k: 0.00022
        capabilities: [function_calling]
      'us.meta.llama3-2-11b-instruct-v1:0':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 4096
        input_cost_per_1k: 0.00035
        output_cost_per_1k: 0.00035
        capabilities: [vision, function_calling]
      'us.meta.llama3-2-1b-instruct-v1:0':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 4096
        input_cost_per_1k: 0.0001
        output_cost_per_1k: 0.0001
        capabilities: [function_calling]
      'us.meta.llama3-2-3b-instruct-v1:0':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 4096
        input_cost_per_1k: 0.00015
        output_cost_per_1k: 0.00015
        capabilities: [function_calling]
      'us.meta.llama3-2-90b-instruct-v1:0':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 4096
        input_cost_per_1k: 0.002
        output_cost_per_1k: 0.002
        capabilities: [vision, function_calling]
      'us.twelvelabs.marengo-embed-2-7-v1:0':
        mode: embedding
        max_input_tokens: 77
        input_cost_per_1k: 0.07
      'us.twelvelabs.pegasus-1-2-v1:0':
        mode: chat
        output_cost_per_1k: 0.0075
  aws_polly:
    model_count: 4
    models:
      'generative':
        litellm_id: 'aws_polly/generative'
        mode: audio_speech
      'long-form':
        litellm_id: 'aws_polly/long-form'
        mode: audio_speech
      'neural':
        litellm_id: 'aws_polly/neural'
        mode: audio_speech
      'standard':
        litellm_id: 'aws_polly/standard'
        mode: audio_speech
  aws_sagemaker:
    model_count: 6
    models:
      'meta-textgeneration-llama-2-13b':
        litellm_id: 'sagemaker/meta-textgeneration-llama-2-13b'
        mode: completion
        max_input_tokens: 4096
        max_output_tokens: 4096
      'meta-textgeneration-llama-2-13b-f':
        litellm_id: 'sagemaker/meta-textgeneration-llama-2-13b-f'
        mode: chat
        max_input_tokens: 4096
        max_output_tokens: 4096
      'meta-textgeneration-llama-2-70b':
        litellm_id: 'sagemaker/meta-textgeneration-llama-2-70b'
        mode: completion
        max_input_tokens: 4096
        max_output_tokens: 4096
      'meta-textgeneration-llama-2-70b-b-f':
        litellm_id: 'sagemaker/meta-textgeneration-llama-2-70b-b-f'
        mode: chat
        max_input_tokens: 4096
        max_output_tokens: 4096
      'meta-textgeneration-llama-2-7b':
        litellm_id: 'sagemaker/meta-textgeneration-llama-2-7b'
        mode: completion
        max_input_tokens: 4096
        max_output_tokens: 4096
      'meta-textgeneration-llama-2-7b-f':
        litellm_id: 'sagemaker/meta-textgeneration-llama-2-7b-f'
        mode: chat
        max_input_tokens: 4096
        max_output_tokens: 4096
  azure_openai:
    model_count: 246
    models:
      'Cohere-embed-v3-english':
        litellm_id: 'azure_ai/Cohere-embed-v3-english'
        mode: embedding
        max_input_tokens: 512
        input_cost_per_1k: 0.0001
      'Cohere-embed-v3-multilingual':
        litellm_id: 'azure_ai/Cohere-embed-v3-multilingual'
        mode: embedding
        max_input_tokens: 512
        input_cost_per_1k: 0.0001
      'FLUX-1.1-pro':
        litellm_id: 'azure_ai/FLUX-1.1-pro'
        mode: image
      'FLUX.1-Kontext-pro':
        litellm_id: 'azure_ai/FLUX.1-Kontext-pro'
        mode: image
      'Llama-3.2-11B-Vision-Instruct':
        litellm_id: 'azure_ai/Llama-3.2-11B-Vision-Instruct'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 2048
        input_cost_per_1k: 0.00037
        output_cost_per_1k: 0.00037
        capabilities: [vision, function_calling]
      'Llama-3.2-90B-Vision-Instruct':
        litellm_id: 'azure_ai/Llama-3.2-90B-Vision-Instruct'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 2048
        input_cost_per_1k: 0.00204
        output_cost_per_1k: 0.00204
        capabilities: [vision, function_calling]
      'Llama-3.3-70B-Instruct':
        litellm_id: 'azure_ai/Llama-3.3-70B-Instruct'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 2048
        input_cost_per_1k: 0.00071
        output_cost_per_1k: 0.00071
        capabilities: [function_calling]
      'Llama-4-Maverick-17B-128E-Instruct-FP8':
        litellm_id: 'azure_ai/Llama-4-Maverick-17B-128E-Instruct-FP8'
        mode: chat
        max_input_tokens: 1000000
        max_output_tokens: 16384
        input_cost_per_1k: 0.00141
        output_cost_per_1k: 0.00035
        capabilities: [vision, function_calling]
      'Llama-4-Scout-17B-16E-Instruct':
        litellm_id: 'azure_ai/Llama-4-Scout-17B-16E-Instruct'
        mode: chat
        max_input_tokens: 10000000
        max_output_tokens: 16384
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.00078
        capabilities: [vision, function_calling]
      'MAI-DS-R1':
        litellm_id: 'azure_ai/MAI-DS-R1'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 8192
        input_cost_per_1k: 0.00135
        output_cost_per_1k: 0.0054
        capabilities: [reasoning]
      'Meta-Llama-3-70B-Instruct':
        litellm_id: 'azure_ai/Meta-Llama-3-70B-Instruct'
        mode: chat
        max_input_tokens: 8192
        max_output_tokens: 2048
        input_cost_per_1k: 0.0011
        output_cost_per_1k: 0.00037
      'Meta-Llama-3.1-405B-Instruct':
        litellm_id: 'azure_ai/Meta-Llama-3.1-405B-Instruct'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 2048
        input_cost_per_1k: 0.00533
        output_cost_per_1k: 0.016
      'Meta-Llama-3.1-70B-Instruct':
        litellm_id: 'azure_ai/Meta-Llama-3.1-70B-Instruct'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 2048
        input_cost_per_1k: 0.00268
        output_cost_per_1k: 0.00354
      'Meta-Llama-3.1-8B-Instruct':
        litellm_id: 'azure_ai/Meta-Llama-3.1-8B-Instruct'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 2048
        input_cost_per_1k: 0.0003
        output_cost_per_1k: 0.00061
      'Phi-3-medium-128k-instruct':
        litellm_id: 'azure_ai/Phi-3-medium-128k-instruct'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 4096
        input_cost_per_1k: 0.00017
        output_cost_per_1k: 0.00068
      'Phi-3-medium-4k-instruct':
        litellm_id: 'azure_ai/Phi-3-medium-4k-instruct'
        mode: chat
        max_input_tokens: 4096
        max_output_tokens: 4096
        input_cost_per_1k: 0.00017
        output_cost_per_1k: 0.00068
      'Phi-3-mini-128k-instruct':
        litellm_id: 'azure_ai/Phi-3-mini-128k-instruct'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 4096
        input_cost_per_1k: 0.00013
        output_cost_per_1k: 0.00052
      'Phi-3-mini-4k-instruct':
        litellm_id: 'azure_ai/Phi-3-mini-4k-instruct'
        mode: chat
        max_input_tokens: 4096
        max_output_tokens: 4096
        input_cost_per_1k: 0.00013
        output_cost_per_1k: 0.00052
      'Phi-3-small-128k-instruct':
        litellm_id: 'azure_ai/Phi-3-small-128k-instruct'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 4096
        input_cost_per_1k: 0.00015
        output_cost_per_1k: 0.0006
      'Phi-3-small-8k-instruct':
        litellm_id: 'azure_ai/Phi-3-small-8k-instruct'
        mode: chat
        max_input_tokens: 8192
        max_output_tokens: 4096
        input_cost_per_1k: 0.00015
        output_cost_per_1k: 0.0006
      'Phi-3.5-MoE-instruct':
        litellm_id: 'azure_ai/Phi-3.5-MoE-instruct'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 4096
        input_cost_per_1k: 0.00016
        output_cost_per_1k: 0.00064
      'Phi-3.5-mini-instruct':
        litellm_id: 'azure_ai/Phi-3.5-mini-instruct'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 4096
        input_cost_per_1k: 0.00013
        output_cost_per_1k: 0.00052
      'Phi-3.5-vision-instruct':
        litellm_id: 'azure_ai/Phi-3.5-vision-instruct'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 4096
        input_cost_per_1k: 0.00013
        output_cost_per_1k: 0.00052
        capabilities: [vision]
      'Phi-4':
        litellm_id: 'azure_ai/Phi-4'
        mode: chat
        max_input_tokens: 16384
        max_output_tokens: 16384
        input_cost_per_1k: 0.000125
        output_cost_per_1k: 0.0005
        capabilities: [function_calling]
      'Phi-4-mini-instruct':
        litellm_id: 'azure_ai/Phi-4-mini-instruct'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 4096
        input_cost_per_1k: 7.5e-05
        output_cost_per_1k: 0.0003
        capabilities: [function_calling]
      'Phi-4-mini-reasoning':
        litellm_id: 'azure_ai/Phi-4-mini-reasoning'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 4096
        input_cost_per_1k: 8e-05
        output_cost_per_1k: 0.00032
        capabilities: [function_calling]
      'Phi-4-multimodal-instruct':
        litellm_id: 'azure_ai/Phi-4-multimodal-instruct'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 4096
        input_cost_per_1k: 8e-05
        output_cost_per_1k: 0.00032
        capabilities: [vision, function_calling, audio_input]
      'Phi-4-reasoning':
        litellm_id: 'azure_ai/Phi-4-reasoning'
        mode: chat
        max_input_tokens: 32768
        max_output_tokens: 4096
        input_cost_per_1k: 0.000125
        output_cost_per_1k: 0.0005
        capabilities: [function_calling, reasoning]
      'ada':
        litellm_id: 'azure/ada'
        mode: embedding
        max_input_tokens: 8191
        input_cost_per_1k: 0.0001
      'claude-haiku-4-5':
        litellm_id: 'azure_ai/claude-haiku-4-5'
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 64000
        input_cost_per_1k: 0.001
        output_cost_per_1k: 0.005
        capabilities: [vision, function_calling, json_mode, prompt_caching, reasoning]
      'claude-opus-4-1':
        litellm_id: 'azure_ai/claude-opus-4-1'
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 32000
        input_cost_per_1k: 0.015
        output_cost_per_1k: 0.075
        capabilities: [vision, function_calling, json_mode, prompt_caching, reasoning]
      'claude-sonnet-4-5':
        litellm_id: 'azure_ai/claude-sonnet-4-5'
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 64000
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.015
        capabilities: [vision, function_calling, json_mode, prompt_caching, reasoning]
      'codex-mini':
        litellm_id: 'azure/codex-mini'
        mode: responses
        max_input_tokens: 200000
        max_output_tokens: 100000
        input_cost_per_1k: 0.0015
        output_cost_per_1k: 0.006
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching, reasoning]
      'cohere-rerank-v3-english':
        litellm_id: 'azure_ai/cohere-rerank-v3-english'
        mode: rerank
        max_input_tokens: 4096
        max_output_tokens: 4096
      'cohere-rerank-v3-multilingual':
        litellm_id: 'azure_ai/cohere-rerank-v3-multilingual'
        mode: rerank
        max_input_tokens: 4096
        max_output_tokens: 4096
      'cohere-rerank-v3.5':
        litellm_id: 'azure_ai/cohere-rerank-v3.5'
        mode: rerank
        max_input_tokens: 4096
        max_output_tokens: 4096
      'cohere-rerank-v4.0-fast':
        litellm_id: 'azure_ai/cohere-rerank-v4.0-fast'
        mode: rerank
        max_input_tokens: 32768
        max_output_tokens: 32768
      'cohere-rerank-v4.0-pro':
        litellm_id: 'azure_ai/cohere-rerank-v4.0-pro'
        mode: rerank
        max_input_tokens: 32768
        max_output_tokens: 32768
      'command-r-plus':
        litellm_id: 'azure/command-r-plus'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 4096
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.015
        capabilities: [function_calling]
      'computer-use-preview':
        litellm_id: 'azure/computer-use-preview'
        mode: chat
        max_input_tokens: 8192
        max_output_tokens: 1024
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.012
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, reasoning]
      'computer-use-preview':
        mode: chat
        max_input_tokens: 8192
        max_output_tokens: 1024
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.012
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, reasoning]
      'container':
        litellm_id: 'azure/container'
        mode: chat
      'deepseek-r1':
        litellm_id: 'azure_ai/deepseek-r1'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 8192
        input_cost_per_1k: 0.00135
        output_cost_per_1k: 0.0054
        capabilities: [reasoning]
      'deepseek-v3':
        litellm_id: 'azure_ai/deepseek-v3'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 8192
        input_cost_per_1k: 0.00114
        output_cost_per_1k: 0.00456
      'deepseek-v3-0324':
        litellm_id: 'azure_ai/deepseek-v3-0324'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 8192
        input_cost_per_1k: 0.00114
        output_cost_per_1k: 0.00456
        capabilities: [function_calling]
      'deepseek-v3.2':
        litellm_id: 'azure_ai/deepseek-v3.2'
        mode: chat
        max_input_tokens: 163840
        max_output_tokens: 163840
        input_cost_per_1k: 0.00058
        output_cost_per_1k: 0.00168
        capabilities: [function_calling, prompt_caching, reasoning]
      'deepseek-v3.2-speciale':
        litellm_id: 'azure_ai/deepseek-v3.2-speciale'
        mode: chat
        max_input_tokens: 163840
        max_output_tokens: 163840
        input_cost_per_1k: 0.00058
        output_cost_per_1k: 0.00168
        capabilities: [function_calling, prompt_caching, reasoning]
      'doc-intelligence/prebuilt-document':
        litellm_id: 'azure_ai/doc-intelligence/prebuilt-document'
        mode: ocr
      'doc-intelligence/prebuilt-layout':
        litellm_id: 'azure_ai/doc-intelligence/prebuilt-layout'
        mode: ocr
      'doc-intelligence/prebuilt-read':
        litellm_id: 'azure_ai/doc-intelligence/prebuilt-read'
        mode: ocr
      'embed-v-4-0':
        litellm_id: 'azure_ai/embed-v-4-0'
        mode: embedding
        max_input_tokens: 128000
        input_cost_per_1k: 0.00012
      'eu/gpt-4o-2024-08-06':
        litellm_id: 'azure/eu/gpt-4o-2024-08-06'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 16384
        input_cost_per_1k: 0.00275
        output_cost_per_1k: 0.011
        capabilities: [vision, function_calling, parallel_function_calling, json_mode, prompt_caching]
        deprecated: true
        deprecation_date: '2026-02-27'
      'eu/gpt-4o-2024-11-20':
        litellm_id: 'azure/eu/gpt-4o-2024-11-20'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 16384
        input_cost_per_1k: 0.00275
        output_cost_per_1k: 0.011
        capabilities: [vision, function_calling, parallel_function_calling, json_mode]
        deprecated: true
        deprecation_date: '2026-03-01'
      'eu/gpt-4o-mini-2024-07-18':
        litellm_id: 'azure/eu/gpt-4o-mini-2024-07-18'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 16384
        input_cost_per_1k: 0.000165
        output_cost_per_1k: 0.00066
        capabilities: [vision, function_calling, parallel_function_calling, json_mode, prompt_caching]
      'eu/gpt-4o-mini-realtime-preview-2024-12-17':
        litellm_id: 'azure/eu/gpt-4o-mini-realtime-preview-2024-12-17'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 4096
        input_cost_per_1k: 0.00066
        output_cost_per_1k: 0.00264
        capabilities: [function_calling, parallel_function_calling, system_messages, audio_input, audio_output]
      'eu/gpt-4o-realtime-preview-2024-10-01':
        litellm_id: 'azure/eu/gpt-4o-realtime-preview-2024-10-01'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 4096
        input_cost_per_1k: 0.0055
        output_cost_per_1k: 0.022
        capabilities: [function_calling, parallel_function_calling, system_messages, audio_input, audio_output]
      'eu/gpt-4o-realtime-preview-2024-12-17':
        litellm_id: 'azure/eu/gpt-4o-realtime-preview-2024-12-17'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 4096
        input_cost_per_1k: 0.0055
        output_cost_per_1k: 0.022
        capabilities: [function_calling, parallel_function_calling, system_messages, audio_input, audio_output]
      'eu/gpt-5-2025-08-07':
        litellm_id: 'azure/eu/gpt-5-2025-08-07'
        mode: chat
        max_input_tokens: 272000
        max_output_tokens: 128000
        input_cost_per_1k: 0.001375
        output_cost_per_1k: 0.011
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching, reasoning]
      'eu/gpt-5-mini-2025-08-07':
        litellm_id: 'azure/eu/gpt-5-mini-2025-08-07'
        mode: chat
        max_input_tokens: 272000
        max_output_tokens: 128000
        input_cost_per_1k: 0.000275
        output_cost_per_1k: 0.0022
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching, reasoning]
      'eu/gpt-5-nano-2025-08-07':
        litellm_id: 'azure/eu/gpt-5-nano-2025-08-07'
        mode: chat
        max_input_tokens: 272000
        max_output_tokens: 128000
        input_cost_per_1k: 5.5e-05
        output_cost_per_1k: 0.00044
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching, reasoning]
      'eu/gpt-5.1':
        litellm_id: 'azure/eu/gpt-5.1'
        mode: chat
        max_input_tokens: 272000
        max_output_tokens: 128000
        input_cost_per_1k: 0.00138
        output_cost_per_1k: 0.011
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching, reasoning]
      'eu/gpt-5.1-chat':
        litellm_id: 'azure/eu/gpt-5.1-chat'
        mode: chat
        max_input_tokens: 272000
        max_output_tokens: 128000
        input_cost_per_1k: 0.00138
        output_cost_per_1k: 0.011
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching, reasoning]
      'eu/gpt-5.1-codex':
        litellm_id: 'azure/eu/gpt-5.1-codex'
        mode: responses
        max_input_tokens: 272000
        max_output_tokens: 128000
        input_cost_per_1k: 0.00138
        output_cost_per_1k: 0.011
        capabilities: [vision, function_calling, parallel_function_calling, json_mode, prompt_caching, reasoning]
      'eu/gpt-5.1-codex-mini':
        litellm_id: 'azure/eu/gpt-5.1-codex-mini'
        mode: responses
        max_input_tokens: 272000
        max_output_tokens: 128000
        input_cost_per_1k: 0.000275
        output_cost_per_1k: 0.0022
        capabilities: [vision, function_calling, parallel_function_calling, json_mode, prompt_caching, reasoning]
      'eu/o1-2024-12-17':
        litellm_id: 'azure/eu/o1-2024-12-17'
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 100000
        input_cost_per_1k: 0.0165
        output_cost_per_1k: 0.066
        capabilities: [vision, function_calling, parallel_function_calling, prompt_caching]
      'eu/o1-mini-2024-09-12':
        litellm_id: 'azure/eu/o1-mini-2024-09-12'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 65536
        input_cost_per_1k: 0.00121
        output_cost_per_1k: 0.00484
        capabilities: [function_calling, parallel_function_calling, prompt_caching]
      'eu/o1-preview-2024-09-12':
        litellm_id: 'azure/eu/o1-preview-2024-09-12'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 32768
        input_cost_per_1k: 0.0165
        output_cost_per_1k: 0.066
        capabilities: [function_calling, parallel_function_calling, prompt_caching]
      'eu/o3-mini-2025-01-31':
        litellm_id: 'azure/eu/o3-mini-2025-01-31'
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 100000
        input_cost_per_1k: 0.00121
        output_cost_per_1k: 0.00484
        capabilities: [prompt_caching, reasoning]
      'global-standard/gpt-4o-2024-08-06':
        litellm_id: 'azure/global-standard/gpt-4o-2024-08-06'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 16384
        input_cost_per_1k: 0.0025
        output_cost_per_1k: 0.01
        capabilities: [vision, function_calling, parallel_function_calling, json_mode, prompt_caching]
        deprecated: true
        deprecation_date: '2026-02-27'
      'global-standard/gpt-4o-2024-11-20':
        litellm_id: 'azure/global-standard/gpt-4o-2024-11-20'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 16384
        input_cost_per_1k: 0.0025
        output_cost_per_1k: 0.01
        capabilities: [vision, function_calling, parallel_function_calling, json_mode]
        deprecated: true
        deprecation_date: '2026-03-01'
      'global-standard/gpt-4o-mini':
        litellm_id: 'azure/global-standard/gpt-4o-mini'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 16384
        input_cost_per_1k: 0.00015
        output_cost_per_1k: 0.0006
        capabilities: [vision, function_calling, parallel_function_calling, json_mode]
      'global/gpt-4o-2024-08-06':
        litellm_id: 'azure/global/gpt-4o-2024-08-06'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 16384
        input_cost_per_1k: 0.0025
        output_cost_per_1k: 0.01
        capabilities: [vision, function_calling, parallel_function_calling, json_mode, prompt_caching]
        deprecated: true
        deprecation_date: '2026-02-27'
      'global/gpt-4o-2024-11-20':
        litellm_id: 'azure/global/gpt-4o-2024-11-20'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 16384
        input_cost_per_1k: 0.0025
        output_cost_per_1k: 0.01
        capabilities: [vision, function_calling, parallel_function_calling, json_mode, prompt_caching]
        deprecated: true
        deprecation_date: '2026-03-01'
      'global/gpt-5.1':
        litellm_id: 'azure/global/gpt-5.1'
        mode: chat
        max_input_tokens: 272000
        max_output_tokens: 128000
        input_cost_per_1k: 0.00125
        output_cost_per_1k: 0.01
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching, reasoning]
      'global/gpt-5.1-chat':
        litellm_id: 'azure/global/gpt-5.1-chat'
        mode: chat
        max_input_tokens: 272000
        max_output_tokens: 128000
        input_cost_per_1k: 0.00125
        output_cost_per_1k: 0.01
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching, reasoning]
      'global/gpt-5.1-codex':
        litellm_id: 'azure/global/gpt-5.1-codex'
        mode: responses
        max_input_tokens: 272000
        max_output_tokens: 128000
        input_cost_per_1k: 0.00125
        output_cost_per_1k: 0.01
        capabilities: [vision, function_calling, parallel_function_calling, json_mode, prompt_caching, reasoning]
      'global/gpt-5.1-codex-mini':
        litellm_id: 'azure/global/gpt-5.1-codex-mini'
        mode: responses
        max_input_tokens: 272000
        max_output_tokens: 128000
        input_cost_per_1k: 0.00025
        output_cost_per_1k: 0.002
        capabilities: [vision, function_calling, parallel_function_calling, json_mode, prompt_caching, reasoning]
      'global/grok-3':
        litellm_id: 'azure_ai/global/grok-3'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.015
        capabilities: [function_calling, web_search]
      'global/grok-3-mini':
        litellm_id: 'azure_ai/global/grok-3-mini'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.00025
        output_cost_per_1k: 0.00127
        capabilities: [function_calling, reasoning, web_search]
      'gpt-3.5-turbo':
        litellm_id: 'azure/gpt-3.5-turbo'
        mode: chat
        max_input_tokens: 4097
        max_output_tokens: 4096
        input_cost_per_1k: 0.0005
        output_cost_per_1k: 0.0015
        capabilities: [function_calling]
      'gpt-3.5-turbo-0125':
        litellm_id: 'azure/gpt-3.5-turbo-0125'
        mode: chat
        max_input_tokens: 16384
        max_output_tokens: 4096
        input_cost_per_1k: 0.0005
        output_cost_per_1k: 0.0015
        capabilities: [function_calling, parallel_function_calling]
        deprecated: true
        deprecation_date: '2025-03-31'
      'gpt-35-turbo':
        litellm_id: 'azure/gpt-35-turbo'
        mode: chat
        max_input_tokens: 4097
        max_output_tokens: 4096
        input_cost_per_1k: 0.0005
        output_cost_per_1k: 0.0015
        capabilities: [function_calling]
      'gpt-35-turbo-0125':
        litellm_id: 'azure/gpt-35-turbo-0125'
        mode: chat
        max_input_tokens: 16384
        max_output_tokens: 4096
        input_cost_per_1k: 0.0005
        output_cost_per_1k: 0.0015
        capabilities: [function_calling, parallel_function_calling]
        deprecated: true
        deprecation_date: '2025-05-31'
      'gpt-35-turbo-0301':
        litellm_id: 'azure/gpt-35-turbo-0301'
        mode: chat
        max_input_tokens: 4097
        max_output_tokens: 4096
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.002
        capabilities: [function_calling, parallel_function_calling]
        deprecated: true
        deprecation_date: '2025-02-13'
      'gpt-35-turbo-0613':
        litellm_id: 'azure/gpt-35-turbo-0613'
        mode: chat
        max_input_tokens: 4097
        max_output_tokens: 4096
        input_cost_per_1k: 0.0015
        output_cost_per_1k: 0.002
        capabilities: [function_calling, parallel_function_calling]
        deprecated: true
        deprecation_date: '2025-02-13'
      'gpt-35-turbo-1106':
        litellm_id: 'azure/gpt-35-turbo-1106'
        mode: chat
        max_input_tokens: 16384
        max_output_tokens: 4096
        input_cost_per_1k: 0.001
        output_cost_per_1k: 0.002
        capabilities: [function_calling, parallel_function_calling]
        deprecated: true
        deprecation_date: '2025-03-31'
      'gpt-35-turbo-16k':
        litellm_id: 'azure/gpt-35-turbo-16k'
        mode: chat
        max_input_tokens: 16385
        max_output_tokens: 4096
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.004
      'gpt-35-turbo-16k-0613':
        litellm_id: 'azure/gpt-35-turbo-16k-0613'
        mode: chat
        max_input_tokens: 16385
        max_output_tokens: 4096
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.004
        capabilities: [function_calling]
      'gpt-4':
        litellm_id: 'azure/gpt-4'
        mode: chat
        max_input_tokens: 8192
        max_output_tokens: 4096
        input_cost_per_1k: 0.03
        output_cost_per_1k: 0.06
        capabilities: [function_calling]
      'gpt-4-0125-preview':
        litellm_id: 'azure/gpt-4-0125-preview'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 4096
        input_cost_per_1k: 0.01
        output_cost_per_1k: 0.03
        capabilities: [function_calling, parallel_function_calling]
      'gpt-4-0613':
        litellm_id: 'azure/gpt-4-0613'
        mode: chat
        max_input_tokens: 8192
        max_output_tokens: 4096
        input_cost_per_1k: 0.03
        output_cost_per_1k: 0.06
        capabilities: [function_calling]
      'gpt-4-1106-preview':
        litellm_id: 'azure/gpt-4-1106-preview'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 4096
        input_cost_per_1k: 0.01
        output_cost_per_1k: 0.03
        capabilities: [function_calling, parallel_function_calling]
      'gpt-4-32k':
        litellm_id: 'azure/gpt-4-32k'
        mode: chat
        max_input_tokens: 32768
        max_output_tokens: 4096
        input_cost_per_1k: 0.06
        output_cost_per_1k: 0.12
      'gpt-4-32k-0613':
        litellm_id: 'azure/gpt-4-32k-0613'
        mode: chat
        max_input_tokens: 32768
        max_output_tokens: 4096
        input_cost_per_1k: 0.06
        output_cost_per_1k: 0.12
      'gpt-4-turbo':
        litellm_id: 'azure/gpt-4-turbo'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 4096
        input_cost_per_1k: 0.01
        output_cost_per_1k: 0.03
        capabilities: [function_calling, parallel_function_calling]
      'gpt-4-turbo-2024-04-09':
        litellm_id: 'azure/gpt-4-turbo-2024-04-09'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 4096
        input_cost_per_1k: 0.01
        output_cost_per_1k: 0.03
        capabilities: [vision, function_calling, parallel_function_calling]
      'gpt-4-turbo-vision-preview':
        litellm_id: 'azure/gpt-4-turbo-vision-preview'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 4096
        input_cost_per_1k: 0.01
        output_cost_per_1k: 0.03
        capabilities: [vision]
      'gpt-4.1':
        litellm_id: 'azure/gpt-4.1'
        mode: chat
        max_input_tokens: 1047576
        max_output_tokens: 32768
        input_cost_per_1k: 0.002
        output_cost_per_1k: 0.008
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching]
      'gpt-4.1-2025-04-14':
        litellm_id: 'azure/gpt-4.1-2025-04-14'
        mode: chat
        max_input_tokens: 1047576
        max_output_tokens: 32768
        input_cost_per_1k: 0.002
        output_cost_per_1k: 0.008
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching]
        deprecated: true
        deprecation_date: '2026-11-04'
      'gpt-4.1-mini':
        litellm_id: 'azure/gpt-4.1-mini'
        mode: chat
        max_input_tokens: 1047576
        max_output_tokens: 32768
        input_cost_per_1k: 0.0004
        output_cost_per_1k: 0.0016
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching]
      'gpt-4.1-mini-2025-04-14':
        litellm_id: 'azure/gpt-4.1-mini-2025-04-14'
        mode: chat
        max_input_tokens: 1047576
        max_output_tokens: 32768
        input_cost_per_1k: 0.0004
        output_cost_per_1k: 0.0016
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching]
        deprecated: true
        deprecation_date: '2026-11-04'
      'gpt-4.1-nano':
        litellm_id: 'azure/gpt-4.1-nano'
        mode: chat
        max_input_tokens: 1047576
        max_output_tokens: 32768
        input_cost_per_1k: 0.0001
        output_cost_per_1k: 0.0004
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching]
      'gpt-4.1-nano-2025-04-14':
        litellm_id: 'azure/gpt-4.1-nano-2025-04-14'
        mode: chat
        max_input_tokens: 1047576
        max_output_tokens: 32768
        input_cost_per_1k: 0.0001
        output_cost_per_1k: 0.0004
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching]
        deprecated: true
        deprecation_date: '2026-11-04'
      'gpt-4.5-preview':
        litellm_id: 'azure/gpt-4.5-preview'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 16384
        input_cost_per_1k: 0.075
        output_cost_per_1k: 0.15
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching]
      'gpt-4o':
        litellm_id: 'azure/gpt-4o'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 16384
        input_cost_per_1k: 0.0025
        output_cost_per_1k: 0.01
        capabilities: [vision, function_calling, parallel_function_calling, json_mode, prompt_caching]
      'gpt-4o-2024-05-13':
        litellm_id: 'azure/gpt-4o-2024-05-13'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 4096
        input_cost_per_1k: 0.005
        output_cost_per_1k: 0.015
        capabilities: [vision, function_calling, parallel_function_calling, prompt_caching]
      'gpt-4o-2024-08-06':
        litellm_id: 'azure/gpt-4o-2024-08-06'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 16384
        input_cost_per_1k: 0.0025
        output_cost_per_1k: 0.01
        capabilities: [vision, function_calling, parallel_function_calling, json_mode, prompt_caching]
        deprecated: true
        deprecation_date: '2026-02-27'
      'gpt-4o-2024-11-20':
        litellm_id: 'azure/gpt-4o-2024-11-20'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 16384
        input_cost_per_1k: 0.00275
        output_cost_per_1k: 0.011
        capabilities: [vision, function_calling, parallel_function_calling, json_mode, prompt_caching]
        deprecated: true
        deprecation_date: '2026-03-01'
      'gpt-4o-audio-preview-2024-12-17':
        litellm_id: 'azure/gpt-4o-audio-preview-2024-12-17'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 16384
        input_cost_per_1k: 0.0025
        output_cost_per_1k: 0.01
        capabilities: [function_calling, parallel_function_calling, system_messages]
      'gpt-4o-mini':
        litellm_id: 'azure/gpt-4o-mini'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 16384
        input_cost_per_1k: 0.000165
        output_cost_per_1k: 0.00066
        capabilities: [vision, function_calling, parallel_function_calling, json_mode, prompt_caching]
      'gpt-4o-mini-2024-07-18':
        litellm_id: 'azure/gpt-4o-mini-2024-07-18'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 16384
        input_cost_per_1k: 0.000165
        output_cost_per_1k: 0.00066
        capabilities: [vision, function_calling, parallel_function_calling, json_mode, prompt_caching]
      'gpt-4o-mini-audio-preview-2024-12-17':
        litellm_id: 'azure/gpt-4o-mini-audio-preview-2024-12-17'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 16384
        input_cost_per_1k: 0.0025
        output_cost_per_1k: 0.01
        capabilities: [function_calling, parallel_function_calling, system_messages]
      'gpt-4o-mini-realtime-preview-2024-12-17':
        litellm_id: 'azure/gpt-4o-mini-realtime-preview-2024-12-17'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 4096
        input_cost_per_1k: 0.0006
        output_cost_per_1k: 0.0024
        capabilities: [function_calling, parallel_function_calling, system_messages, audio_input, audio_output]
      'gpt-4o-mini-transcribe':
        litellm_id: 'azure/gpt-4o-mini-transcribe'
        mode: audio_transcription
        max_input_tokens: 16000
        max_output_tokens: 2000
        input_cost_per_1k: 0.00125
        output_cost_per_1k: 0.005
      'gpt-4o-mini-tts':
        litellm_id: 'azure/gpt-4o-mini-tts'
        mode: audio_speech
        input_cost_per_1k: 0.0025
        output_cost_per_1k: 0.01
      'gpt-4o-realtime-preview-2024-10-01':
        litellm_id: 'azure/gpt-4o-realtime-preview-2024-10-01'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 4096
        input_cost_per_1k: 0.005
        output_cost_per_1k: 0.02
        capabilities: [function_calling, parallel_function_calling, system_messages, audio_input, audio_output]
      'gpt-4o-realtime-preview-2024-12-17':
        litellm_id: 'azure/gpt-4o-realtime-preview-2024-12-17'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 4096
        input_cost_per_1k: 0.005
        output_cost_per_1k: 0.02
        capabilities: [function_calling, parallel_function_calling, system_messages, audio_input, audio_output]
      'gpt-4o-transcribe':
        litellm_id: 'azure/gpt-4o-transcribe'
        mode: audio_transcription
        max_input_tokens: 16000
        max_output_tokens: 2000
        input_cost_per_1k: 0.0025
        output_cost_per_1k: 0.01
      'gpt-4o-transcribe-diarize':
        litellm_id: 'azure/gpt-4o-transcribe-diarize'
        mode: audio_transcription
        max_input_tokens: 16000
        max_output_tokens: 2000
        input_cost_per_1k: 0.0025
        output_cost_per_1k: 0.01
      'gpt-5':
        litellm_id: 'azure/gpt-5'
        mode: chat
        max_input_tokens: 272000
        max_output_tokens: 128000
        input_cost_per_1k: 0.00125
        output_cost_per_1k: 0.01
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching, reasoning]
      'gpt-5-2025-08-07':
        litellm_id: 'azure/gpt-5-2025-08-07'
        mode: chat
        max_input_tokens: 272000
        max_output_tokens: 128000
        input_cost_per_1k: 0.00125
        output_cost_per_1k: 0.01
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching, reasoning]
      'gpt-5-chat':
        litellm_id: 'azure/gpt-5-chat'
        mode: chat
        max_input_tokens: 272000
        max_output_tokens: 128000
        input_cost_per_1k: 0.00125
        output_cost_per_1k: 0.01
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching, reasoning]
      'gpt-5-chat-latest':
        litellm_id: 'azure/gpt-5-chat-latest'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 16384
        input_cost_per_1k: 0.00125
        output_cost_per_1k: 0.01
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching, reasoning]
      'gpt-5-codex':
        litellm_id: 'azure/gpt-5-codex'
        mode: responses
        max_input_tokens: 272000
        max_output_tokens: 128000
        input_cost_per_1k: 0.00125
        output_cost_per_1k: 0.01
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching, reasoning]
      'gpt-5-mini':
        litellm_id: 'azure/gpt-5-mini'
        mode: chat
        max_input_tokens: 272000
        max_output_tokens: 128000
        input_cost_per_1k: 0.00025
        output_cost_per_1k: 0.002
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching, reasoning]
      'gpt-5-mini-2025-08-07':
        litellm_id: 'azure/gpt-5-mini-2025-08-07'
        mode: chat
        max_input_tokens: 272000
        max_output_tokens: 128000
        input_cost_per_1k: 0.00025
        output_cost_per_1k: 0.002
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching, reasoning]
      'gpt-5-nano':
        litellm_id: 'azure/gpt-5-nano'
        mode: chat
        max_input_tokens: 272000
        max_output_tokens: 128000
        input_cost_per_1k: 5e-05
        output_cost_per_1k: 0.0004
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching, reasoning]
      'gpt-5-nano-2025-08-07':
        litellm_id: 'azure/gpt-5-nano-2025-08-07'
        mode: chat
        max_input_tokens: 272000
        max_output_tokens: 128000
        input_cost_per_1k: 5e-05
        output_cost_per_1k: 0.0004
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching, reasoning]
      'gpt-5-pro':
        litellm_id: 'azure/gpt-5-pro'
        mode: responses
        max_input_tokens: 272000
        max_output_tokens: 128000
        input_cost_per_1k: 0.015
        output_cost_per_1k: 0.12
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching, reasoning]
      'gpt-5.1':
        litellm_id: 'azure/gpt-5.1'
        mode: chat
        max_input_tokens: 272000
        max_output_tokens: 128000
        input_cost_per_1k: 0.00125
        output_cost_per_1k: 0.01
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching, reasoning]
      'gpt-5.1-2025-11-13':
        litellm_id: 'azure/gpt-5.1-2025-11-13'
        mode: chat
        max_input_tokens: 272000
        max_output_tokens: 128000
        input_cost_per_1k: 0.00125
        output_cost_per_1k: 0.01
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching, reasoning]
      'gpt-5.1-chat':
        litellm_id: 'azure/gpt-5.1-chat'
        mode: chat
        max_input_tokens: 272000
        max_output_tokens: 128000
        input_cost_per_1k: 0.00125
        output_cost_per_1k: 0.01
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching, reasoning]
      'gpt-5.1-chat-2025-11-13':
        litellm_id: 'azure/gpt-5.1-chat-2025-11-13'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 16384
        input_cost_per_1k: 0.00125
        output_cost_per_1k: 0.01
        capabilities: [vision, system_messages, json_mode, prompt_caching, reasoning]
      'gpt-5.1-codex':
        litellm_id: 'azure/gpt-5.1-codex'
        mode: responses
        max_input_tokens: 272000
        max_output_tokens: 128000
        input_cost_per_1k: 0.00125
        output_cost_per_1k: 0.01
        capabilities: [vision, function_calling, parallel_function_calling, json_mode, prompt_caching, reasoning]
      'gpt-5.1-codex-2025-11-13':
        litellm_id: 'azure/gpt-5.1-codex-2025-11-13'
        mode: responses
        max_input_tokens: 272000
        max_output_tokens: 128000
        input_cost_per_1k: 0.00125
        output_cost_per_1k: 0.01
        capabilities: [vision, function_calling, parallel_function_calling, json_mode, prompt_caching, reasoning]
      'gpt-5.1-codex-max':
        litellm_id: 'azure/gpt-5.1-codex-max'
        mode: responses
        max_input_tokens: 400000
        max_output_tokens: 128000
        input_cost_per_1k: 0.00125
        output_cost_per_1k: 0.01
        capabilities: [vision, function_calling, parallel_function_calling, json_mode, prompt_caching, reasoning]
      'gpt-5.1-codex-mini':
        litellm_id: 'azure/gpt-5.1-codex-mini'
        mode: responses
        max_input_tokens: 272000
        max_output_tokens: 128000
        input_cost_per_1k: 0.00025
        output_cost_per_1k: 0.002
        capabilities: [vision, function_calling, parallel_function_calling, json_mode, prompt_caching, reasoning]
      'gpt-5.1-codex-mini-2025-11-13':
        litellm_id: 'azure/gpt-5.1-codex-mini-2025-11-13'
        mode: responses
        max_input_tokens: 272000
        max_output_tokens: 128000
        input_cost_per_1k: 0.00025
        output_cost_per_1k: 0.002
        capabilities: [vision, function_calling, parallel_function_calling, json_mode, prompt_caching, reasoning]
      'gpt-5.2':
        litellm_id: 'azure/gpt-5.2'
        mode: chat
        max_input_tokens: 400000
        max_output_tokens: 128000
        input_cost_per_1k: 0.00175
        output_cost_per_1k: 0.014
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching, reasoning]
      'gpt-5.2-2025-12-11':
        litellm_id: 'azure/gpt-5.2-2025-12-11'
        mode: chat
        max_input_tokens: 400000
        max_output_tokens: 128000
        input_cost_per_1k: 0.00175
        output_cost_per_1k: 0.014
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching, reasoning]
      'gpt-5.2-chat':
        litellm_id: 'azure/gpt-5.2-chat'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 16384
        input_cost_per_1k: 0.00175
        output_cost_per_1k: 0.014
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching, reasoning]
      'gpt-5.2-chat-2025-12-11':
        litellm_id: 'azure/gpt-5.2-chat-2025-12-11'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 16384
        input_cost_per_1k: 0.00175
        output_cost_per_1k: 0.014
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching, reasoning]
      'gpt-5.2-pro':
        litellm_id: 'azure/gpt-5.2-pro'
        mode: responses
        max_input_tokens: 400000
        max_output_tokens: 128000
        input_cost_per_1k: 0.021
        output_cost_per_1k: 0.168
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching, reasoning, web_search]
      'gpt-5.2-pro-2025-12-11':
        litellm_id: 'azure/gpt-5.2-pro-2025-12-11'
        mode: responses
        max_input_tokens: 400000
        max_output_tokens: 128000
        input_cost_per_1k: 0.021
        output_cost_per_1k: 0.168
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching, reasoning, web_search]
      'gpt-audio-2025-08-28':
        litellm_id: 'azure/gpt-audio-2025-08-28'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 16384
        input_cost_per_1k: 0.0025
        output_cost_per_1k: 0.01
        capabilities: [function_calling, parallel_function_calling, system_messages]
      'gpt-audio-mini-2025-10-06':
        litellm_id: 'azure/gpt-audio-mini-2025-10-06'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 16384
        input_cost_per_1k: 0.0006
        output_cost_per_1k: 0.0024
        capabilities: [function_calling, parallel_function_calling, system_messages]
      'gpt-image-1':
        litellm_id: 'azure/gpt-image-1'
        mode: image
      'gpt-image-1-mini':
        litellm_id: 'azure/gpt-image-1-mini'
        mode: image
      'gpt-image-1.5':
        litellm_id: 'azure/gpt-image-1.5'
        mode: image
        input_cost_per_1k: 0.005
      'gpt-image-1.5-2025-12-16':
        litellm_id: 'azure/gpt-image-1.5-2025-12-16'
        mode: image
        input_cost_per_1k: 0.005
      'gpt-oss-120b':
        litellm_id: 'azure_ai/gpt-oss-120b'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.00015
        output_cost_per_1k: 0.0006
        capabilities: [function_calling, parallel_function_calling, json_mode]
      'gpt-realtime-2025-08-28':
        litellm_id: 'azure/gpt-realtime-2025-08-28'
        mode: chat
        max_input_tokens: 32000
        max_output_tokens: 4096
        input_cost_per_1k: 0.004
        output_cost_per_1k: 0.016
        capabilities: [function_calling, parallel_function_calling, system_messages, audio_input, audio_output]
      'gpt-realtime-mini-2025-10-06':
        litellm_id: 'azure/gpt-realtime-mini-2025-10-06'
        mode: chat
        max_input_tokens: 32000
        max_output_tokens: 4096
        input_cost_per_1k: 0.0006
        output_cost_per_1k: 0.0024
        capabilities: [function_calling, parallel_function_calling, system_messages, audio_input, audio_output]
      'grok-3':
        litellm_id: 'azure_ai/grok-3'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.0033
        output_cost_per_1k: 0.0165
        capabilities: [function_calling, web_search]
      'grok-3-mini':
        litellm_id: 'azure_ai/grok-3-mini'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.000275
        output_cost_per_1k: 0.00138
        capabilities: [function_calling, reasoning, web_search]
      'grok-4':
        litellm_id: 'azure_ai/grok-4'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.0055
        output_cost_per_1k: 0.0275
        capabilities: [function_calling, json_mode, web_search]
      'grok-4-fast-non-reasoning':
        litellm_id: 'azure_ai/grok-4-fast-non-reasoning'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.00043
        output_cost_per_1k: 0.00173
        capabilities: [function_calling, json_mode, web_search]
      'grok-4-fast-reasoning':
        litellm_id: 'azure_ai/grok-4-fast-reasoning'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.00043
        output_cost_per_1k: 0.00173
        capabilities: [function_calling, json_mode, web_search]
      'grok-code-fast-1':
        litellm_id: 'azure_ai/grok-code-fast-1'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.0035
        output_cost_per_1k: 0.0175
        capabilities: [function_calling, json_mode, web_search]
      'hd/1024-x-1024/dall-e-3':
        litellm_id: 'azure/hd/1024-x-1024/dall-e-3'
        mode: image
      'hd/1024-x-1792/dall-e-3':
        litellm_id: 'azure/hd/1024-x-1792/dall-e-3'
        mode: image
      'hd/1792-x-1024/dall-e-3':
        litellm_id: 'azure/hd/1792-x-1024/dall-e-3'
        mode: image
      'high/1024-x-1024/gpt-image-1':
        litellm_id: 'azure/high/1024-x-1024/gpt-image-1'
        mode: image
      'high/1024-x-1024/gpt-image-1-mini':
        litellm_id: 'azure/high/1024-x-1024/gpt-image-1-mini'
        mode: image
      'high/1024-x-1536/gpt-image-1':
        litellm_id: 'azure/high/1024-x-1536/gpt-image-1'
        mode: image
      'high/1024-x-1536/gpt-image-1-mini':
        litellm_id: 'azure/high/1024-x-1536/gpt-image-1-mini'
        mode: image
      'high/1536-x-1024/gpt-image-1':
        litellm_id: 'azure/high/1536-x-1024/gpt-image-1'
        mode: image
      'high/1536-x-1024/gpt-image-1-mini':
        litellm_id: 'azure/high/1536-x-1024/gpt-image-1-mini'
        mode: image
      'jais-30b-chat':
        litellm_id: 'azure_ai/jais-30b-chat'
        mode: chat
        max_input_tokens: 8192
        max_output_tokens: 8192
        input_cost_per_1k: 3.2
        output_cost_per_1k: 9.71
      'jamba-instruct':
        litellm_id: 'azure_ai/jamba-instruct'
        mode: chat
        max_input_tokens: 70000
        max_output_tokens: 4096
        input_cost_per_1k: 0.0005
        output_cost_per_1k: 0.0007
      'low/1024-x-1024/gpt-image-1':
        litellm_id: 'azure/low/1024-x-1024/gpt-image-1'
        mode: image
      'low/1024-x-1024/gpt-image-1-mini':
        litellm_id: 'azure/low/1024-x-1024/gpt-image-1-mini'
        mode: image
      'low/1024-x-1536/gpt-image-1':
        litellm_id: 'azure/low/1024-x-1536/gpt-image-1'
        mode: image
      'low/1024-x-1536/gpt-image-1-mini':
        litellm_id: 'azure/low/1024-x-1536/gpt-image-1-mini'
        mode: image
      'low/1536-x-1024/gpt-image-1':
        litellm_id: 'azure/low/1536-x-1024/gpt-image-1'
        mode: image
      'low/1536-x-1024/gpt-image-1-mini':
        litellm_id: 'azure/low/1536-x-1024/gpt-image-1-mini'
        mode: image
      'medium/1024-x-1024/gpt-image-1':
        litellm_id: 'azure/medium/1024-x-1024/gpt-image-1'
        mode: image
      'medium/1024-x-1024/gpt-image-1-mini':
        litellm_id: 'azure/medium/1024-x-1024/gpt-image-1-mini'
        mode: image
      'medium/1024-x-1536/gpt-image-1':
        litellm_id: 'azure/medium/1024-x-1536/gpt-image-1'
        mode: image
      'medium/1024-x-1536/gpt-image-1-mini':
        litellm_id: 'azure/medium/1024-x-1536/gpt-image-1-mini'
        mode: image
      'medium/1536-x-1024/gpt-image-1':
        litellm_id: 'azure/medium/1536-x-1024/gpt-image-1'
        mode: image
      'medium/1536-x-1024/gpt-image-1-mini':
        litellm_id: 'azure/medium/1536-x-1024/gpt-image-1-mini'
        mode: image
      'ministral-3b':
        litellm_id: 'azure_ai/ministral-3b'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 4096
        input_cost_per_1k: 4e-05
        output_cost_per_1k: 4e-05
        capabilities: [function_calling]
      'mistral-document-ai-2505':
        litellm_id: 'azure_ai/mistral-document-ai-2505'
        mode: ocr
      'mistral-large':
        litellm_id: 'azure_ai/mistral-large'
        mode: chat
        max_input_tokens: 32000
        max_output_tokens: 8191
        input_cost_per_1k: 0.004
        output_cost_per_1k: 0.012
        capabilities: [function_calling]
      'mistral-large-2402':
        litellm_id: 'azure/mistral-large-2402'
        mode: chat
        max_input_tokens: 32000
        input_cost_per_1k: 0.008
        output_cost_per_1k: 0.024
        capabilities: [function_calling]
      'mistral-large-2407':
        litellm_id: 'azure_ai/mistral-large-2407'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 4096
        input_cost_per_1k: 0.002
        output_cost_per_1k: 0.006
        capabilities: [function_calling]
      'mistral-large-3':
        litellm_id: 'azure_ai/mistral-large-3'
        mode: chat
        max_input_tokens: 256000
        max_output_tokens: 8191
        input_cost_per_1k: 0.0005
        output_cost_per_1k: 0.0015
        capabilities: [vision, function_calling]
      'mistral-large-latest':
        litellm_id: 'azure/mistral-large-latest'
        mode: chat
        max_input_tokens: 32000
        input_cost_per_1k: 0.008
        output_cost_per_1k: 0.024
        capabilities: [function_calling]
      'mistral-large-latest':
        litellm_id: 'azure_ai/mistral-large-latest'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 4096
        input_cost_per_1k: 0.002
        output_cost_per_1k: 0.006
        capabilities: [function_calling]
      'mistral-medium-2505':
        litellm_id: 'azure_ai/mistral-medium-2505'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 8191
        input_cost_per_1k: 0.0004
        output_cost_per_1k: 0.002
        capabilities: [function_calling]
      'mistral-nemo':
        litellm_id: 'azure_ai/mistral-nemo'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 4096
        input_cost_per_1k: 0.00015
        output_cost_per_1k: 0.00015
        capabilities: [function_calling]
      'mistral-small':
        litellm_id: 'azure_ai/mistral-small'
        mode: chat
        max_input_tokens: 32000
        max_output_tokens: 8191
        input_cost_per_1k: 0.001
        output_cost_per_1k: 0.003
        capabilities: [function_calling]
      'mistral-small-2503':
        litellm_id: 'azure_ai/mistral-small-2503'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 128000
        input_cost_per_1k: 0.001
        output_cost_per_1k: 0.003
        capabilities: [vision, function_calling]
      'o1':
        litellm_id: 'azure/o1'
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 100000
        input_cost_per_1k: 0.015
        output_cost_per_1k: 0.06
        capabilities: [vision, function_calling, parallel_function_calling, prompt_caching, reasoning]
      'o1-2024-12-17':
        litellm_id: 'azure/o1-2024-12-17'
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 100000
        input_cost_per_1k: 0.015
        output_cost_per_1k: 0.06
        capabilities: [vision, function_calling, parallel_function_calling, prompt_caching, reasoning]
      'o1-mini':
        litellm_id: 'azure/o1-mini'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 65536
        input_cost_per_1k: 0.00121
        output_cost_per_1k: 0.00484
        capabilities: [function_calling, parallel_function_calling, prompt_caching, reasoning]
      'o1-mini-2024-09-12':
        litellm_id: 'azure/o1-mini-2024-09-12'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 65536
        input_cost_per_1k: 0.0011
        output_cost_per_1k: 0.0044
        capabilities: [function_calling, parallel_function_calling, prompt_caching, reasoning]
      'o1-preview':
        litellm_id: 'azure/o1-preview'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 32768
        input_cost_per_1k: 0.015
        output_cost_per_1k: 0.06
        capabilities: [function_calling, parallel_function_calling, prompt_caching, reasoning]
      'o1-preview-2024-09-12':
        litellm_id: 'azure/o1-preview-2024-09-12'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 32768
        input_cost_per_1k: 0.015
        output_cost_per_1k: 0.06
        capabilities: [function_calling, parallel_function_calling, prompt_caching, reasoning]
      'o3':
        litellm_id: 'azure/o3'
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 100000
        input_cost_per_1k: 0.002
        output_cost_per_1k: 0.008
        capabilities: [vision, function_calling, json_mode, prompt_caching, reasoning]
      'o3-2025-04-16':
        litellm_id: 'azure/o3-2025-04-16'
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 100000
        input_cost_per_1k: 0.002
        output_cost_per_1k: 0.008
        capabilities: [vision, function_calling, json_mode, prompt_caching, reasoning]
        deprecated: true
        deprecation_date: '2026-04-16'
      'o3-deep-research':
        litellm_id: 'azure/o3-deep-research'
        mode: responses
        max_input_tokens: 200000
        max_output_tokens: 100000
        input_cost_per_1k: 0.01
        output_cost_per_1k: 0.04
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching, reasoning, web_search]
      'o3-mini':
        litellm_id: 'azure/o3-mini'
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 100000
        input_cost_per_1k: 0.0011
        output_cost_per_1k: 0.0044
        capabilities: [json_mode, prompt_caching, reasoning]
      'o3-mini-2025-01-31':
        litellm_id: 'azure/o3-mini-2025-01-31'
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 100000
        input_cost_per_1k: 0.0011
        output_cost_per_1k: 0.0044
        capabilities: [prompt_caching, reasoning]
      'o3-pro':
        litellm_id: 'azure/o3-pro'
        mode: responses
        max_input_tokens: 200000
        max_output_tokens: 100000
        input_cost_per_1k: 0.02
        output_cost_per_1k: 0.08
        capabilities: [vision, function_calling, json_mode, reasoning]
      'o3-pro-2025-06-10':
        litellm_id: 'azure/o3-pro-2025-06-10'
        mode: responses
        max_input_tokens: 200000
        max_output_tokens: 100000
        input_cost_per_1k: 0.02
        output_cost_per_1k: 0.08
        capabilities: [vision, function_calling, json_mode, reasoning]
      'o4-mini':
        litellm_id: 'azure/o4-mini'
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 100000
        input_cost_per_1k: 0.0011
        output_cost_per_1k: 0.0044
        capabilities: [vision, function_calling, json_mode, prompt_caching, reasoning]
      'o4-mini-2025-04-16':
        litellm_id: 'azure/o4-mini-2025-04-16'
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 100000
        input_cost_per_1k: 0.0011
        output_cost_per_1k: 0.0044
        capabilities: [vision, function_calling, json_mode, prompt_caching, reasoning]
      'sora-2':
        litellm_id: 'azure/sora-2'
        mode: video_generation
      'sora-2-pro':
        litellm_id: 'azure/sora-2-pro'
        mode: video_generation
      'sora-2-pro-high-res':
        litellm_id: 'azure/sora-2-pro-high-res'
        mode: video_generation
      'speech/azure-tts':
        litellm_id: 'azure/speech/azure-tts'
        mode: audio_speech
      'speech/azure-tts-hd':
        litellm_id: 'azure/speech/azure-tts-hd'
        mode: audio_speech
      'standard/1024-x-1024/dall-e-2':
        litellm_id: 'azure/standard/1024-x-1024/dall-e-2'
        mode: image
      'standard/1024-x-1024/dall-e-3':
        litellm_id: 'azure/standard/1024-x-1024/dall-e-3'
        mode: image
      'standard/1024-x-1792/dall-e-3':
        litellm_id: 'azure/standard/1024-x-1792/dall-e-3'
        mode: image
      'standard/1792-x-1024/dall-e-3':
        litellm_id: 'azure/standard/1792-x-1024/dall-e-3'
        mode: image
      'text-embedding-3-large':
        litellm_id: 'azure/text-embedding-3-large'
        mode: embedding
        max_input_tokens: 8191
        input_cost_per_1k: 0.00013
      'text-embedding-3-small':
        litellm_id: 'azure/text-embedding-3-small'
        mode: embedding
        max_input_tokens: 8191
        input_cost_per_1k: 2e-05
        deprecated: true
        deprecation_date: '2026-04-30'
      'text-embedding-ada-002':
        litellm_id: 'azure/text-embedding-ada-002'
        mode: embedding
        max_input_tokens: 8191
        input_cost_per_1k: 0.0001
      'tts-1':
        litellm_id: 'azure/tts-1'
        mode: audio_speech
      'tts-1-hd':
        litellm_id: 'azure/tts-1-hd'
        mode: audio_speech
      'us/gpt-4.1-2025-04-14':
        litellm_id: 'azure/us/gpt-4.1-2025-04-14'
        mode: chat
        max_input_tokens: 1047576
        max_output_tokens: 32768
        input_cost_per_1k: 0.0022
        output_cost_per_1k: 0.0088
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching]
        deprecated: true
        deprecation_date: '2026-11-04'
      'us/gpt-4.1-mini-2025-04-14':
        litellm_id: 'azure/us/gpt-4.1-mini-2025-04-14'
        mode: chat
        max_input_tokens: 1047576
        max_output_tokens: 32768
        input_cost_per_1k: 0.00044
        output_cost_per_1k: 0.00176
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching]
        deprecated: true
        deprecation_date: '2026-11-04'
      'us/gpt-4.1-nano-2025-04-14':
        litellm_id: 'azure/us/gpt-4.1-nano-2025-04-14'
        mode: chat
        max_input_tokens: 1047576
        max_output_tokens: 32768
        input_cost_per_1k: 0.00011
        output_cost_per_1k: 0.00044
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching]
        deprecated: true
        deprecation_date: '2026-11-04'
      'us/gpt-4o-2024-08-06':
        litellm_id: 'azure/us/gpt-4o-2024-08-06'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 16384
        input_cost_per_1k: 0.00275
        output_cost_per_1k: 0.011
        capabilities: [vision, function_calling, parallel_function_calling, json_mode, prompt_caching]
        deprecated: true
        deprecation_date: '2026-02-27'
      'us/gpt-4o-2024-11-20':
        litellm_id: 'azure/us/gpt-4o-2024-11-20'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 16384
        input_cost_per_1k: 0.00275
        output_cost_per_1k: 0.011
        capabilities: [vision, function_calling, parallel_function_calling, json_mode]
        deprecated: true
        deprecation_date: '2026-03-01'
      'us/gpt-4o-mini-2024-07-18':
        litellm_id: 'azure/us/gpt-4o-mini-2024-07-18'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 16384
        input_cost_per_1k: 0.000165
        output_cost_per_1k: 0.00066
        capabilities: [vision, function_calling, parallel_function_calling, json_mode, prompt_caching]
      'us/gpt-4o-mini-realtime-preview-2024-12-17':
        litellm_id: 'azure/us/gpt-4o-mini-realtime-preview-2024-12-17'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 4096
        input_cost_per_1k: 0.00066
        output_cost_per_1k: 0.00264
        capabilities: [function_calling, parallel_function_calling, system_messages, audio_input, audio_output]
      'us/gpt-4o-realtime-preview-2024-10-01':
        litellm_id: 'azure/us/gpt-4o-realtime-preview-2024-10-01'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 4096
        input_cost_per_1k: 0.0055
        output_cost_per_1k: 0.022
        capabilities: [function_calling, parallel_function_calling, system_messages, audio_input, audio_output]
      'us/gpt-4o-realtime-preview-2024-12-17':
        litellm_id: 'azure/us/gpt-4o-realtime-preview-2024-12-17'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 4096
        input_cost_per_1k: 0.0055
        output_cost_per_1k: 0.022
        capabilities: [function_calling, parallel_function_calling, system_messages, audio_input, audio_output]
      'us/gpt-5-2025-08-07':
        litellm_id: 'azure/us/gpt-5-2025-08-07'
        mode: chat
        max_input_tokens: 272000
        max_output_tokens: 128000
        input_cost_per_1k: 0.001375
        output_cost_per_1k: 0.011
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching, reasoning]
      'us/gpt-5-mini-2025-08-07':
        litellm_id: 'azure/us/gpt-5-mini-2025-08-07'
        mode: chat
        max_input_tokens: 272000
        max_output_tokens: 128000
        input_cost_per_1k: 0.000275
        output_cost_per_1k: 0.0022
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching, reasoning]
      'us/gpt-5-nano-2025-08-07':
        litellm_id: 'azure/us/gpt-5-nano-2025-08-07'
        mode: chat
        max_input_tokens: 272000
        max_output_tokens: 128000
        input_cost_per_1k: 5.5e-05
        output_cost_per_1k: 0.00044
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching, reasoning]
      'us/gpt-5.1':
        litellm_id: 'azure/us/gpt-5.1'
        mode: chat
        max_input_tokens: 272000
        max_output_tokens: 128000
        input_cost_per_1k: 0.00138
        output_cost_per_1k: 0.011
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching, reasoning]
      'us/gpt-5.1-chat':
        litellm_id: 'azure/us/gpt-5.1-chat'
        mode: chat
        max_input_tokens: 272000
        max_output_tokens: 128000
        input_cost_per_1k: 0.00138
        output_cost_per_1k: 0.011
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching, reasoning]
      'us/gpt-5.1-codex':
        litellm_id: 'azure/us/gpt-5.1-codex'
        mode: responses
        max_input_tokens: 272000
        max_output_tokens: 128000
        input_cost_per_1k: 0.00138
        output_cost_per_1k: 0.011
        capabilities: [vision, function_calling, parallel_function_calling, json_mode, prompt_caching, reasoning]
      'us/gpt-5.1-codex-mini':
        litellm_id: 'azure/us/gpt-5.1-codex-mini'
        mode: responses
        max_input_tokens: 272000
        max_output_tokens: 128000
        input_cost_per_1k: 0.000275
        output_cost_per_1k: 0.0022
        capabilities: [vision, function_calling, parallel_function_calling, json_mode, prompt_caching, reasoning]
      'us/o1-2024-12-17':
        litellm_id: 'azure/us/o1-2024-12-17'
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 100000
        input_cost_per_1k: 0.0165
        output_cost_per_1k: 0.066
        capabilities: [vision, function_calling, parallel_function_calling, prompt_caching]
      'us/o1-mini-2024-09-12':
        litellm_id: 'azure/us/o1-mini-2024-09-12'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 65536
        input_cost_per_1k: 0.00121
        output_cost_per_1k: 0.00484
        capabilities: [function_calling, parallel_function_calling, prompt_caching]
      'us/o1-preview-2024-09-12':
        litellm_id: 'azure/us/o1-preview-2024-09-12'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 32768
        input_cost_per_1k: 0.0165
        output_cost_per_1k: 0.066
        capabilities: [function_calling, parallel_function_calling, prompt_caching]
      'us/o3-2025-04-16':
        litellm_id: 'azure/us/o3-2025-04-16'
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 100000
        input_cost_per_1k: 0.0022
        output_cost_per_1k: 0.0088
        capabilities: [vision, function_calling, json_mode, prompt_caching, reasoning]
        deprecated: true
        deprecation_date: '2026-04-16'
      'us/o3-mini-2025-01-31':
        litellm_id: 'azure/us/o3-mini-2025-01-31'
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 100000
        input_cost_per_1k: 0.00121
        output_cost_per_1k: 0.00484
        capabilities: [prompt_caching, reasoning]
      'us/o4-mini-2025-04-16':
        litellm_id: 'azure/us/o4-mini-2025-04-16'
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 100000
        input_cost_per_1k: 0.00121
        output_cost_per_1k: 0.00484
        capabilities: [vision, function_calling, json_mode, prompt_caching, reasoning]
      'whisper-1':
        litellm_id: 'azure/whisper-1'
        mode: audio_transcription
  azure_text:
    model_count: 3
    models:
      'azure/gpt-3.5-turbo-instruct-0914':
        mode: completion
        max_input_tokens: 4097
        input_cost_per_1k: 0.0015
        output_cost_per_1k: 0.002
      'azure/gpt-35-turbo-instruct':
        mode: completion
        max_input_tokens: 4097
        input_cost_per_1k: 0.0015
        output_cost_per_1k: 0.002
      'azure/gpt-35-turbo-instruct-0914':
        mode: completion
        max_input_tokens: 4097
        input_cost_per_1k: 0.0015
        output_cost_per_1k: 0.002
  bedrock_converse:
    model_count: 87
    models:
      'amazon.nova-2-lite-v1:0':
        mode: chat
        max_input_tokens: 1000000
        max_output_tokens: 64000
        input_cost_per_1k: 0.0003
        output_cost_per_1k: 0.0025
        capabilities: [vision, function_calling, json_mode, prompt_caching, reasoning]
      'amazon.nova-lite-v1:0':
        mode: chat
        max_input_tokens: 300000
        max_output_tokens: 10000
        input_cost_per_1k: 6e-05
        output_cost_per_1k: 0.00024
        capabilities: [vision, function_calling, json_mode, prompt_caching]
      'amazon.nova-micro-v1:0':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 10000
        input_cost_per_1k: 3.5e-05
        output_cost_per_1k: 0.00014
        capabilities: [function_calling, json_mode, prompt_caching]
      'amazon.nova-pro-v1:0':
        mode: chat
        max_input_tokens: 300000
        max_output_tokens: 10000
        input_cost_per_1k: 0.0008
        output_cost_per_1k: 0.0032
        capabilities: [vision, function_calling, json_mode, prompt_caching]
      'anthropic.claude-3-7-sonnet-20250219-v1:0':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 8192
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.015
        capabilities: [vision, function_calling, json_mode, prompt_caching, reasoning]
      'anthropic.claude-haiku-4-5-20251001-v1:0':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 64000
        input_cost_per_1k: 0.001
        output_cost_per_1k: 0.005
        capabilities: [vision, function_calling, json_mode, prompt_caching, reasoning]
      'anthropic.claude-haiku-4-5@20251001':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 64000
        input_cost_per_1k: 0.001
        output_cost_per_1k: 0.005
        capabilities: [vision, function_calling, json_mode, prompt_caching, reasoning]
      'anthropic.claude-opus-4-1-20250805-v1:0':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 32000
        input_cost_per_1k: 0.015
        output_cost_per_1k: 0.075
        capabilities: [vision, function_calling, json_mode, prompt_caching, reasoning]
      'anthropic.claude-opus-4-20250514-v1:0':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 32000
        input_cost_per_1k: 0.015
        output_cost_per_1k: 0.075
        capabilities: [vision, function_calling, json_mode, prompt_caching, reasoning]
      'anthropic.claude-opus-4-5-20251101-v1:0':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 64000
        input_cost_per_1k: 0.005
        output_cost_per_1k: 0.025
        capabilities: [vision, function_calling, json_mode, prompt_caching, reasoning]
      'anthropic.claude-sonnet-4-20250514-v1:0':
        mode: chat
        max_input_tokens: 1000000
        max_output_tokens: 64000
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.015
        capabilities: [vision, function_calling, json_mode, prompt_caching, reasoning]
      'anthropic.claude-sonnet-4-5-20250929-v1:0':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 64000
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.015
        capabilities: [vision, function_calling, json_mode, prompt_caching, reasoning]
      'apac.amazon.nova-2-lite-v1:0':
        mode: chat
        max_input_tokens: 1000000
        max_output_tokens: 64000
        input_cost_per_1k: 0.00033
        output_cost_per_1k: 0.00275
        capabilities: [vision, function_calling, json_mode, prompt_caching, reasoning]
      'apac.amazon.nova-lite-v1:0':
        mode: chat
        max_input_tokens: 300000
        max_output_tokens: 10000
        input_cost_per_1k: 6.3e-05
        output_cost_per_1k: 0.000252
        capabilities: [vision, function_calling, json_mode, prompt_caching]
      'apac.amazon.nova-micro-v1:0':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 10000
        input_cost_per_1k: 3.7e-05
        output_cost_per_1k: 0.000148
        capabilities: [function_calling, json_mode, prompt_caching]
      'apac.amazon.nova-pro-v1:0':
        mode: chat
        max_input_tokens: 300000
        max_output_tokens: 10000
        input_cost_per_1k: 0.00084
        output_cost_per_1k: 0.00336
        capabilities: [vision, function_calling, json_mode, prompt_caching]
      'apac.anthropic.claude-haiku-4-5-20251001-v1:0':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 64000
        input_cost_per_1k: 0.0011
        output_cost_per_1k: 0.0055
        capabilities: [vision, function_calling, json_mode, prompt_caching, reasoning]
      'apac.anthropic.claude-sonnet-4-20250514-v1:0':
        mode: chat
        max_input_tokens: 1000000
        max_output_tokens: 64000
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.015
        capabilities: [vision, function_calling, json_mode, prompt_caching, reasoning]
      'au.anthropic.claude-haiku-4-5-20251001-v1:0':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 64000
        input_cost_per_1k: 0.0011
        output_cost_per_1k: 0.0055
        capabilities: [vision, function_calling, json_mode, prompt_caching, reasoning]
      'au.anthropic.claude-sonnet-4-5-20250929-v1:0':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 64000
        input_cost_per_1k: 0.0033
        output_cost_per_1k: 0.0165
        capabilities: [vision, function_calling, json_mode, prompt_caching, reasoning]
      'deepseek.v3-v1:0':
        mode: chat
        max_input_tokens: 163840
        max_output_tokens: 81920
        input_cost_per_1k: 0.00058
        output_cost_per_1k: 0.00168
        capabilities: [function_calling, reasoning]
      'eu.amazon.nova-2-lite-v1:0':
        mode: chat
        max_input_tokens: 1000000
        max_output_tokens: 64000
        input_cost_per_1k: 0.00033
        output_cost_per_1k: 0.00275
        capabilities: [vision, function_calling, json_mode, prompt_caching, reasoning]
      'eu.amazon.nova-lite-v1:0':
        mode: chat
        max_input_tokens: 300000
        max_output_tokens: 10000
        input_cost_per_1k: 7.8e-05
        output_cost_per_1k: 0.000312
        capabilities: [vision, function_calling, json_mode, prompt_caching]
      'eu.amazon.nova-micro-v1:0':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 10000
        input_cost_per_1k: 4.6e-05
        output_cost_per_1k: 0.000184
        capabilities: [function_calling, json_mode, prompt_caching]
      'eu.amazon.nova-pro-v1:0':
        mode: chat
        max_input_tokens: 300000
        max_output_tokens: 10000
        input_cost_per_1k: 0.00105
        output_cost_per_1k: 0.0042
        capabilities: [vision, function_calling, json_mode, prompt_caching]
      'eu.anthropic.claude-haiku-4-5-20251001-v1:0':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 64000
        input_cost_per_1k: 0.0011
        output_cost_per_1k: 0.0055
        capabilities: [vision, function_calling, json_mode, prompt_caching, reasoning]
        deprecated: true
        deprecation_date: '2026-10-15'
      'eu.anthropic.claude-opus-4-1-20250805-v1:0':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 32000
        input_cost_per_1k: 0.015
        output_cost_per_1k: 0.075
        capabilities: [vision, function_calling, json_mode, prompt_caching, reasoning]
      'eu.anthropic.claude-opus-4-20250514-v1:0':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 32000
        input_cost_per_1k: 0.015
        output_cost_per_1k: 0.075
        capabilities: [vision, function_calling, json_mode, prompt_caching, reasoning]
      'eu.anthropic.claude-opus-4-5-20251101-v1:0':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 64000
        input_cost_per_1k: 0.005
        output_cost_per_1k: 0.025
        capabilities: [vision, function_calling, json_mode, prompt_caching, reasoning]
      'eu.anthropic.claude-sonnet-4-20250514-v1:0':
        mode: chat
        max_input_tokens: 1000000
        max_output_tokens: 64000
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.015
        capabilities: [vision, function_calling, json_mode, prompt_caching, reasoning]
      'eu.anthropic.claude-sonnet-4-5-20250929-v1:0':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 64000
        input_cost_per_1k: 0.0033
        output_cost_per_1k: 0.0165
        capabilities: [vision, function_calling, json_mode, prompt_caching, reasoning]
      'eu.mistral.pixtral-large-2502-v1:0':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 4096
        input_cost_per_1k: 0.002
        output_cost_per_1k: 0.006
        capabilities: [function_calling]
      'global.amazon.nova-2-lite-v1:0':
        mode: chat
        max_input_tokens: 1000000
        max_output_tokens: 64000
        input_cost_per_1k: 0.0003
        output_cost_per_1k: 0.0025
        capabilities: [vision, function_calling, json_mode, prompt_caching, reasoning]
      'global.anthropic.claude-haiku-4-5-20251001-v1:0':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 64000
        input_cost_per_1k: 0.001
        output_cost_per_1k: 0.005
        capabilities: [vision, function_calling, json_mode, prompt_caching, reasoning]
      'global.anthropic.claude-opus-4-5-20251101-v1:0':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 64000
        input_cost_per_1k: 0.005
        output_cost_per_1k: 0.025
        capabilities: [vision, function_calling, json_mode, prompt_caching, reasoning]
      'global.anthropic.claude-sonnet-4-20250514-v1:0':
        mode: chat
        max_input_tokens: 1000000
        max_output_tokens: 64000
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.015
        capabilities: [vision, function_calling, json_mode, prompt_caching, reasoning]
      'global.anthropic.claude-sonnet-4-5-20250929-v1:0':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 64000
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.015
        capabilities: [vision, function_calling, json_mode, prompt_caching, reasoning]
      'google.gemma-3-12b-it':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 8192
        input_cost_per_1k: 9e-05
        output_cost_per_1k: 0.00029
        capabilities: [vision, system_messages]
      'google.gemma-3-27b-it':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 8192
        input_cost_per_1k: 0.00023
        output_cost_per_1k: 0.00038
        capabilities: [vision, system_messages]
      'google.gemma-3-4b-it':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 8192
        input_cost_per_1k: 4e-05
        output_cost_per_1k: 8e-05
        capabilities: [vision, system_messages]
      'jp.anthropic.claude-haiku-4-5-20251001-v1:0':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 64000
        input_cost_per_1k: 0.0011
        output_cost_per_1k: 0.0055
        capabilities: [vision, function_calling, json_mode, prompt_caching, reasoning]
      'jp.anthropic.claude-sonnet-4-5-20250929-v1:0':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 64000
        input_cost_per_1k: 0.0033
        output_cost_per_1k: 0.0165
        capabilities: [vision, function_calling, json_mode, prompt_caching, reasoning]
      'meta.llama3-3-70b-instruct-v1:0':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 4096
        input_cost_per_1k: 0.00072
        output_cost_per_1k: 0.00072
        capabilities: [function_calling]
      'meta.llama4-maverick-17b-instruct-v1:0':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 4096
        input_cost_per_1k: 0.00024
        output_cost_per_1k: 0.00097
        capabilities: [function_calling]
      'meta.llama4-scout-17b-instruct-v1:0':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 4096
        input_cost_per_1k: 0.00017
        output_cost_per_1k: 0.00066
        capabilities: [function_calling]
      'minimax.minimax-m2':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 8192
        input_cost_per_1k: 0.0003
        output_cost_per_1k: 0.0012
        capabilities: [system_messages]
      'mistral.magistral-small-2509':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 8192
        input_cost_per_1k: 0.0005
        output_cost_per_1k: 0.0015
        capabilities: [function_calling, system_messages, reasoning]
      'mistral.ministral-3-14b-instruct':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 8192
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
        capabilities: [function_calling, system_messages]
      'mistral.ministral-3-3b-instruct':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 8192
        input_cost_per_1k: 0.0001
        output_cost_per_1k: 0.0001
        capabilities: [function_calling, system_messages]
      'mistral.ministral-3-8b-instruct':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 8192
        input_cost_per_1k: 0.00015
        output_cost_per_1k: 0.00015
        capabilities: [function_calling, system_messages]
      'mistral.mistral-large-3-675b-instruct':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 8192
        input_cost_per_1k: 0.0005
        output_cost_per_1k: 0.0015
        capabilities: [function_calling, system_messages]
      'mistral.voxtral-mini-3b-2507':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 8192
        input_cost_per_1k: 4e-05
        output_cost_per_1k: 4e-05
        capabilities: [system_messages, audio_input]
      'mistral.voxtral-small-24b-2507':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 8192
        input_cost_per_1k: 0.0001
        output_cost_per_1k: 0.0003
        capabilities: [system_messages, audio_input]
      'moonshot.kimi-k2-thinking':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 8192
        input_cost_per_1k: 0.0006
        output_cost_per_1k: 0.0025
        capabilities: [system_messages, reasoning]
      'nvidia.nemotron-nano-12b-v2':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 8192
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0006
        capabilities: [vision, system_messages]
      'nvidia.nemotron-nano-9b-v2':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 8192
        input_cost_per_1k: 6e-05
        output_cost_per_1k: 0.00023
        capabilities: [system_messages]
      'openai.gpt-oss-120b-1:0':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 128000
        input_cost_per_1k: 0.00015
        output_cost_per_1k: 0.0006
        capabilities: [function_calling, json_mode, reasoning]
      'openai.gpt-oss-20b-1:0':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 128000
        input_cost_per_1k: 7e-05
        output_cost_per_1k: 0.0003
        capabilities: [function_calling, json_mode, reasoning]
      'openai.gpt-oss-safeguard-120b':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 8192
        input_cost_per_1k: 0.00015
        output_cost_per_1k: 0.0006
        capabilities: [system_messages]
      'openai.gpt-oss-safeguard-20b':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 8192
        input_cost_per_1k: 7e-05
        output_cost_per_1k: 0.0002
        capabilities: [system_messages]
      'qwen.qwen3-235b-a22b-2507-v1:0':
        mode: chat
        max_input_tokens: 262144
        max_output_tokens: 131072
        input_cost_per_1k: 0.00022
        output_cost_per_1k: 0.00088
        capabilities: [function_calling, reasoning]
      'qwen.qwen3-32b-v1:0':
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 16384
        input_cost_per_1k: 0.00015
        output_cost_per_1k: 0.0006
        capabilities: [function_calling, reasoning]
      'qwen.qwen3-coder-30b-a3b-v1:0':
        mode: chat
        max_input_tokens: 262144
        max_output_tokens: 131072
        input_cost_per_1k: 0.00015
        output_cost_per_1k: 0.0006
        capabilities: [function_calling, reasoning]
      'qwen.qwen3-coder-480b-a35b-v1:0':
        mode: chat
        max_input_tokens: 262000
        max_output_tokens: 65536
        input_cost_per_1k: 0.00022
        output_cost_per_1k: 0.0018
        capabilities: [function_calling, reasoning]
      'qwen.qwen3-next-80b-a3b':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 8192
        input_cost_per_1k: 0.00015
        output_cost_per_1k: 0.0012
        capabilities: [function_calling, system_messages]
      'qwen.qwen3-vl-235b-a22b':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 8192
        input_cost_per_1k: 0.00053
        output_cost_per_1k: 0.00266
        capabilities: [vision, function_calling, system_messages]
      'us.amazon.nova-2-lite-v1:0':
        mode: chat
        max_input_tokens: 1000000
        max_output_tokens: 64000
        input_cost_per_1k: 0.00033
        output_cost_per_1k: 0.00275
        capabilities: [vision, function_calling, json_mode, prompt_caching, reasoning]
      'us.amazon.nova-lite-v1:0':
        mode: chat
        max_input_tokens: 300000
        max_output_tokens: 10000
        input_cost_per_1k: 6e-05
        output_cost_per_1k: 0.00024
        capabilities: [vision, function_calling, json_mode, prompt_caching]
      'us.amazon.nova-micro-v1:0':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 10000
        input_cost_per_1k: 3.5e-05
        output_cost_per_1k: 0.00014
        capabilities: [function_calling, json_mode, prompt_caching]
      'us.amazon.nova-premier-v1:0':
        mode: chat
        max_input_tokens: 1000000
        max_output_tokens: 10000
        input_cost_per_1k: 0.0025
        output_cost_per_1k: 0.0125
        capabilities: [vision, function_calling, json_mode]
      'us.amazon.nova-pro-v1:0':
        mode: chat
        max_input_tokens: 300000
        max_output_tokens: 10000
        input_cost_per_1k: 0.0008
        output_cost_per_1k: 0.0032
        capabilities: [vision, function_calling, json_mode, prompt_caching]
      'us.anthropic.claude-3-7-sonnet-20250219-v1:0':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 8192
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.015
        capabilities: [vision, function_calling, json_mode, prompt_caching, reasoning]
      'us.anthropic.claude-haiku-4-5-20251001-v1:0':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 64000
        input_cost_per_1k: 0.0011
        output_cost_per_1k: 0.0055
        capabilities: [vision, function_calling, json_mode, prompt_caching, reasoning]
      'us.anthropic.claude-opus-4-1-20250805-v1:0':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 32000
        input_cost_per_1k: 0.015
        output_cost_per_1k: 0.075
        capabilities: [vision, function_calling, json_mode, prompt_caching, reasoning]
      'us.anthropic.claude-opus-4-20250514-v1:0':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 32000
        input_cost_per_1k: 0.015
        output_cost_per_1k: 0.075
        capabilities: [vision, function_calling, json_mode, prompt_caching, reasoning]
      'us.anthropic.claude-opus-4-5-20251101-v1:0':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 64000
        input_cost_per_1k: 0.005
        output_cost_per_1k: 0.025
        capabilities: [vision, function_calling, json_mode, prompt_caching, reasoning]
      'us.anthropic.claude-sonnet-4-20250514-v1:0':
        mode: chat
        max_input_tokens: 1000000
        max_output_tokens: 64000
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.015
        capabilities: [vision, function_calling, json_mode, prompt_caching, reasoning]
      'us.anthropic.claude-sonnet-4-5-20250929-v1:0':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 64000
        input_cost_per_1k: 0.0033
        output_cost_per_1k: 0.0165
        capabilities: [vision, function_calling, json_mode, prompt_caching, reasoning]
      'us.deepseek.r1-v1:0':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 4096
        input_cost_per_1k: 0.00135
        output_cost_per_1k: 0.0054
        capabilities: [reasoning]
      'us.meta.llama3-3-70b-instruct-v1:0':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 4096
        input_cost_per_1k: 0.00072
        output_cost_per_1k: 0.00072
        capabilities: [function_calling]
      'us.meta.llama4-maverick-17b-instruct-v1:0':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 4096
        input_cost_per_1k: 0.00024
        output_cost_per_1k: 0.00097
        capabilities: [function_calling]
      'us.meta.llama4-scout-17b-instruct-v1:0':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 4096
        input_cost_per_1k: 0.00017
        output_cost_per_1k: 0.00066
        capabilities: [function_calling]
      'us.mistral.pixtral-large-2502-v1:0':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 4096
        input_cost_per_1k: 0.002
        output_cost_per_1k: 0.006
        capabilities: [function_calling]
      'us.writer.palmyra-x4-v1:0':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 8192
        input_cost_per_1k: 0.0025
        output_cost_per_1k: 0.01
        capabilities: [function_calling]
      'us.writer.palmyra-x5-v1:0':
        mode: chat
        max_input_tokens: 1000000
        max_output_tokens: 8192
        input_cost_per_1k: 0.0006
        output_cost_per_1k: 0.006
        capabilities: [function_calling]
      'writer.palmyra-x4-v1:0':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 8192
        input_cost_per_1k: 0.0025
        output_cost_per_1k: 0.01
        capabilities: [function_calling]
      'writer.palmyra-x5-v1:0':
        mode: chat
        max_input_tokens: 1000000
        max_output_tokens: 8192
        input_cost_per_1k: 0.0006
        output_cost_per_1k: 0.006
        capabilities: [function_calling]
  cerebras:
    model_count: 6
    models:
      'gpt-oss-120b':
        litellm_id: 'cerebras/gpt-oss-120b'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 32768
        input_cost_per_1k: 0.00025
        output_cost_per_1k: 0.00069
        capabilities: [function_calling, parallel_function_calling, json_mode, reasoning]
      'llama-3.3-70b':
        litellm_id: 'cerebras/llama-3.3-70b'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 128000
        input_cost_per_1k: 0.00085
        output_cost_per_1k: 0.0012
        capabilities: [function_calling]
      'llama3.1-70b':
        litellm_id: 'cerebras/llama3.1-70b'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 128000
        input_cost_per_1k: 0.0006
        output_cost_per_1k: 0.0006
        capabilities: [function_calling]
      'llama3.1-8b':
        litellm_id: 'cerebras/llama3.1-8b'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 128000
        input_cost_per_1k: 0.0001
        output_cost_per_1k: 0.0001
        capabilities: [function_calling]
      'qwen-3-32b':
        litellm_id: 'cerebras/qwen-3-32b'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 128000
        input_cost_per_1k: 0.0004
        output_cost_per_1k: 0.0008
        capabilities: [function_calling]
      'zai-glm-4.6':
        litellm_id: 'cerebras/zai-glm-4.6'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 128000
        input_cost_per_1k: 0.00225
        output_cost_per_1k: 0.00275
        capabilities: [function_calling, reasoning]
  cloudflare:
    model_count: 4
    models:
      '@cf/meta/llama-2-7b-chat-fp16':
        litellm_id: 'cloudflare/@cf/meta/llama-2-7b-chat-fp16'
        mode: chat
        max_input_tokens: 3072
        max_output_tokens: 3072
        input_cost_per_1k: 0.001923
        output_cost_per_1k: 0.001923
      '@cf/meta/llama-2-7b-chat-int8':
        litellm_id: 'cloudflare/@cf/meta/llama-2-7b-chat-int8'
        mode: chat
        max_input_tokens: 2048
        max_output_tokens: 2048
        input_cost_per_1k: 0.001923
        output_cost_per_1k: 0.001923
      '@cf/mistral/mistral-7b-instruct-v0.1':
        litellm_id: 'cloudflare/@cf/mistral/mistral-7b-instruct-v0.1'
        mode: chat
        max_input_tokens: 8192
        max_output_tokens: 8192
        input_cost_per_1k: 0.001923
        output_cost_per_1k: 0.001923
      '@hf/thebloke/codellama-7b-instruct-awq':
        litellm_id: 'cloudflare/@hf/thebloke/codellama-7b-instruct-awq'
        mode: chat
        max_input_tokens: 4096
        max_output_tokens: 4096
        input_cost_per_1k: 0.001923
        output_cost_per_1k: 0.001923
  codestral:
    model_count: 2
    models:
      'codestral-2405':
        litellm_id: 'codestral/codestral-2405'
        mode: chat
        max_input_tokens: 32000
        max_output_tokens: 8191
      'codestral-latest':
        litellm_id: 'codestral/codestral-latest'
        mode: chat
        max_input_tokens: 32000
        max_output_tokens: 8191
  cohere:
    model_count: 22
    models:
      'command':
        mode: completion
        max_input_tokens: 4096
        max_output_tokens: 4096
        input_cost_per_1k: 0.001
        output_cost_per_1k: 0.002
      'command-a-03-2025':
        mode: chat
        max_input_tokens: 256000
        max_output_tokens: 8000
        input_cost_per_1k: 0.0025
        output_cost_per_1k: 0.01
        capabilities: [function_calling]
      'command-light':
        mode: chat
        max_input_tokens: 4096
        max_output_tokens: 4096
        input_cost_per_1k: 0.0003
        output_cost_per_1k: 0.0006
      'command-nightly':
        mode: completion
        max_input_tokens: 4096
        max_output_tokens: 4096
        input_cost_per_1k: 0.001
        output_cost_per_1k: 0.002
      'command-r':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 4096
        input_cost_per_1k: 0.00015
        output_cost_per_1k: 0.0006
        capabilities: [function_calling]
      'command-r-08-2024':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 4096
        input_cost_per_1k: 0.00015
        output_cost_per_1k: 0.0006
        capabilities: [function_calling]
      'command-r-plus':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 4096
        input_cost_per_1k: 0.0025
        output_cost_per_1k: 0.01
        capabilities: [function_calling]
      'command-r-plus-08-2024':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 4096
        input_cost_per_1k: 0.0025
        output_cost_per_1k: 0.01
        capabilities: [function_calling]
      'command-r7b-12-2024':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 4096
        input_cost_per_1k: 0.00015
        output_cost_per_1k: 3.75e-05
        capabilities: [function_calling]
      'embed-english-light-v2.0':
        mode: embedding
        max_input_tokens: 1024
        input_cost_per_1k: 0.0001
      'embed-english-light-v3.0':
        mode: embedding
        max_input_tokens: 1024
        input_cost_per_1k: 0.0001
      'embed-english-v2.0':
        mode: embedding
        max_input_tokens: 4096
        input_cost_per_1k: 0.0001
      'embed-english-v3.0':
        mode: embedding
        max_input_tokens: 1024
        input_cost_per_1k: 0.0001
      'embed-multilingual-light-v3.0':
        mode: embedding
        max_input_tokens: 1024
        input_cost_per_1k: 0.1
      'embed-multilingual-v2.0':
        mode: embedding
        max_input_tokens: 768
        input_cost_per_1k: 0.0001
      'embed-multilingual-v3.0':
        mode: embedding
        max_input_tokens: 1024
        input_cost_per_1k: 0.0001
      'embed-v4.0':
        litellm_id: 'cohere/embed-v4.0'
        mode: embedding
        max_input_tokens: 128000
        input_cost_per_1k: 0.00012
      'rerank-english-v2.0':
        mode: rerank
        max_input_tokens: 4096
        max_output_tokens: 4096
      'rerank-english-v3.0':
        mode: rerank
        max_input_tokens: 4096
        max_output_tokens: 4096
      'rerank-multilingual-v2.0':
        mode: rerank
        max_input_tokens: 4096
        max_output_tokens: 4096
      'rerank-multilingual-v3.0':
        mode: rerank
        max_input_tokens: 4096
        max_output_tokens: 4096
      'rerank-v3.5':
        mode: rerank
        max_input_tokens: 4096
        max_output_tokens: 4096
  dashscope:
    model_count: 22
    models:
      'qwen-coder':
        litellm_id: 'dashscope/qwen-coder'
        mode: chat
        max_input_tokens: 1000000
        max_output_tokens: 16384
        input_cost_per_1k: 0.0003
        output_cost_per_1k: 0.0015
        capabilities: [function_calling, reasoning]
      'qwen-flash':
        litellm_id: 'dashscope/qwen-flash'
        mode: chat
        max_input_tokens: 997952
        max_output_tokens: 32768
        capabilities: [function_calling, reasoning]
      'qwen-flash-2025-07-28':
        litellm_id: 'dashscope/qwen-flash-2025-07-28'
        mode: chat
        max_input_tokens: 997952
        max_output_tokens: 32768
        capabilities: [function_calling, reasoning]
      'qwen-max':
        litellm_id: 'dashscope/qwen-max'
        mode: chat
        max_input_tokens: 30720
        max_output_tokens: 8192
        input_cost_per_1k: 0.0016
        output_cost_per_1k: 0.0064
        capabilities: [function_calling, reasoning]
      'qwen-plus':
        litellm_id: 'dashscope/qwen-plus'
        mode: chat
        max_input_tokens: 129024
        max_output_tokens: 16384
        input_cost_per_1k: 0.0004
        output_cost_per_1k: 0.0012
        capabilities: [function_calling, reasoning]
      'qwen-plus-2025-01-25':
        litellm_id: 'dashscope/qwen-plus-2025-01-25'
        mode: chat
        max_input_tokens: 129024
        max_output_tokens: 8192
        input_cost_per_1k: 0.0004
        output_cost_per_1k: 0.0012
        capabilities: [function_calling, reasoning]
      'qwen-plus-2025-04-28':
        litellm_id: 'dashscope/qwen-plus-2025-04-28'
        mode: chat
        max_input_tokens: 129024
        max_output_tokens: 16384
        input_cost_per_1k: 0.0004
        output_cost_per_1k: 0.0012
        capabilities: [function_calling, reasoning]
      'qwen-plus-2025-07-14':
        litellm_id: 'dashscope/qwen-plus-2025-07-14'
        mode: chat
        max_input_tokens: 129024
        max_output_tokens: 16384
        input_cost_per_1k: 0.0004
        output_cost_per_1k: 0.0012
        capabilities: [function_calling, reasoning]
      'qwen-plus-2025-07-28':
        litellm_id: 'dashscope/qwen-plus-2025-07-28'
        mode: chat
        max_input_tokens: 997952
        max_output_tokens: 32768
        capabilities: [function_calling, reasoning]
      'qwen-plus-2025-09-11':
        litellm_id: 'dashscope/qwen-plus-2025-09-11'
        mode: chat
        max_input_tokens: 997952
        max_output_tokens: 32768
        capabilities: [function_calling, reasoning]
      'qwen-plus-latest':
        litellm_id: 'dashscope/qwen-plus-latest'
        mode: chat
        max_input_tokens: 997952
        max_output_tokens: 32768
        capabilities: [function_calling, reasoning]
      'qwen-turbo':
        litellm_id: 'dashscope/qwen-turbo'
        mode: chat
        max_input_tokens: 129024
        max_output_tokens: 16384
        input_cost_per_1k: 5e-05
        output_cost_per_1k: 0.0002
        capabilities: [function_calling, reasoning]
      'qwen-turbo-2024-11-01':
        litellm_id: 'dashscope/qwen-turbo-2024-11-01'
        mode: chat
        max_input_tokens: 1000000
        max_output_tokens: 8192
        input_cost_per_1k: 5e-05
        output_cost_per_1k: 0.0002
        capabilities: [function_calling, reasoning]
      'qwen-turbo-2025-04-28':
        litellm_id: 'dashscope/qwen-turbo-2025-04-28'
        mode: chat
        max_input_tokens: 1000000
        max_output_tokens: 16384
        input_cost_per_1k: 5e-05
        output_cost_per_1k: 0.0002
        capabilities: [function_calling, reasoning]
      'qwen-turbo-latest':
        litellm_id: 'dashscope/qwen-turbo-latest'
        mode: chat
        max_input_tokens: 1000000
        max_output_tokens: 16384
        input_cost_per_1k: 5e-05
        output_cost_per_1k: 0.0002
        capabilities: [function_calling, reasoning]
      'qwen3-30b-a3b':
        litellm_id: 'dashscope/qwen3-30b-a3b'
        mode: chat
        max_input_tokens: 129024
        max_output_tokens: 16384
        capabilities: [function_calling, reasoning]
      'qwen3-coder-flash':
        litellm_id: 'dashscope/qwen3-coder-flash'
        mode: chat
        max_input_tokens: 997952
        max_output_tokens: 65536
        capabilities: [function_calling, reasoning]
      'qwen3-coder-flash-2025-07-28':
        litellm_id: 'dashscope/qwen3-coder-flash-2025-07-28'
        mode: chat
        max_input_tokens: 997952
        max_output_tokens: 65536
        capabilities: [function_calling, reasoning]
      'qwen3-coder-plus':
        litellm_id: 'dashscope/qwen3-coder-plus'
        mode: chat
        max_input_tokens: 997952
        max_output_tokens: 65536
        capabilities: [function_calling, reasoning]
      'qwen3-coder-plus-2025-07-22':
        litellm_id: 'dashscope/qwen3-coder-plus-2025-07-22'
        mode: chat
        max_input_tokens: 997952
        max_output_tokens: 65536
        capabilities: [function_calling, reasoning]
      'qwen3-max-preview':
        litellm_id: 'dashscope/qwen3-max-preview'
        mode: chat
        max_input_tokens: 258048
        max_output_tokens: 65536
        capabilities: [function_calling, reasoning]
      'qwq-plus':
        litellm_id: 'dashscope/qwq-plus'
        mode: chat
        max_input_tokens: 98304
        max_output_tokens: 8192
        input_cost_per_1k: 0.0008
        output_cost_per_1k: 0.0024
        capabilities: [function_calling, reasoning]
  databricks:
    model_count: 28
    models:
      'databricks-bge-large-en':
        litellm_id: 'databricks/databricks-bge-large-en'
        mode: embedding
        max_input_tokens: 512
        input_cost_per_1k: 0.00010003
      'databricks-claude-3-7-sonnet':
        litellm_id: 'databricks/databricks-claude-3-7-sonnet'
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 128000
        input_cost_per_1k: 0.00299999
        output_cost_per_1k: 0.01500002
        capabilities: [function_calling, reasoning]
      'databricks-claude-haiku-4-5':
        litellm_id: 'databricks/databricks-claude-haiku-4-5'
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 64000
        input_cost_per_1k: 0.00100002
        output_cost_per_1k: 0.00500003
        capabilities: [function_calling, reasoning]
      'databricks-claude-opus-4':
        litellm_id: 'databricks/databricks-claude-opus-4'
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 32000
        input_cost_per_1k: 0.01500002
        output_cost_per_1k: 0.07500003
        capabilities: [function_calling, reasoning]
      'databricks-claude-opus-4-1':
        litellm_id: 'databricks/databricks-claude-opus-4-1'
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 32000
        input_cost_per_1k: 0.01500002
        output_cost_per_1k: 0.07500003
        capabilities: [function_calling, reasoning]
      'databricks-claude-opus-4-5':
        litellm_id: 'databricks/databricks-claude-opus-4-5'
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 64000
        input_cost_per_1k: 0.00500003
        output_cost_per_1k: 0.02500001
        capabilities: [function_calling, reasoning]
      'databricks-claude-sonnet-4':
        litellm_id: 'databricks/databricks-claude-sonnet-4'
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 64000
        input_cost_per_1k: 0.00299999
        output_cost_per_1k: 0.01500002
        capabilities: [function_calling, reasoning]
      'databricks-claude-sonnet-4-1':
        litellm_id: 'databricks/databricks-claude-sonnet-4-1'
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 64000
        input_cost_per_1k: 0.00299999
        output_cost_per_1k: 0.01500002
        capabilities: [function_calling, reasoning]
      'databricks-claude-sonnet-4-5':
        litellm_id: 'databricks/databricks-claude-sonnet-4-5'
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 64000
        input_cost_per_1k: 0.00299999
        output_cost_per_1k: 0.01500002
        capabilities: [function_calling, reasoning]
      'databricks-gemini-2-5-flash':
        litellm_id: 'databricks/databricks-gemini-2-5-flash'
        mode: chat
        max_input_tokens: 1048576
        max_output_tokens: 65535
        input_cost_per_1k: 0.00030002
        output_cost_per_1k: 0.00249998
        capabilities: [function_calling]
      'databricks-gemini-2-5-pro':
        litellm_id: 'databricks/databricks-gemini-2-5-pro'
        mode: chat
        max_input_tokens: 1048576
        max_output_tokens: 65536
        input_cost_per_1k: 0.00124999
        output_cost_per_1k: 0.00999999
        capabilities: [function_calling]
      'databricks-gemma-3-12b':
        litellm_id: 'databricks/databricks-gemma-3-12b'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 32000
        input_cost_per_1k: 0.00015001
        output_cost_per_1k: 0.00050001
      'databricks-gpt-5':
        litellm_id: 'databricks/databricks-gpt-5'
        mode: chat
        max_input_tokens: 400000
        max_output_tokens: 128000
        input_cost_per_1k: 0.00124999
        output_cost_per_1k: 0.00999999
      'databricks-gpt-5-1':
        litellm_id: 'databricks/databricks-gpt-5-1'
        mode: chat
        max_input_tokens: 400000
        max_output_tokens: 128000
        input_cost_per_1k: 0.00124999
        output_cost_per_1k: 0.00999999
      'databricks-gpt-5-mini':
        litellm_id: 'databricks/databricks-gpt-5-mini'
        mode: chat
        max_input_tokens: 400000
        max_output_tokens: 128000
        input_cost_per_1k: 0.00024997
        output_cost_per_1k: 0.00199997
      'databricks-gpt-5-nano':
        litellm_id: 'databricks/databricks-gpt-5-nano'
        mode: chat
        max_input_tokens: 400000
        max_output_tokens: 128000
        input_cost_per_1k: 4.998e-05
        output_cost_per_1k: 0.00039998
      'databricks-gpt-oss-120b':
        litellm_id: 'databricks/databricks-gpt-oss-120b'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.00015001
        output_cost_per_1k: 0.00059997
      'databricks-gpt-oss-20b':
        litellm_id: 'databricks/databricks-gpt-oss-20b'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 7e-05
        output_cost_per_1k: 0.00030002
      'databricks-gte-large-en':
        litellm_id: 'databricks/databricks-gte-large-en'
        mode: embedding
        max_input_tokens: 8192
        input_cost_per_1k: 0.00012999
      'databricks-llama-2-70b-chat':
        litellm_id: 'databricks/databricks-llama-2-70b-chat'
        mode: chat
        max_input_tokens: 4096
        max_output_tokens: 4096
        input_cost_per_1k: 0.00050001
        output_cost_per_1k: 0.00150003
      'databricks-llama-4-maverick':
        litellm_id: 'databricks/databricks-llama-4-maverick'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 128000
        input_cost_per_1k: 0.00050001
        output_cost_per_1k: 0.00150003
      'databricks-meta-llama-3-1-405b-instruct':
        litellm_id: 'databricks/databricks-meta-llama-3-1-405b-instruct'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 128000
        input_cost_per_1k: 0.00500003
        output_cost_per_1k: 0.01500002
      'databricks-meta-llama-3-1-8b-instruct':
        litellm_id: 'databricks/databricks-meta-llama-3-1-8b-instruct'
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 128000
        input_cost_per_1k: 0.00015001
        output_cost_per_1k: 0.00045003
      'databricks-meta-llama-3-3-70b-instruct':
        litellm_id: 'databricks/databricks-meta-llama-3-3-70b-instruct'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 128000
        input_cost_per_1k: 0.00050001
        output_cost_per_1k: 0.00150003
      'databricks-meta-llama-3-70b-instruct':
        litellm_id: 'databricks/databricks-meta-llama-3-70b-instruct'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 128000
        input_cost_per_1k: 0.00100002
        output_cost_per_1k: 0.00299999
      'databricks-mixtral-8x7b-instruct':
        litellm_id: 'databricks/databricks-mixtral-8x7b-instruct'
        mode: chat
        max_input_tokens: 4096
        max_output_tokens: 4096
        input_cost_per_1k: 0.00050001
        output_cost_per_1k: 0.00100002
      'databricks-mpt-30b-instruct':
        litellm_id: 'databricks/databricks-mpt-30b-instruct'
        mode: chat
        max_input_tokens: 8192
        max_output_tokens: 8192
        input_cost_per_1k: 0.00100002
        output_cost_per_1k: 0.00100002
      'databricks-mpt-7b-instruct':
        litellm_id: 'databricks/databricks-mpt-7b-instruct'
        mode: chat
        max_input_tokens: 8192
        max_output_tokens: 8192
        input_cost_per_1k: 0.00050001
  dataforseo:
    model_count: 1
    models:
      'search':
        litellm_id: 'dataforseo/search'
        mode: search
  deepgram:
    model_count: 36
    models:
      'base':
        litellm_id: 'deepgram/base'
        mode: audio_transcription
      'base-conversationalai':
        litellm_id: 'deepgram/base-conversationalai'
        mode: audio_transcription
      'base-finance':
        litellm_id: 'deepgram/base-finance'
        mode: audio_transcription
      'base-general':
        litellm_id: 'deepgram/base-general'
        mode: audio_transcription
      'base-meeting':
        litellm_id: 'deepgram/base-meeting'
        mode: audio_transcription
      'base-phonecall':
        litellm_id: 'deepgram/base-phonecall'
        mode: audio_transcription
      'base-video':
        litellm_id: 'deepgram/base-video'
        mode: audio_transcription
      'base-voicemail':
        litellm_id: 'deepgram/base-voicemail'
        mode: audio_transcription
      'enhanced':
        litellm_id: 'deepgram/enhanced'
        mode: audio_transcription
      'enhanced-finance':
        litellm_id: 'deepgram/enhanced-finance'
        mode: audio_transcription
      'enhanced-general':
        litellm_id: 'deepgram/enhanced-general'
        mode: audio_transcription
      'enhanced-meeting':
        litellm_id: 'deepgram/enhanced-meeting'
        mode: audio_transcription
      'enhanced-phonecall':
        litellm_id: 'deepgram/enhanced-phonecall'
        mode: audio_transcription
      'nova':
        litellm_id: 'deepgram/nova'
        mode: audio_transcription
      'nova-2':
        litellm_id: 'deepgram/nova-2'
        mode: audio_transcription
      'nova-2-atc':
        litellm_id: 'deepgram/nova-2-atc'
        mode: audio_transcription
      'nova-2-automotive':
        litellm_id: 'deepgram/nova-2-automotive'
        mode: audio_transcription
      'nova-2-conversationalai':
        litellm_id: 'deepgram/nova-2-conversationalai'
        mode: audio_transcription
      'nova-2-drivethru':
        litellm_id: 'deepgram/nova-2-drivethru'
        mode: audio_transcription
      'nova-2-finance':
        litellm_id: 'deepgram/nova-2-finance'
        mode: audio_transcription
      'nova-2-general':
        litellm_id: 'deepgram/nova-2-general'
        mode: audio_transcription
      'nova-2-meeting':
        litellm_id: 'deepgram/nova-2-meeting'
        mode: audio_transcription
      'nova-2-phonecall':
        litellm_id: 'deepgram/nova-2-phonecall'
        mode: audio_transcription
      'nova-2-video':
        litellm_id: 'deepgram/nova-2-video'
        mode: audio_transcription
      'nova-2-voicemail':
        litellm_id: 'deepgram/nova-2-voicemail'
        mode: audio_transcription
      'nova-3':
        litellm_id: 'deepgram/nova-3'
        mode: audio_transcription
      'nova-3-general':
        litellm_id: 'deepgram/nova-3-general'
        mode: audio_transcription
      'nova-3-medical':
        litellm_id: 'deepgram/nova-3-medical'
        mode: audio_transcription
      'nova-general':
        litellm_id: 'deepgram/nova-general'
        mode: audio_transcription
      'nova-phonecall':
        litellm_id: 'deepgram/nova-phonecall'
        mode: audio_transcription
      'whisper':
        litellm_id: 'deepgram/whisper'
        mode: audio_transcription
      'whisper-base':
        litellm_id: 'deepgram/whisper-base'
        mode: audio_transcription
      'whisper-large':
        litellm_id: 'deepgram/whisper-large'
        mode: audio_transcription
      'whisper-medium':
        litellm_id: 'deepgram/whisper-medium'
        mode: audio_transcription
      'whisper-small':
        litellm_id: 'deepgram/whisper-small'
        mode: audio_transcription
      'whisper-tiny':
        litellm_id: 'deepgram/whisper-tiny'
        mode: audio_transcription
  deepinfra:
    model_count: 67
    models:
      'Gryphe/MythoMax-L2-13b':
        litellm_id: 'deepinfra/Gryphe/MythoMax-L2-13b'
        mode: chat
        max_input_tokens: 4096
        max_output_tokens: 4096
        input_cost_per_1k: 8e-05
        output_cost_per_1k: 9e-05
      'NousResearch/Hermes-3-Llama-3.1-405B':
        litellm_id: 'deepinfra/NousResearch/Hermes-3-Llama-3.1-405B'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.001
        output_cost_per_1k: 0.001
      'NousResearch/Hermes-3-Llama-3.1-70B':
        litellm_id: 'deepinfra/NousResearch/Hermes-3-Llama-3.1-70B'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.0003
        output_cost_per_1k: 0.0003
      'Qwen/QwQ-32B':
        litellm_id: 'deepinfra/Qwen/QwQ-32B'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.00015
        output_cost_per_1k: 0.0004
      'Qwen/Qwen2.5-72B-Instruct':
        litellm_id: 'deepinfra/Qwen/Qwen2.5-72B-Instruct'
        mode: chat
        max_input_tokens: 32768
        max_output_tokens: 32768
        input_cost_per_1k: 0.00012
        output_cost_per_1k: 0.00039
      'Qwen/Qwen2.5-7B-Instruct':
        litellm_id: 'deepinfra/Qwen/Qwen2.5-7B-Instruct'
        mode: chat
        max_input_tokens: 32768
        max_output_tokens: 32768
        input_cost_per_1k: 4e-05
        output_cost_per_1k: 0.0001
      'Qwen/Qwen2.5-VL-32B-Instruct':
        litellm_id: 'deepinfra/Qwen/Qwen2.5-VL-32B-Instruct'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 128000
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0006
        capabilities: [vision]
      'Qwen/Qwen3-14B':
        litellm_id: 'deepinfra/Qwen/Qwen3-14B'
        mode: chat
        max_input_tokens: 40960
        max_output_tokens: 40960
        input_cost_per_1k: 6e-05
        output_cost_per_1k: 0.00024
      'Qwen/Qwen3-235B-A22B':
        litellm_id: 'deepinfra/Qwen/Qwen3-235B-A22B'
        mode: chat
        max_input_tokens: 40960
        max_output_tokens: 40960
        input_cost_per_1k: 0.00018
        output_cost_per_1k: 0.00054
      'Qwen/Qwen3-235B-A22B-Instruct-2507':
        litellm_id: 'deepinfra/Qwen/Qwen3-235B-A22B-Instruct-2507'
        mode: chat
        max_input_tokens: 262144
        max_output_tokens: 262144
        input_cost_per_1k: 9e-05
        output_cost_per_1k: 0.0006
      'Qwen/Qwen3-235B-A22B-Thinking-2507':
        litellm_id: 'deepinfra/Qwen/Qwen3-235B-A22B-Thinking-2507'
        mode: chat
        max_input_tokens: 262144
        max_output_tokens: 262144
        input_cost_per_1k: 0.0003
        output_cost_per_1k: 0.0029
      'Qwen/Qwen3-30B-A3B':
        litellm_id: 'deepinfra/Qwen/Qwen3-30B-A3B'
        mode: chat
        max_input_tokens: 40960
        max_output_tokens: 40960
        input_cost_per_1k: 8e-05
        output_cost_per_1k: 0.00029
      'Qwen/Qwen3-32B':
        litellm_id: 'deepinfra/Qwen/Qwen3-32B'
        mode: chat
        max_input_tokens: 40960
        max_output_tokens: 40960
        input_cost_per_1k: 0.0001
        output_cost_per_1k: 0.00028
      'Qwen/Qwen3-Coder-480B-A35B-Instruct':
        litellm_id: 'deepinfra/Qwen/Qwen3-Coder-480B-A35B-Instruct'
        mode: chat
        max_input_tokens: 262144
        max_output_tokens: 262144
        input_cost_per_1k: 0.0004
        output_cost_per_1k: 0.0016
      'Qwen/Qwen3-Coder-480B-A35B-Instruct-Turbo':
        litellm_id: 'deepinfra/Qwen/Qwen3-Coder-480B-A35B-Instruct-Turbo'
        mode: chat
        max_input_tokens: 262144
        max_output_tokens: 262144
        input_cost_per_1k: 0.00029
        output_cost_per_1k: 0.0012
      'Qwen/Qwen3-Next-80B-A3B-Instruct':
        litellm_id: 'deepinfra/Qwen/Qwen3-Next-80B-A3B-Instruct'
        mode: chat
        max_input_tokens: 262144
        max_output_tokens: 262144
        input_cost_per_1k: 0.00014
        output_cost_per_1k: 0.0014
      'Qwen/Qwen3-Next-80B-A3B-Thinking':
        litellm_id: 'deepinfra/Qwen/Qwen3-Next-80B-A3B-Thinking'
        mode: chat
        max_input_tokens: 262144
        max_output_tokens: 262144
        input_cost_per_1k: 0.00014
        output_cost_per_1k: 0.0014
      'Sao10K/L3-8B-Lunaris-v1-Turbo':
        litellm_id: 'deepinfra/Sao10K/L3-8B-Lunaris-v1-Turbo'
        mode: chat
        max_input_tokens: 8192
        max_output_tokens: 8192
        input_cost_per_1k: 4e-05
        output_cost_per_1k: 5e-05
      'Sao10K/L3.1-70B-Euryale-v2.2':
        litellm_id: 'deepinfra/Sao10K/L3.1-70B-Euryale-v2.2'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.00065
        output_cost_per_1k: 0.00075
      'Sao10K/L3.3-70B-Euryale-v2.3':
        litellm_id: 'deepinfra/Sao10K/L3.3-70B-Euryale-v2.3'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.00065
        output_cost_per_1k: 0.00075
      'allenai/olmOCR-7B-0725-FP8':
        litellm_id: 'deepinfra/allenai/olmOCR-7B-0725-FP8'
        mode: chat
        max_input_tokens: 16384
        max_output_tokens: 16384
        input_cost_per_1k: 0.00027
        output_cost_per_1k: 0.0015
      'anthropic/claude-3-7-sonnet-latest':
        litellm_id: 'deepinfra/anthropic/claude-3-7-sonnet-latest'
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 200000
        input_cost_per_1k: 0.0033
        output_cost_per_1k: 0.0165
      'anthropic/claude-4-opus':
        litellm_id: 'deepinfra/anthropic/claude-4-opus'
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 200000
        input_cost_per_1k: 0.0165
        output_cost_per_1k: 0.0825
      'anthropic/claude-4-sonnet':
        litellm_id: 'deepinfra/anthropic/claude-4-sonnet'
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 200000
        input_cost_per_1k: 0.0033
        output_cost_per_1k: 0.0165
      'deepseek-ai/DeepSeek-R1':
        litellm_id: 'deepinfra/deepseek-ai/DeepSeek-R1'
        mode: chat
        max_input_tokens: 163840
        max_output_tokens: 163840
        input_cost_per_1k: 0.0007
        output_cost_per_1k: 0.0024
      'deepseek-ai/DeepSeek-R1-0528':
        litellm_id: 'deepinfra/deepseek-ai/DeepSeek-R1-0528'
        mode: chat
        max_input_tokens: 163840
        max_output_tokens: 163840
        input_cost_per_1k: 0.0005
        output_cost_per_1k: 0.00215
      'deepseek-ai/DeepSeek-R1-0528-Turbo':
        litellm_id: 'deepinfra/deepseek-ai/DeepSeek-R1-0528-Turbo'
        mode: chat
        max_input_tokens: 32768
        max_output_tokens: 32768
        input_cost_per_1k: 0.001
        output_cost_per_1k: 0.003
      'deepseek-ai/DeepSeek-R1-Distill-Llama-70B':
        litellm_id: 'deepinfra/deepseek-ai/DeepSeek-R1-Distill-Llama-70B'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0006
      'deepseek-ai/DeepSeek-R1-Distill-Qwen-32B':
        litellm_id: 'deepinfra/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.00027
        output_cost_per_1k: 0.00027
      'deepseek-ai/DeepSeek-R1-Turbo':
        litellm_id: 'deepinfra/deepseek-ai/DeepSeek-R1-Turbo'
        mode: chat
        max_input_tokens: 40960
        max_output_tokens: 40960
        input_cost_per_1k: 0.001
        output_cost_per_1k: 0.003
      'deepseek-ai/DeepSeek-V3':
        litellm_id: 'deepinfra/deepseek-ai/DeepSeek-V3'
        mode: chat
        max_input_tokens: 163840
        max_output_tokens: 163840
        input_cost_per_1k: 0.00038
        output_cost_per_1k: 0.00089
      'deepseek-ai/DeepSeek-V3-0324':
        litellm_id: 'deepinfra/deepseek-ai/DeepSeek-V3-0324'
        mode: chat
        max_input_tokens: 163840
        max_output_tokens: 163840
        input_cost_per_1k: 0.00025
        output_cost_per_1k: 0.00088
      'deepseek-ai/DeepSeek-V3.1':
        litellm_id: 'deepinfra/deepseek-ai/DeepSeek-V3.1'
        mode: chat
        max_input_tokens: 163840
        max_output_tokens: 163840
        input_cost_per_1k: 0.00027
        output_cost_per_1k: 0.001
        capabilities: [reasoning]
      'deepseek-ai/DeepSeek-V3.1-Terminus':
        litellm_id: 'deepinfra/deepseek-ai/DeepSeek-V3.1-Terminus'
        mode: chat
        max_input_tokens: 163840
        max_output_tokens: 163840
        input_cost_per_1k: 0.00027
        output_cost_per_1k: 0.001
      'google/gemini-2.0-flash-001':
        litellm_id: 'deepinfra/google/gemini-2.0-flash-001'
        mode: chat
        max_input_tokens: 1000000
        max_output_tokens: 1000000
        input_cost_per_1k: 0.0001
        output_cost_per_1k: 0.0004
      'google/gemini-2.5-flash':
        litellm_id: 'deepinfra/google/gemini-2.5-flash'
        mode: chat
        max_input_tokens: 1000000
        max_output_tokens: 1000000
        input_cost_per_1k: 0.0003
        output_cost_per_1k: 0.0025
      'google/gemini-2.5-pro':
        litellm_id: 'deepinfra/google/gemini-2.5-pro'
        mode: chat
        max_input_tokens: 1000000
        max_output_tokens: 1000000
        input_cost_per_1k: 0.00125
        output_cost_per_1k: 0.01
      'google/gemma-3-12b-it':
        litellm_id: 'deepinfra/google/gemma-3-12b-it'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 5e-05
        output_cost_per_1k: 0.0001
      'google/gemma-3-27b-it':
        litellm_id: 'deepinfra/google/gemma-3-27b-it'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 9e-05
        output_cost_per_1k: 0.00016
      'google/gemma-3-4b-it':
        litellm_id: 'deepinfra/google/gemma-3-4b-it'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 4e-05
        output_cost_per_1k: 8e-05
      'meta-llama/Llama-3.2-11B-Vision-Instruct':
        litellm_id: 'deepinfra/meta-llama/Llama-3.2-11B-Vision-Instruct'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 4.9e-05
        output_cost_per_1k: 4.9e-05
      'meta-llama/Llama-3.2-3B-Instruct':
        litellm_id: 'deepinfra/meta-llama/Llama-3.2-3B-Instruct'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 2e-05
        output_cost_per_1k: 2e-05
      'meta-llama/Llama-3.3-70B-Instruct':
        litellm_id: 'deepinfra/meta-llama/Llama-3.3-70B-Instruct'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.00023
        output_cost_per_1k: 0.0004
      'meta-llama/Llama-3.3-70B-Instruct-Turbo':
        litellm_id: 'deepinfra/meta-llama/Llama-3.3-70B-Instruct-Turbo'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.00013
        output_cost_per_1k: 0.00039
      'meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8':
        litellm_id: 'deepinfra/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8'
        mode: chat
        max_input_tokens: 1048576
        max_output_tokens: 1048576
        input_cost_per_1k: 0.00015
        output_cost_per_1k: 0.0006
      'meta-llama/Llama-4-Scout-17B-16E-Instruct':
        litellm_id: 'deepinfra/meta-llama/Llama-4-Scout-17B-16E-Instruct'
        mode: chat
        max_input_tokens: 327680
        max_output_tokens: 327680
        input_cost_per_1k: 8e-05
        output_cost_per_1k: 0.0003
      'meta-llama/Llama-Guard-3-8B':
        litellm_id: 'deepinfra/meta-llama/Llama-Guard-3-8B'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 5.5e-05
        output_cost_per_1k: 5.5e-05
      'meta-llama/Llama-Guard-4-12B':
        litellm_id: 'deepinfra/meta-llama/Llama-Guard-4-12B'
        mode: chat
        max_input_tokens: 163840
        max_output_tokens: 163840
        input_cost_per_1k: 0.00018
        output_cost_per_1k: 0.00018
      'meta-llama/Meta-Llama-3-8B-Instruct':
        litellm_id: 'deepinfra/meta-llama/Meta-Llama-3-8B-Instruct'
        mode: chat
        max_input_tokens: 8192
        max_output_tokens: 8192
        input_cost_per_1k: 3e-05
        output_cost_per_1k: 6e-05
      'meta-llama/Meta-Llama-3.1-70B-Instruct':
        litellm_id: 'deepinfra/meta-llama/Meta-Llama-3.1-70B-Instruct'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.0004
        output_cost_per_1k: 0.0004
      'meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo':
        litellm_id: 'deepinfra/meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.0001
        output_cost_per_1k: 0.00028
      'meta-llama/Meta-Llama-3.1-8B-Instruct':
        litellm_id: 'deepinfra/meta-llama/Meta-Llama-3.1-8B-Instruct'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 3e-05
        output_cost_per_1k: 5e-05
      'meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo':
        litellm_id: 'deepinfra/meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 2e-05
        output_cost_per_1k: 3e-05
      'microsoft/WizardLM-2-8x22B':
        litellm_id: 'deepinfra/microsoft/WizardLM-2-8x22B'
        mode: chat
        max_input_tokens: 65536
        max_output_tokens: 65536
        input_cost_per_1k: 0.00048
        output_cost_per_1k: 0.00048
      'microsoft/phi-4':
        litellm_id: 'deepinfra/microsoft/phi-4'
        mode: chat
        max_input_tokens: 16384
        max_output_tokens: 16384
        input_cost_per_1k: 7e-05
        output_cost_per_1k: 0.00014
      'mistralai/Mistral-Nemo-Instruct-2407':
        litellm_id: 'deepinfra/mistralai/Mistral-Nemo-Instruct-2407'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 2e-05
        output_cost_per_1k: 4e-05
      'mistralai/Mistral-Small-24B-Instruct-2501':
        litellm_id: 'deepinfra/mistralai/Mistral-Small-24B-Instruct-2501'
        mode: chat
        max_input_tokens: 32768
        max_output_tokens: 32768
        input_cost_per_1k: 5e-05
        output_cost_per_1k: 8e-05
      'mistralai/Mistral-Small-3.2-24B-Instruct-2506':
        litellm_id: 'deepinfra/mistralai/Mistral-Small-3.2-24B-Instruct-2506'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 128000
        input_cost_per_1k: 7.5e-05
        output_cost_per_1k: 0.0002
      'mistralai/Mixtral-8x7B-Instruct-v0.1':
        litellm_id: 'deepinfra/mistralai/Mixtral-8x7B-Instruct-v0.1'
        mode: chat
        max_input_tokens: 32768
        max_output_tokens: 32768
        input_cost_per_1k: 0.0004
        output_cost_per_1k: 0.0004
      'moonshotai/Kimi-K2-Instruct':
        litellm_id: 'deepinfra/moonshotai/Kimi-K2-Instruct'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.0005
        output_cost_per_1k: 0.002
      'moonshotai/Kimi-K2-Instruct-0905':
        litellm_id: 'deepinfra/moonshotai/Kimi-K2-Instruct-0905'
        mode: chat
        max_input_tokens: 262144
        max_output_tokens: 262144
        input_cost_per_1k: 0.0005
        output_cost_per_1k: 0.002
      'nvidia/Llama-3.1-Nemotron-70B-Instruct':
        litellm_id: 'deepinfra/nvidia/Llama-3.1-Nemotron-70B-Instruct'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.0006
        output_cost_per_1k: 0.0006
      'nvidia/Llama-3.3-Nemotron-Super-49B-v1.5':
        litellm_id: 'deepinfra/nvidia/Llama-3.3-Nemotron-Super-49B-v1.5'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.0001
        output_cost_per_1k: 0.0004
      'nvidia/NVIDIA-Nemotron-Nano-9B-v2':
        litellm_id: 'deepinfra/nvidia/NVIDIA-Nemotron-Nano-9B-v2'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 4e-05
        output_cost_per_1k: 0.00016
      'openai/gpt-oss-120b':
        litellm_id: 'deepinfra/openai/gpt-oss-120b'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 5e-05
        output_cost_per_1k: 0.00045
      'openai/gpt-oss-20b':
        litellm_id: 'deepinfra/openai/gpt-oss-20b'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 4e-05
        output_cost_per_1k: 0.00015
      'zai-org/GLM-4.5':
        litellm_id: 'deepinfra/zai-org/GLM-4.5'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.0004
        output_cost_per_1k: 0.0016
  deepseek:
    model_count: 8
    models:
      'deepseek-chat':
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 8192
        input_cost_per_1k: 0.0006
        output_cost_per_1k: 0.0017
        capabilities: [function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching]
      'deepseek-chat':
        litellm_id: 'deepseek/deepseek-chat'
        mode: chat
        max_input_tokens: 65536
        max_output_tokens: 8192
        input_cost_per_1k: 0.00027
        output_cost_per_1k: 0.0011
        capabilities: [function_calling, prompt_caching]
      'deepseek-coder':
        litellm_id: 'deepseek/deepseek-coder'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 4096
        input_cost_per_1k: 0.00014
        output_cost_per_1k: 0.00028
        capabilities: [function_calling, prompt_caching]
      'deepseek-r1':
        litellm_id: 'deepseek/deepseek-r1'
        mode: chat
        max_input_tokens: 65536
        max_output_tokens: 8192
        input_cost_per_1k: 0.00055
        output_cost_per_1k: 0.00219
        capabilities: [function_calling, prompt_caching, reasoning]
      'deepseek-reasoner':
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 65536
        input_cost_per_1k: 0.0006
        output_cost_per_1k: 0.0017
        capabilities: [system_messages, json_mode, prompt_caching, reasoning]
      'deepseek-reasoner':
        litellm_id: 'deepseek/deepseek-reasoner'
        mode: chat
        max_input_tokens: 65536
        max_output_tokens: 8192
        input_cost_per_1k: 0.00055
        output_cost_per_1k: 0.00219
        capabilities: [function_calling, prompt_caching, reasoning]
      'deepseek-v3':
        litellm_id: 'deepseek/deepseek-v3'
        mode: chat
        max_input_tokens: 65536
        max_output_tokens: 8192
        input_cost_per_1k: 0.00027
        output_cost_per_1k: 0.0011
        capabilities: [function_calling, prompt_caching]
      'deepseek-v3.2':
        litellm_id: 'deepseek/deepseek-v3.2'
        mode: chat
        max_input_tokens: 163840
        max_output_tokens: 163840
        input_cost_per_1k: 0.00028
        output_cost_per_1k: 0.0004
        capabilities: [function_calling, prompt_caching, reasoning]
  elevenlabs:
    model_count: 2
    models:
      'scribe_v1':
        litellm_id: 'elevenlabs/scribe_v1'
        mode: audio_transcription
      'scribe_v1_experimental':
        litellm_id: 'elevenlabs/scribe_v1_experimental'
        mode: audio_transcription
  exa_ai:
    model_count: 1
    models:
      'search':
        litellm_id: 'exa_ai/search'
        mode: search
  fal_ai:
    model_count: 12
    models:
      'bria/text-to-image/3.2':
        litellm_id: 'fal_ai/bria/text-to-image/3.2'
        mode: image
      'fal-ai/bytedance/dreamina/v3.1/text-to-image':
        litellm_id: 'fal_ai/fal-ai/bytedance/dreamina/v3.1/text-to-image'
        mode: image
      'fal-ai/bytedance/seedream/v3/text-to-image':
        litellm_id: 'fal_ai/fal-ai/bytedance/seedream/v3/text-to-image'
        mode: image
      'fal-ai/flux-pro/v1.1':
        litellm_id: 'fal_ai/fal-ai/flux-pro/v1.1'
        mode: image
      'fal-ai/flux-pro/v1.1-ultra':
        litellm_id: 'fal_ai/fal-ai/flux-pro/v1.1-ultra'
        mode: image
      'fal-ai/flux/schnell':
        litellm_id: 'fal_ai/fal-ai/flux/schnell'
        mode: image
      'fal-ai/ideogram/v3':
        litellm_id: 'fal_ai/fal-ai/ideogram/v3'
        mode: image
      'fal-ai/imagen4/preview':
        litellm_id: 'fal_ai/fal-ai/imagen4/preview'
        mode: image
      'fal-ai/imagen4/preview/fast':
        litellm_id: 'fal_ai/fal-ai/imagen4/preview/fast'
        mode: image
      'fal-ai/imagen4/preview/ultra':
        litellm_id: 'fal_ai/fal-ai/imagen4/preview/ultra'
        mode: image
      'fal-ai/recraft/v3/text-to-image':
        litellm_id: 'fal_ai/fal-ai/recraft/v3/text-to-image'
        mode: image
      'fal-ai/stable-diffusion-v35-medium':
        litellm_id: 'fal_ai/fal-ai/stable-diffusion-v35-medium'
        mode: image
  featherless_ai:
    model_count: 2
    models:
      'featherless-ai/Qwerky-72B':
        litellm_id: 'featherless_ai/featherless-ai/Qwerky-72B'
        mode: chat
        max_input_tokens: 32768
        max_output_tokens: 4096
      'featherless-ai/Qwerky-QwQ-32B':
        litellm_id: 'featherless_ai/featherless-ai/Qwerky-QwQ-32B'
        mode: chat
        max_input_tokens: 32768
        max_output_tokens: 4096
  firecrawl:
    model_count: 1
    models:
      'search':
        litellm_id: 'firecrawl/search'
        mode: search
  fireworks:
    model_count: 263
    models:
      'accounts/fireworks/models/':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/'
        mode: embedding
        max_input_tokens: 40960
        max_output_tokens: 40960
        input_cost_per_1k: 0.0001
      'accounts/fireworks/models/SSD-1B':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/SSD-1B'
        mode: image
        max_input_tokens: 4096
        max_output_tokens: 4096
        input_cost_per_1k: 1.3e-07
        output_cost_per_1k: 1.3e-07
      'accounts/fireworks/models/chronos-hermes-13b-v2':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/chronos-hermes-13b-v2'
        mode: chat
        max_input_tokens: 4096
        max_output_tokens: 4096
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
      'accounts/fireworks/models/code-llama-13b':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/code-llama-13b'
        mode: chat
        max_input_tokens: 16384
        max_output_tokens: 16384
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
      'accounts/fireworks/models/code-llama-13b-instruct':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/code-llama-13b-instruct'
        mode: chat
        max_input_tokens: 16384
        max_output_tokens: 16384
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
      'accounts/fireworks/models/code-llama-13b-python':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/code-llama-13b-python'
        mode: chat
        max_input_tokens: 16384
        max_output_tokens: 16384
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
      'accounts/fireworks/models/code-llama-34b':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/code-llama-34b'
        mode: chat
        max_input_tokens: 16384
        max_output_tokens: 16384
        input_cost_per_1k: 0.0009
        output_cost_per_1k: 0.0009
      'accounts/fireworks/models/code-llama-34b-instruct':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/code-llama-34b-instruct'
        mode: chat
        max_input_tokens: 16384
        max_output_tokens: 16384
        input_cost_per_1k: 0.0009
        output_cost_per_1k: 0.0009
      'accounts/fireworks/models/code-llama-34b-python':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/code-llama-34b-python'
        mode: chat
        max_input_tokens: 16384
        max_output_tokens: 16384
        input_cost_per_1k: 0.0009
        output_cost_per_1k: 0.0009
      'accounts/fireworks/models/code-llama-70b':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/code-llama-70b'
        mode: chat
        max_input_tokens: 4096
        max_output_tokens: 4096
        input_cost_per_1k: 0.0009
        output_cost_per_1k: 0.0009
      'accounts/fireworks/models/code-llama-70b-instruct':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/code-llama-70b-instruct'
        mode: chat
        max_input_tokens: 4096
        max_output_tokens: 4096
        input_cost_per_1k: 0.0009
        output_cost_per_1k: 0.0009
      'accounts/fireworks/models/code-llama-70b-python':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/code-llama-70b-python'
        mode: chat
        max_input_tokens: 4096
        max_output_tokens: 4096
        input_cost_per_1k: 0.0009
        output_cost_per_1k: 0.0009
      'accounts/fireworks/models/code-llama-7b':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/code-llama-7b'
        mode: chat
        max_input_tokens: 16384
        max_output_tokens: 16384
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
      'accounts/fireworks/models/code-llama-7b-instruct':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/code-llama-7b-instruct'
        mode: chat
        max_input_tokens: 16384
        max_output_tokens: 16384
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
      'accounts/fireworks/models/code-llama-7b-python':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/code-llama-7b-python'
        mode: chat
        max_input_tokens: 16384
        max_output_tokens: 16384
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
      'accounts/fireworks/models/code-qwen-1p5-7b':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/code-qwen-1p5-7b'
        mode: chat
        max_input_tokens: 65536
        max_output_tokens: 65536
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
      'accounts/fireworks/models/codegemma-2b':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/codegemma-2b'
        mode: chat
        max_input_tokens: 8192
        max_output_tokens: 8192
        input_cost_per_1k: 0.0001
        output_cost_per_1k: 0.0001
      'accounts/fireworks/models/codegemma-7b':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/codegemma-7b'
        mode: chat
        max_input_tokens: 8192
        max_output_tokens: 8192
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
      'accounts/fireworks/models/cogito-671b-v2-p1':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/cogito-671b-v2-p1'
        mode: chat
        max_input_tokens: 163840
        max_output_tokens: 163840
        input_cost_per_1k: 0.0012
        output_cost_per_1k: 0.0012
      'accounts/fireworks/models/cogito-v1-preview-llama-3b':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/cogito-v1-preview-llama-3b'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.0001
        output_cost_per_1k: 0.0001
      'accounts/fireworks/models/cogito-v1-preview-llama-70b':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/cogito-v1-preview-llama-70b'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.0009
        output_cost_per_1k: 0.0009
      'accounts/fireworks/models/cogito-v1-preview-llama-8b':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/cogito-v1-preview-llama-8b'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
      'accounts/fireworks/models/cogito-v1-preview-qwen-14b':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/cogito-v1-preview-qwen-14b'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
      'accounts/fireworks/models/cogito-v1-preview-qwen-32b':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/cogito-v1-preview-qwen-32b'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.0009
        output_cost_per_1k: 0.0009
      'accounts/fireworks/models/dbrx-instruct':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/dbrx-instruct'
        mode: chat
        max_input_tokens: 32768
        max_output_tokens: 32768
        input_cost_per_1k: 0.0012
        output_cost_per_1k: 0.0012
      'accounts/fireworks/models/deepseek-coder-1b-base':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/deepseek-coder-1b-base'
        mode: chat
        max_input_tokens: 16384
        max_output_tokens: 16384
        input_cost_per_1k: 0.0001
        output_cost_per_1k: 0.0001
      'accounts/fireworks/models/deepseek-coder-33b-instruct':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/deepseek-coder-33b-instruct'
        mode: chat
        max_input_tokens: 16384
        max_output_tokens: 16384
        input_cost_per_1k: 0.0009
        output_cost_per_1k: 0.0009
      'accounts/fireworks/models/deepseek-coder-7b-base':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/deepseek-coder-7b-base'
        mode: chat
        max_input_tokens: 4096
        max_output_tokens: 4096
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
      'accounts/fireworks/models/deepseek-coder-7b-base-v1p5':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/deepseek-coder-7b-base-v1p5'
        mode: chat
        max_input_tokens: 4096
        max_output_tokens: 4096
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
      'accounts/fireworks/models/deepseek-coder-7b-instruct-v1p5':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/deepseek-coder-7b-instruct-v1p5'
        mode: chat
        max_input_tokens: 4096
        max_output_tokens: 4096
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
      'accounts/fireworks/models/deepseek-coder-v2-instruct':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/deepseek-coder-v2-instruct'
        mode: chat
        max_input_tokens: 65536
        max_output_tokens: 65536
        input_cost_per_1k: 0.0012
        output_cost_per_1k: 0.0012
        capabilities: [json_mode]
      'accounts/fireworks/models/deepseek-coder-v2-lite-base':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/deepseek-coder-v2-lite-base'
        mode: chat
        max_input_tokens: 163840
        max_output_tokens: 163840
        input_cost_per_1k: 0.0005
        output_cost_per_1k: 0.0005
      'accounts/fireworks/models/deepseek-coder-v2-lite-instruct':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/deepseek-coder-v2-lite-instruct'
        mode: chat
        max_input_tokens: 163840
        max_output_tokens: 163840
        input_cost_per_1k: 0.0005
        output_cost_per_1k: 0.0005
      'accounts/fireworks/models/deepseek-prover-v2':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/deepseek-prover-v2'
        mode: chat
        max_input_tokens: 163840
        max_output_tokens: 163840
        input_cost_per_1k: 0.0012
        output_cost_per_1k: 0.0012
      'accounts/fireworks/models/deepseek-r1':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/deepseek-r1'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 20480
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.008
        capabilities: [json_mode]
      'accounts/fireworks/models/deepseek-r1-0528':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/deepseek-r1-0528'
        mode: chat
        max_input_tokens: 160000
        max_output_tokens: 160000
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.008
        capabilities: [json_mode]
      'accounts/fireworks/models/deepseek-r1-0528-distill-qwen3-8b':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/deepseek-r1-0528-distill-qwen3-8b'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
      'accounts/fireworks/models/deepseek-r1-basic':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/deepseek-r1-basic'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 20480
        input_cost_per_1k: 0.00055
        output_cost_per_1k: 0.00219
        capabilities: [json_mode]
      'accounts/fireworks/models/deepseek-r1-distill-llama-70b':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/deepseek-r1-distill-llama-70b'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.0009
        output_cost_per_1k: 0.0009
      'accounts/fireworks/models/deepseek-r1-distill-llama-8b':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/deepseek-r1-distill-llama-8b'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
      'accounts/fireworks/models/deepseek-r1-distill-qwen-14b':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/deepseek-r1-distill-qwen-14b'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
      'accounts/fireworks/models/deepseek-r1-distill-qwen-1p5b':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/deepseek-r1-distill-qwen-1p5b'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.0001
        output_cost_per_1k: 0.0001
      'accounts/fireworks/models/deepseek-r1-distill-qwen-32b':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/deepseek-r1-distill-qwen-32b'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.0009
        output_cost_per_1k: 0.0009
      'accounts/fireworks/models/deepseek-r1-distill-qwen-7b':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/deepseek-r1-distill-qwen-7b'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
      'accounts/fireworks/models/deepseek-v2-lite-chat':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/deepseek-v2-lite-chat'
        mode: chat
        max_input_tokens: 163840
        max_output_tokens: 163840
        input_cost_per_1k: 0.0005
        output_cost_per_1k: 0.0005
      'accounts/fireworks/models/deepseek-v2p5':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/deepseek-v2p5'
        mode: chat
        max_input_tokens: 32768
        max_output_tokens: 32768
        input_cost_per_1k: 0.0012
        output_cost_per_1k: 0.0012
      'accounts/fireworks/models/deepseek-v3':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/deepseek-v3'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 8192
        input_cost_per_1k: 0.0009
        output_cost_per_1k: 0.0009
        capabilities: [json_mode]
      'accounts/fireworks/models/deepseek-v3-0324':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/deepseek-v3-0324'
        mode: chat
        max_input_tokens: 163840
        max_output_tokens: 163840
        input_cost_per_1k: 0.0009
        output_cost_per_1k: 0.0009
        capabilities: [json_mode]
      'accounts/fireworks/models/deepseek-v3p1':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/deepseek-v3p1'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 8192
        input_cost_per_1k: 0.00056
        output_cost_per_1k: 0.00168
        capabilities: [json_mode, reasoning]
      'accounts/fireworks/models/deepseek-v3p1-terminus':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/deepseek-v3p1-terminus'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 8192
        input_cost_per_1k: 0.00056
        output_cost_per_1k: 0.00168
        capabilities: [json_mode, reasoning]
      'accounts/fireworks/models/deepseek-v3p2':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/deepseek-v3p2'
        mode: chat
        max_input_tokens: 163840
        max_output_tokens: 163840
        input_cost_per_1k: 0.0012
        output_cost_per_1k: 0.0012
        capabilities: [function_calling, json_mode, reasoning]
      'accounts/fireworks/models/devstral-small-2505':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/devstral-small-2505'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.0009
        output_cost_per_1k: 0.0009
      'accounts/fireworks/models/dobby-mini-unhinged-plus-llama-3-1-8b':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/dobby-mini-unhinged-plus-llama-3-1-8b'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
      'accounts/fireworks/models/dobby-unhinged-llama-3-3-70b-new':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/dobby-unhinged-llama-3-3-70b-new'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.0009
        output_cost_per_1k: 0.0009
      'accounts/fireworks/models/dolphin-2-9-2-qwen2-72b':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/dolphin-2-9-2-qwen2-72b'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.0009
        output_cost_per_1k: 0.0009
      'accounts/fireworks/models/dolphin-2p6-mixtral-8x7b':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/dolphin-2p6-mixtral-8x7b'
        mode: chat
        max_input_tokens: 32768
        max_output_tokens: 32768
        input_cost_per_1k: 0.0005
        output_cost_per_1k: 0.0005
      'accounts/fireworks/models/ernie-4p5-21b-a3b-pt':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/ernie-4p5-21b-a3b-pt'
        mode: chat
        max_input_tokens: 4096
        max_output_tokens: 4096
        input_cost_per_1k: 0.0001
        output_cost_per_1k: 0.0001
      'accounts/fireworks/models/ernie-4p5-300b-a47b-pt':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/ernie-4p5-300b-a47b-pt'
        mode: chat
        max_input_tokens: 4096
        max_output_tokens: 4096
        input_cost_per_1k: 0.0001
        output_cost_per_1k: 0.0001
      'accounts/fireworks/models/fare-20b':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/fare-20b'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.0009
        output_cost_per_1k: 0.0009
      'accounts/fireworks/models/firefunction-v1':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/firefunction-v1'
        mode: chat
        max_input_tokens: 32768
        max_output_tokens: 32768
        input_cost_per_1k: 0.0005
        output_cost_per_1k: 0.0005
      'accounts/fireworks/models/firefunction-v2':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/firefunction-v2'
        mode: chat
        max_input_tokens: 8192
        max_output_tokens: 8192
        input_cost_per_1k: 0.0009
        output_cost_per_1k: 0.0009
        capabilities: [function_calling, json_mode]
      'accounts/fireworks/models/firellava-13b':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/firellava-13b'
        mode: chat
        max_input_tokens: 4096
        max_output_tokens: 4096
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
      'accounts/fireworks/models/firesearch-ocr-v6':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/firesearch-ocr-v6'
        mode: chat
        max_input_tokens: 8192
        max_output_tokens: 8192
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
      'accounts/fireworks/models/fireworks-asr-large':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/fireworks-asr-large'
        mode: audio_transcription
        max_input_tokens: 4096
        max_output_tokens: 4096
      'accounts/fireworks/models/fireworks-asr-v2':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/fireworks-asr-v2'
        mode: audio_transcription
        max_input_tokens: 4096
        max_output_tokens: 4096
      'accounts/fireworks/models/flux-1-dev':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/flux-1-dev'
        mode: chat
        max_input_tokens: 4096
        max_output_tokens: 4096
        input_cost_per_1k: 0.0001
        output_cost_per_1k: 0.0001
      'accounts/fireworks/models/flux-1-dev-controlnet-union':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/flux-1-dev-controlnet-union'
        mode: chat
        max_input_tokens: 4096
        max_output_tokens: 4096
        input_cost_per_1k: 1e-06
        output_cost_per_1k: 1e-06
      'accounts/fireworks/models/flux-1-dev-fp8':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/flux-1-dev-fp8'
        mode: image
        max_input_tokens: 4096
        max_output_tokens: 4096
        input_cost_per_1k: 5e-07
        output_cost_per_1k: 5e-07
      'accounts/fireworks/models/flux-1-schnell':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/flux-1-schnell'
        mode: chat
        max_input_tokens: 4096
        max_output_tokens: 4096
        input_cost_per_1k: 0.0001
        output_cost_per_1k: 0.0001
      'accounts/fireworks/models/flux-1-schnell-fp8':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/flux-1-schnell-fp8'
        mode: image
        max_input_tokens: 4096
        max_output_tokens: 4096
        input_cost_per_1k: 3.5e-07
        output_cost_per_1k: 3.5e-07
      'accounts/fireworks/models/flux-kontext-max':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/flux-kontext-max'
        mode: image
        max_input_tokens: 4096
        max_output_tokens: 4096
        input_cost_per_1k: 8e-05
        output_cost_per_1k: 8e-05
      'accounts/fireworks/models/flux-kontext-pro':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/flux-kontext-pro'
        mode: image
        max_input_tokens: 4096
        max_output_tokens: 4096
        input_cost_per_1k: 4e-05
        output_cost_per_1k: 4e-05
      'accounts/fireworks/models/gemma-2b-it':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/gemma-2b-it'
        mode: chat
        max_input_tokens: 8192
        max_output_tokens: 8192
        input_cost_per_1k: 0.0001
        output_cost_per_1k: 0.0001
      'accounts/fireworks/models/gemma-3-27b-it':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/gemma-3-27b-it'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.0009
        output_cost_per_1k: 0.0009
      'accounts/fireworks/models/gemma-7b':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/gemma-7b'
        mode: chat
        max_input_tokens: 8192
        max_output_tokens: 8192
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
      'accounts/fireworks/models/gemma-7b-it':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/gemma-7b-it'
        mode: chat
        max_input_tokens: 8192
        max_output_tokens: 8192
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
      'accounts/fireworks/models/gemma2-9b-it':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/gemma2-9b-it'
        mode: chat
        max_input_tokens: 8192
        max_output_tokens: 8192
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
      'accounts/fireworks/models/glm-4p5':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/glm-4p5'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 96000
        input_cost_per_1k: 0.00055
        output_cost_per_1k: 0.00219
        capabilities: [function_calling, json_mode, reasoning]
      'accounts/fireworks/models/glm-4p5-air':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/glm-4p5-air'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 96000
        input_cost_per_1k: 0.00022
        output_cost_per_1k: 0.00088
        capabilities: [function_calling, json_mode, reasoning]
      'accounts/fireworks/models/glm-4p5v':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/glm-4p5v'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.0012
        output_cost_per_1k: 0.0012
        capabilities: [reasoning]
      'accounts/fireworks/models/glm-4p6':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/glm-4p6'
        mode: chat
        max_input_tokens: 202800
        max_output_tokens: 202800
        input_cost_per_1k: 0.00055
        output_cost_per_1k: 0.00219
        capabilities: [function_calling, json_mode, reasoning]
      'accounts/fireworks/models/gpt-oss-120b':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/gpt-oss-120b'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.00015
        output_cost_per_1k: 0.0006
        capabilities: [function_calling, json_mode, reasoning]
      'accounts/fireworks/models/gpt-oss-20b':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/gpt-oss-20b'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 5e-05
        output_cost_per_1k: 0.0002
        capabilities: [function_calling, json_mode, reasoning]
      'accounts/fireworks/models/gpt-oss-safeguard-120b':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/gpt-oss-safeguard-120b'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.0012
        output_cost_per_1k: 0.0012
      'accounts/fireworks/models/gpt-oss-safeguard-20b':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/gpt-oss-safeguard-20b'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.0005
        output_cost_per_1k: 0.0005
      'accounts/fireworks/models/hermes-2-pro-mistral-7b':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/hermes-2-pro-mistral-7b'
        mode: chat
        max_input_tokens: 32768
        max_output_tokens: 32768
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
      'accounts/fireworks/models/internvl3-38b':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/internvl3-38b'
        mode: chat
        max_input_tokens: 16384
        max_output_tokens: 16384
        input_cost_per_1k: 0.0009
        output_cost_per_1k: 0.0009
      'accounts/fireworks/models/internvl3-78b':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/internvl3-78b'
        mode: chat
        max_input_tokens: 16384
        max_output_tokens: 16384
        input_cost_per_1k: 0.0009
        output_cost_per_1k: 0.0009
      'accounts/fireworks/models/internvl3-8b':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/internvl3-8b'
        mode: chat
        max_input_tokens: 16384
        max_output_tokens: 16384
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
      'accounts/fireworks/models/japanese-stable-diffusion-xl':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/japanese-stable-diffusion-xl'
        mode: image
        max_input_tokens: 4096
        max_output_tokens: 4096
        input_cost_per_1k: 1.3e-07
        output_cost_per_1k: 1.3e-07
      'accounts/fireworks/models/kat-coder':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/kat-coder'
        mode: chat
        max_input_tokens: 262144
        max_output_tokens: 262144
        input_cost_per_1k: 0.0009
        output_cost_per_1k: 0.0009
      'accounts/fireworks/models/kat-dev-32b':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/kat-dev-32b'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.0009
        output_cost_per_1k: 0.0009
      'accounts/fireworks/models/kat-dev-72b-exp':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/kat-dev-72b-exp'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.0009
        output_cost_per_1k: 0.0009
      'accounts/fireworks/models/kimi-k2-instruct':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/kimi-k2-instruct'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 16384
        input_cost_per_1k: 0.0006
        output_cost_per_1k: 0.0025
        capabilities: [function_calling, json_mode]
      'accounts/fireworks/models/kimi-k2-instruct-0905':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/kimi-k2-instruct-0905'
        mode: chat
        max_input_tokens: 262144
        max_output_tokens: 32768
        input_cost_per_1k: 0.0006
        output_cost_per_1k: 0.0025
        capabilities: [function_calling, json_mode]
      'accounts/fireworks/models/kimi-k2-thinking':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/kimi-k2-thinking'
        mode: chat
        max_input_tokens: 262144
        max_output_tokens: 262144
        input_cost_per_1k: 0.0006
        output_cost_per_1k: 0.0025
        capabilities: [function_calling, json_mode, web_search]
      'accounts/fireworks/models/llama-guard-2-8b':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/llama-guard-2-8b'
        mode: chat
        max_input_tokens: 8192
        max_output_tokens: 8192
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
      'accounts/fireworks/models/llama-guard-3-1b':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/llama-guard-3-1b'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.0001
        output_cost_per_1k: 0.0001
      'accounts/fireworks/models/llama-guard-3-8b':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/llama-guard-3-8b'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
      'accounts/fireworks/models/llama-v2-13b':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/llama-v2-13b'
        mode: chat
        max_input_tokens: 4096
        max_output_tokens: 4096
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
      'accounts/fireworks/models/llama-v2-13b-chat':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/llama-v2-13b-chat'
        mode: chat
        max_input_tokens: 4096
        max_output_tokens: 4096
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
      'accounts/fireworks/models/llama-v2-70b':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/llama-v2-70b'
        mode: chat
        max_input_tokens: 4096
        max_output_tokens: 4096
        input_cost_per_1k: 0.0001
        output_cost_per_1k: 0.0001
      'accounts/fireworks/models/llama-v2-70b-chat':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/llama-v2-70b-chat'
        mode: chat
        max_input_tokens: 2048
        max_output_tokens: 2048
        input_cost_per_1k: 0.0009
        output_cost_per_1k: 0.0009
      'accounts/fireworks/models/llama-v2-7b':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/llama-v2-7b'
        mode: chat
        max_input_tokens: 4096
        max_output_tokens: 4096
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
      'accounts/fireworks/models/llama-v2-7b-chat':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/llama-v2-7b-chat'
        mode: chat
        max_input_tokens: 4096
        max_output_tokens: 4096
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
      'accounts/fireworks/models/llama-v3-70b-instruct':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/llama-v3-70b-instruct'
        mode: chat
        max_input_tokens: 8192
        max_output_tokens: 8192
        input_cost_per_1k: 0.0009
        output_cost_per_1k: 0.0009
      'accounts/fireworks/models/llama-v3-70b-instruct-hf':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/llama-v3-70b-instruct-hf'
        mode: chat
        max_input_tokens: 8192
        max_output_tokens: 8192
        input_cost_per_1k: 0.0009
        output_cost_per_1k: 0.0009
      'accounts/fireworks/models/llama-v3-8b':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/llama-v3-8b'
        mode: chat
        max_input_tokens: 8192
        max_output_tokens: 8192
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
      'accounts/fireworks/models/llama-v3-8b-instruct-hf':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/llama-v3-8b-instruct-hf'
        mode: chat
        max_input_tokens: 8192
        max_output_tokens: 8192
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
      'accounts/fireworks/models/llama-v3p1-405b-instruct':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/llama-v3p1-405b-instruct'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 16384
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.003
        capabilities: [function_calling, json_mode]
      'accounts/fireworks/models/llama-v3p1-405b-instruct-long':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/llama-v3p1-405b-instruct-long'
        mode: chat
        max_input_tokens: 4096
        max_output_tokens: 4096
        input_cost_per_1k: 0.0001
        output_cost_per_1k: 0.0001
      'accounts/fireworks/models/llama-v3p1-70b-instruct':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/llama-v3p1-70b-instruct'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.0009
        output_cost_per_1k: 0.0009
      'accounts/fireworks/models/llama-v3p1-70b-instruct-1b':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/llama-v3p1-70b-instruct-1b'
        mode: chat
        max_input_tokens: 4096
        max_output_tokens: 4096
        input_cost_per_1k: 0.0001
        output_cost_per_1k: 0.0001
      'accounts/fireworks/models/llama-v3p1-8b-instruct':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/llama-v3p1-8b-instruct'
        mode: chat
        max_input_tokens: 16384
        max_output_tokens: 16384
        input_cost_per_1k: 0.0001
        output_cost_per_1k: 0.0001
        capabilities: [json_mode]
      'accounts/fireworks/models/llama-v3p1-nemotron-70b-instruct':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/llama-v3p1-nemotron-70b-instruct'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.0009
        output_cost_per_1k: 0.0009
      'accounts/fireworks/models/llama-v3p2-11b-vision-instruct':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/llama-v3p2-11b-vision-instruct'
        mode: chat
        max_input_tokens: 16384
        max_output_tokens: 16384
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
        capabilities: [vision, json_mode]
      'accounts/fireworks/models/llama-v3p2-1b':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/llama-v3p2-1b'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.0001
        output_cost_per_1k: 0.0001
      'accounts/fireworks/models/llama-v3p2-1b-instruct':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/llama-v3p2-1b-instruct'
        mode: chat
        max_input_tokens: 16384
        max_output_tokens: 16384
        input_cost_per_1k: 0.0001
        output_cost_per_1k: 0.0001
        capabilities: [json_mode]
      'accounts/fireworks/models/llama-v3p2-3b':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/llama-v3p2-3b'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.0001
        output_cost_per_1k: 0.0001
      'accounts/fireworks/models/llama-v3p2-3b-instruct':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/llama-v3p2-3b-instruct'
        mode: chat
        max_input_tokens: 16384
        max_output_tokens: 16384
        input_cost_per_1k: 0.0001
        output_cost_per_1k: 0.0001
        capabilities: [json_mode]
      'accounts/fireworks/models/llama-v3p2-90b-vision-instruct':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/llama-v3p2-90b-vision-instruct'
        mode: chat
        max_input_tokens: 16384
        max_output_tokens: 16384
        input_cost_per_1k: 0.0009
        output_cost_per_1k: 0.0009
        capabilities: [vision, json_mode]
      'accounts/fireworks/models/llama-v3p3-70b-instruct':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/llama-v3p3-70b-instruct'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.0009
        output_cost_per_1k: 0.0009
      'accounts/fireworks/models/llama4-maverick-instruct-basic':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/llama4-maverick-instruct-basic'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.00022
        output_cost_per_1k: 0.00088
        capabilities: [json_mode]
      'accounts/fireworks/models/llama4-scout-instruct-basic':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/llama4-scout-instruct-basic'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.00015
        output_cost_per_1k: 0.0006
        capabilities: [json_mode]
      'accounts/fireworks/models/llamaguard-7b':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/llamaguard-7b'
        mode: chat
        max_input_tokens: 4096
        max_output_tokens: 4096
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
      'accounts/fireworks/models/llava-yi-34b':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/llava-yi-34b'
        mode: chat
        max_input_tokens: 4096
        max_output_tokens: 4096
        input_cost_per_1k: 0.0009
        output_cost_per_1k: 0.0009
      'accounts/fireworks/models/minimax-m1-80k':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/minimax-m1-80k'
        mode: chat
        max_input_tokens: 4096
        max_output_tokens: 4096
        input_cost_per_1k: 0.0001
        output_cost_per_1k: 0.0001
      'accounts/fireworks/models/minimax-m2':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/minimax-m2'
        mode: chat
        max_input_tokens: 4096
        max_output_tokens: 4096
        input_cost_per_1k: 0.0003
        output_cost_per_1k: 0.0012
      'accounts/fireworks/models/ministral-3-14b-instruct-2512':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/ministral-3-14b-instruct-2512'
        mode: chat
        max_input_tokens: 256000
        max_output_tokens: 256000
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
      'accounts/fireworks/models/ministral-3-3b-instruct-2512':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/ministral-3-3b-instruct-2512'
        mode: chat
        max_input_tokens: 256000
        max_output_tokens: 256000
        input_cost_per_1k: 0.0001
        output_cost_per_1k: 0.0001
      'accounts/fireworks/models/ministral-3-8b-instruct-2512':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/ministral-3-8b-instruct-2512'
        mode: chat
        max_input_tokens: 256000
        max_output_tokens: 256000
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
      'accounts/fireworks/models/mistral-7b':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/mistral-7b'
        mode: chat
        max_input_tokens: 32768
        max_output_tokens: 32768
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
      'accounts/fireworks/models/mistral-7b-instruct-4k':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/mistral-7b-instruct-4k'
        mode: chat
        max_input_tokens: 32768
        max_output_tokens: 32768
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
      'accounts/fireworks/models/mistral-7b-instruct-v0p2':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/mistral-7b-instruct-v0p2'
        mode: chat
        max_input_tokens: 32768
        max_output_tokens: 32768
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
      'accounts/fireworks/models/mistral-7b-instruct-v3':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/mistral-7b-instruct-v3'
        mode: chat
        max_input_tokens: 32768
        max_output_tokens: 32768
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
      'accounts/fireworks/models/mistral-7b-v0p2':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/mistral-7b-v0p2'
        mode: chat
        max_input_tokens: 32768
        max_output_tokens: 32768
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
      'accounts/fireworks/models/mistral-large-3-fp8':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/mistral-large-3-fp8'
        mode: chat
        max_input_tokens: 256000
        max_output_tokens: 256000
        input_cost_per_1k: 0.0012
        output_cost_per_1k: 0.0012
      'accounts/fireworks/models/mistral-nemo-base-2407':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/mistral-nemo-base-2407'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 128000
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
      'accounts/fireworks/models/mistral-nemo-instruct-2407':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/mistral-nemo-instruct-2407'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 128000
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
      'accounts/fireworks/models/mistral-small-24b-instruct-2501':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/mistral-small-24b-instruct-2501'
        mode: chat
        max_input_tokens: 32768
        max_output_tokens: 32768
        input_cost_per_1k: 0.0009
        output_cost_per_1k: 0.0009
      'accounts/fireworks/models/mixtral-8x22b':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/mixtral-8x22b'
        mode: chat
        max_input_tokens: 65536
        max_output_tokens: 65536
        input_cost_per_1k: 0.0012
        output_cost_per_1k: 0.0012
      'accounts/fireworks/models/mixtral-8x22b-instruct':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/mixtral-8x22b-instruct'
        mode: chat
        max_input_tokens: 65536
        max_output_tokens: 65536
        input_cost_per_1k: 0.0012
        output_cost_per_1k: 0.0012
      'accounts/fireworks/models/mixtral-8x22b-instruct-hf':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/mixtral-8x22b-instruct-hf'
        mode: chat
        max_input_tokens: 65536
        max_output_tokens: 65536
        input_cost_per_1k: 0.0012
        output_cost_per_1k: 0.0012
        capabilities: [function_calling, json_mode]
      'accounts/fireworks/models/mixtral-8x7b':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/mixtral-8x7b'
        mode: chat
        max_input_tokens: 32768
        max_output_tokens: 32768
        input_cost_per_1k: 0.0005
        output_cost_per_1k: 0.0005
      'accounts/fireworks/models/mixtral-8x7b-instruct':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/mixtral-8x7b-instruct'
        mode: chat
        max_input_tokens: 32768
        max_output_tokens: 32768
        input_cost_per_1k: 0.0005
        output_cost_per_1k: 0.0005
      'accounts/fireworks/models/mixtral-8x7b-instruct-hf':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/mixtral-8x7b-instruct-hf'
        mode: chat
        max_input_tokens: 32768
        max_output_tokens: 32768
        input_cost_per_1k: 0.0005
        output_cost_per_1k: 0.0005
      'accounts/fireworks/models/mythomax-l2-13b':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/mythomax-l2-13b'
        mode: chat
        max_input_tokens: 4096
        max_output_tokens: 4096
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
      'accounts/fireworks/models/nemotron-nano-v2-12b-vl':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/nemotron-nano-v2-12b-vl'
        mode: chat
        max_input_tokens: 4096
        max_output_tokens: 4096
        input_cost_per_1k: 0.0001
        output_cost_per_1k: 0.0001
      'accounts/fireworks/models/nous-capybara-7b-v1p9':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/nous-capybara-7b-v1p9'
        mode: chat
        max_input_tokens: 32768
        max_output_tokens: 32768
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
      'accounts/fireworks/models/nous-hermes-2-mixtral-8x7b-dpo':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/nous-hermes-2-mixtral-8x7b-dpo'
        mode: chat
        max_input_tokens: 32768
        max_output_tokens: 32768
        input_cost_per_1k: 0.0005
        output_cost_per_1k: 0.0005
      'accounts/fireworks/models/nous-hermes-2-yi-34b':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/nous-hermes-2-yi-34b'
        mode: chat
        max_input_tokens: 4096
        max_output_tokens: 4096
        input_cost_per_1k: 0.0009
        output_cost_per_1k: 0.0009
      'accounts/fireworks/models/nous-hermes-llama2-13b':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/nous-hermes-llama2-13b'
        mode: chat
        max_input_tokens: 4096
        max_output_tokens: 4096
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
      'accounts/fireworks/models/nous-hermes-llama2-70b':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/nous-hermes-llama2-70b'
        mode: chat
        max_input_tokens: 4096
        max_output_tokens: 4096
        input_cost_per_1k: 0.0009
        output_cost_per_1k: 0.0009
      'accounts/fireworks/models/nous-hermes-llama2-7b':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/nous-hermes-llama2-7b'
        mode: chat
        max_input_tokens: 4096
        max_output_tokens: 4096
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
      'accounts/fireworks/models/nvidia-nemotron-nano-12b-v2':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/nvidia-nemotron-nano-12b-v2'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
      'accounts/fireworks/models/nvidia-nemotron-nano-9b-v2':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/nvidia-nemotron-nano-9b-v2'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
      'accounts/fireworks/models/openchat-3p5-0106-7b':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/openchat-3p5-0106-7b'
        mode: chat
        max_input_tokens: 8192
        max_output_tokens: 8192
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
      'accounts/fireworks/models/openhermes-2-mistral-7b':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/openhermes-2-mistral-7b'
        mode: chat
        max_input_tokens: 32768
        max_output_tokens: 32768
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
      'accounts/fireworks/models/openhermes-2p5-mistral-7b':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/openhermes-2p5-mistral-7b'
        mode: chat
        max_input_tokens: 32768
        max_output_tokens: 32768
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
      'accounts/fireworks/models/openorca-7b':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/openorca-7b'
        mode: chat
        max_input_tokens: 32768
        max_output_tokens: 32768
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
      'accounts/fireworks/models/phi-2-3b':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/phi-2-3b'
        mode: chat
        max_input_tokens: 2048
        max_output_tokens: 2048
        input_cost_per_1k: 0.0001
        output_cost_per_1k: 0.0001
      'accounts/fireworks/models/phi-3-mini-128k-instruct':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/phi-3-mini-128k-instruct'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.0001
        output_cost_per_1k: 0.0001
      'accounts/fireworks/models/phi-3-vision-128k-instruct':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/phi-3-vision-128k-instruct'
        mode: chat
        max_input_tokens: 32064
        max_output_tokens: 32064
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
      'accounts/fireworks/models/phind-code-llama-34b-python-v1':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/phind-code-llama-34b-python-v1'
        mode: chat
        max_input_tokens: 16384
        max_output_tokens: 16384
        input_cost_per_1k: 0.0009
        output_cost_per_1k: 0.0009
      'accounts/fireworks/models/phind-code-llama-34b-v1':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/phind-code-llama-34b-v1'
        mode: chat
        max_input_tokens: 16384
        max_output_tokens: 16384
        input_cost_per_1k: 0.0009
        output_cost_per_1k: 0.0009
      'accounts/fireworks/models/phind-code-llama-34b-v2':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/phind-code-llama-34b-v2'
        mode: chat
        max_input_tokens: 16384
        max_output_tokens: 16384
        input_cost_per_1k: 0.0009
        output_cost_per_1k: 0.0009
      'accounts/fireworks/models/playground-v2-1024px-aesthetic':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/playground-v2-1024px-aesthetic'
        mode: image
        max_input_tokens: 4096
        max_output_tokens: 4096
        input_cost_per_1k: 1.3e-07
        output_cost_per_1k: 1.3e-07
      'accounts/fireworks/models/playground-v2-5-1024px-aesthetic':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/playground-v2-5-1024px-aesthetic'
        mode: image
        max_input_tokens: 4096
        max_output_tokens: 4096
        input_cost_per_1k: 1.3e-07
        output_cost_per_1k: 1.3e-07
      'accounts/fireworks/models/pythia-12b':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/pythia-12b'
        mode: chat
        max_input_tokens: 2048
        max_output_tokens: 2048
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
      'accounts/fireworks/models/qwen-qwq-32b-preview':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/qwen-qwq-32b-preview'
        mode: chat
        max_input_tokens: 32768
        max_output_tokens: 32768
        input_cost_per_1k: 0.0009
        output_cost_per_1k: 0.0009
      'accounts/fireworks/models/qwen-v2p5-14b-instruct':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/qwen-v2p5-14b-instruct'
        mode: chat
        max_input_tokens: 32768
        max_output_tokens: 32768
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
      'accounts/fireworks/models/qwen-v2p5-7b':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/qwen-v2p5-7b'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
      'accounts/fireworks/models/qwen1p5-72b-chat':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/qwen1p5-72b-chat'
        mode: chat
        max_input_tokens: 32768
        max_output_tokens: 32768
        input_cost_per_1k: 0.0009
        output_cost_per_1k: 0.0009
      'accounts/fireworks/models/qwen2-72b-instruct':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/qwen2-72b-instruct'
        mode: chat
        max_input_tokens: 32768
        max_output_tokens: 32768
        input_cost_per_1k: 0.0009
        output_cost_per_1k: 0.0009
        capabilities: [json_mode]
      'accounts/fireworks/models/qwen2-7b-instruct':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/qwen2-7b-instruct'
        mode: chat
        max_input_tokens: 32768
        max_output_tokens: 32768
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
      'accounts/fireworks/models/qwen2-vl-2b-instruct':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/qwen2-vl-2b-instruct'
        mode: chat
        max_input_tokens: 32768
        max_output_tokens: 32768
        input_cost_per_1k: 0.0001
        output_cost_per_1k: 0.0001
      'accounts/fireworks/models/qwen2-vl-72b-instruct':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/qwen2-vl-72b-instruct'
        mode: chat
        max_input_tokens: 32768
        max_output_tokens: 32768
        input_cost_per_1k: 0.0009
        output_cost_per_1k: 0.0009
      'accounts/fireworks/models/qwen2-vl-7b-instruct':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/qwen2-vl-7b-instruct'
        mode: chat
        max_input_tokens: 32768
        max_output_tokens: 32768
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
      'accounts/fireworks/models/qwen2p5-0p5b-instruct':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/qwen2p5-0p5b-instruct'
        mode: chat
        max_input_tokens: 32768
        max_output_tokens: 32768
        input_cost_per_1k: 0.0001
        output_cost_per_1k: 0.0001
      'accounts/fireworks/models/qwen2p5-14b':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/qwen2p5-14b'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
      'accounts/fireworks/models/qwen2p5-1p5b-instruct':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/qwen2p5-1p5b-instruct'
        mode: chat
        max_input_tokens: 32768
        max_output_tokens: 32768
        input_cost_per_1k: 0.0001
        output_cost_per_1k: 0.0001
      'accounts/fireworks/models/qwen2p5-32b':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/qwen2p5-32b'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.0009
        output_cost_per_1k: 0.0009
      'accounts/fireworks/models/qwen2p5-32b-instruct':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/qwen2p5-32b-instruct'
        mode: chat
        max_input_tokens: 32768
        max_output_tokens: 32768
        input_cost_per_1k: 0.0009
        output_cost_per_1k: 0.0009
      'accounts/fireworks/models/qwen2p5-72b':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/qwen2p5-72b'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.0009
        output_cost_per_1k: 0.0009
      'accounts/fireworks/models/qwen2p5-72b-instruct':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/qwen2p5-72b-instruct'
        mode: chat
        max_input_tokens: 32768
        max_output_tokens: 32768
        input_cost_per_1k: 0.0009
        output_cost_per_1k: 0.0009
      'accounts/fireworks/models/qwen2p5-7b-instruct':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/qwen2p5-7b-instruct'
        mode: chat
        max_input_tokens: 32768
        max_output_tokens: 32768
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
      'accounts/fireworks/models/qwen2p5-coder-0p5b':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/qwen2p5-coder-0p5b'
        mode: chat
        max_input_tokens: 32768
        max_output_tokens: 32768
        input_cost_per_1k: 0.0001
        output_cost_per_1k: 0.0001
      'accounts/fireworks/models/qwen2p5-coder-0p5b-instruct':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/qwen2p5-coder-0p5b-instruct'
        mode: chat
        max_input_tokens: 32768
        max_output_tokens: 32768
        input_cost_per_1k: 0.0001
        output_cost_per_1k: 0.0001
      'accounts/fireworks/models/qwen2p5-coder-14b':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/qwen2p5-coder-14b'
        mode: chat
        max_input_tokens: 32768
        max_output_tokens: 32768
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
      'accounts/fireworks/models/qwen2p5-coder-14b-instruct':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/qwen2p5-coder-14b-instruct'
        mode: chat
        max_input_tokens: 32768
        max_output_tokens: 32768
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
      'accounts/fireworks/models/qwen2p5-coder-1p5b':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/qwen2p5-coder-1p5b'
        mode: chat
        max_input_tokens: 32768
        max_output_tokens: 32768
        input_cost_per_1k: 0.0001
        output_cost_per_1k: 0.0001
      'accounts/fireworks/models/qwen2p5-coder-1p5b-instruct':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/qwen2p5-coder-1p5b-instruct'
        mode: chat
        max_input_tokens: 32768
        max_output_tokens: 32768
        input_cost_per_1k: 0.0001
        output_cost_per_1k: 0.0001
      'accounts/fireworks/models/qwen2p5-coder-32b':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/qwen2p5-coder-32b'
        mode: chat
        max_input_tokens: 32768
        max_output_tokens: 32768
        input_cost_per_1k: 0.0009
        output_cost_per_1k: 0.0009
      'accounts/fireworks/models/qwen2p5-coder-32b-instruct':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/qwen2p5-coder-32b-instruct'
        mode: chat
        max_input_tokens: 4096
        max_output_tokens: 4096
        input_cost_per_1k: 0.0009
        output_cost_per_1k: 0.0009
        capabilities: [json_mode]
      'accounts/fireworks/models/qwen2p5-coder-32b-instruct-128k':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/qwen2p5-coder-32b-instruct-128k'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.0009
        output_cost_per_1k: 0.0009
      'accounts/fireworks/models/qwen2p5-coder-32b-instruct-32k-rope':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/qwen2p5-coder-32b-instruct-32k-rope'
        mode: chat
        max_input_tokens: 32768
        max_output_tokens: 32768
        input_cost_per_1k: 0.0009
        output_cost_per_1k: 0.0009
      'accounts/fireworks/models/qwen2p5-coder-32b-instruct-64k':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/qwen2p5-coder-32b-instruct-64k'
        mode: chat
        max_input_tokens: 65536
        max_output_tokens: 65536
        input_cost_per_1k: 0.0009
        output_cost_per_1k: 0.0009
      'accounts/fireworks/models/qwen2p5-coder-3b':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/qwen2p5-coder-3b'
        mode: chat
        max_input_tokens: 32768
        max_output_tokens: 32768
        input_cost_per_1k: 0.0001
        output_cost_per_1k: 0.0001
      'accounts/fireworks/models/qwen2p5-coder-3b-instruct':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/qwen2p5-coder-3b-instruct'
        mode: chat
        max_input_tokens: 32768
        max_output_tokens: 32768
        input_cost_per_1k: 0.0001
        output_cost_per_1k: 0.0001
      'accounts/fireworks/models/qwen2p5-coder-7b':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/qwen2p5-coder-7b'
        mode: chat
        max_input_tokens: 32768
        max_output_tokens: 32768
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
      'accounts/fireworks/models/qwen2p5-coder-7b-instruct':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/qwen2p5-coder-7b-instruct'
        mode: chat
        max_input_tokens: 32768
        max_output_tokens: 32768
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
      'accounts/fireworks/models/qwen2p5-math-72b-instruct':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/qwen2p5-math-72b-instruct'
        mode: chat
        max_input_tokens: 4096
        max_output_tokens: 4096
        input_cost_per_1k: 0.0009
        output_cost_per_1k: 0.0009
      'accounts/fireworks/models/qwen2p5-vl-32b-instruct':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/qwen2p5-vl-32b-instruct'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 128000
        input_cost_per_1k: 0.0009
        output_cost_per_1k: 0.0009
      'accounts/fireworks/models/qwen2p5-vl-3b-instruct':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/qwen2p5-vl-3b-instruct'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 128000
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
      'accounts/fireworks/models/qwen2p5-vl-72b-instruct':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/qwen2p5-vl-72b-instruct'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 128000
        input_cost_per_1k: 0.0009
        output_cost_per_1k: 0.0009
      'accounts/fireworks/models/qwen2p5-vl-7b-instruct':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/qwen2p5-vl-7b-instruct'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 128000
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
      'accounts/fireworks/models/qwen3-0p6b':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/qwen3-0p6b'
        mode: chat
        max_input_tokens: 40960
        max_output_tokens: 40960
        input_cost_per_1k: 0.0001
        output_cost_per_1k: 0.0001
      'accounts/fireworks/models/qwen3-14b':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/qwen3-14b'
        mode: chat
        max_input_tokens: 40960
        max_output_tokens: 40960
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
      'accounts/fireworks/models/qwen3-1p7b':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/qwen3-1p7b'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.0001
        output_cost_per_1k: 0.0001
      'accounts/fireworks/models/qwen3-1p7b-fp8-draft':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/qwen3-1p7b-fp8-draft'
        mode: chat
        max_input_tokens: 262144
        max_output_tokens: 262144
        input_cost_per_1k: 0.0001
        output_cost_per_1k: 0.0001
      'accounts/fireworks/models/qwen3-1p7b-fp8-draft-131072':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/qwen3-1p7b-fp8-draft-131072'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.0001
        output_cost_per_1k: 0.0001
      'accounts/fireworks/models/qwen3-1p7b-fp8-draft-40960':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/qwen3-1p7b-fp8-draft-40960'
        mode: chat
        max_input_tokens: 40960
        max_output_tokens: 40960
        input_cost_per_1k: 0.0001
        output_cost_per_1k: 0.0001
      'accounts/fireworks/models/qwen3-235b-a22b':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/qwen3-235b-a22b'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.00022
        output_cost_per_1k: 0.00088
      'accounts/fireworks/models/qwen3-235b-a22b-instruct-2507':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/qwen3-235b-a22b-instruct-2507'
        mode: chat
        max_input_tokens: 262144
        max_output_tokens: 262144
        input_cost_per_1k: 0.00022
        output_cost_per_1k: 0.00088
      'accounts/fireworks/models/qwen3-235b-a22b-thinking-2507':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/qwen3-235b-a22b-thinking-2507'
        mode: chat
        max_input_tokens: 262144
        max_output_tokens: 262144
        input_cost_per_1k: 0.00022
        output_cost_per_1k: 0.00088
      'accounts/fireworks/models/qwen3-30b-a3b':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/qwen3-30b-a3b'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.00015
        output_cost_per_1k: 0.0006
      'accounts/fireworks/models/qwen3-30b-a3b-instruct-2507':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/qwen3-30b-a3b-instruct-2507'
        mode: chat
        max_input_tokens: 262144
        max_output_tokens: 262144
        input_cost_per_1k: 0.0005
        output_cost_per_1k: 0.0005
      'accounts/fireworks/models/qwen3-30b-a3b-thinking-2507':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/qwen3-30b-a3b-thinking-2507'
        mode: chat
        max_input_tokens: 262144
        max_output_tokens: 262144
        input_cost_per_1k: 0.0009
        output_cost_per_1k: 0.0009
      'accounts/fireworks/models/qwen3-32b':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/qwen3-32b'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.0009
        output_cost_per_1k: 0.0009
        capabilities: [reasoning]
      'accounts/fireworks/models/qwen3-4b':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/qwen3-4b'
        mode: chat
        max_input_tokens: 40960
        max_output_tokens: 40960
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
      'accounts/fireworks/models/qwen3-4b-instruct-2507':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/qwen3-4b-instruct-2507'
        mode: chat
        max_input_tokens: 262144
        max_output_tokens: 262144
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
      'accounts/fireworks/models/qwen3-8b':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/qwen3-8b'
        mode: chat
        max_input_tokens: 40960
        max_output_tokens: 40960
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
        capabilities: [reasoning]
      'accounts/fireworks/models/qwen3-coder-30b-a3b-instruct':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/qwen3-coder-30b-a3b-instruct'
        mode: chat
        max_input_tokens: 262144
        max_output_tokens: 262144
        input_cost_per_1k: 0.00015
        output_cost_per_1k: 0.0006
      'accounts/fireworks/models/qwen3-coder-480b-a35b-instruct':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/qwen3-coder-480b-a35b-instruct'
        mode: chat
        max_input_tokens: 262144
        max_output_tokens: 262144
        input_cost_per_1k: 0.00045
        output_cost_per_1k: 0.0018
        capabilities: [reasoning]
      'accounts/fireworks/models/qwen3-coder-480b-instruct-bf16':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/qwen3-coder-480b-instruct-bf16'
        mode: chat
        max_input_tokens: 4096
        max_output_tokens: 4096
        input_cost_per_1k: 0.0009
        output_cost_per_1k: 0.0009
      'accounts/fireworks/models/qwen3-embedding-0p6b':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/qwen3-embedding-0p6b'
        mode: embedding
        max_input_tokens: 32768
        max_output_tokens: 32768
      'accounts/fireworks/models/qwen3-embedding-4b':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/qwen3-embedding-4b'
        mode: embedding
        max_input_tokens: 40960
        max_output_tokens: 40960
      'accounts/fireworks/models/qwen3-next-80b-a3b-instruct':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/qwen3-next-80b-a3b-instruct'
        mode: chat
        max_input_tokens: 4096
        max_output_tokens: 4096
        input_cost_per_1k: 0.0009
        output_cost_per_1k: 0.0009
      'accounts/fireworks/models/qwen3-next-80b-a3b-thinking':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/qwen3-next-80b-a3b-thinking'
        mode: chat
        max_input_tokens: 4096
        max_output_tokens: 4096
        input_cost_per_1k: 0.0009
        output_cost_per_1k: 0.0009
      'accounts/fireworks/models/qwen3-reranker-0p6b':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/qwen3-reranker-0p6b'
        mode: rerank
        max_input_tokens: 40960
        max_output_tokens: 40960
      'accounts/fireworks/models/qwen3-reranker-4b':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/qwen3-reranker-4b'
        mode: rerank
        max_input_tokens: 40960
        max_output_tokens: 40960
      'accounts/fireworks/models/qwen3-reranker-8b':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/qwen3-reranker-8b'
        mode: rerank
        max_input_tokens: 40960
        max_output_tokens: 40960
      'accounts/fireworks/models/qwen3-vl-235b-a22b-instruct':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/qwen3-vl-235b-a22b-instruct'
        mode: chat
        max_input_tokens: 262144
        max_output_tokens: 262144
        input_cost_per_1k: 0.00022
        output_cost_per_1k: 0.00088
      'accounts/fireworks/models/qwen3-vl-235b-a22b-thinking':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/qwen3-vl-235b-a22b-thinking'
        mode: chat
        max_input_tokens: 262144
        max_output_tokens: 262144
        input_cost_per_1k: 0.00022
        output_cost_per_1k: 0.00088
      'accounts/fireworks/models/qwen3-vl-30b-a3b-instruct':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/qwen3-vl-30b-a3b-instruct'
        mode: chat
        max_input_tokens: 262144
        max_output_tokens: 262144
        input_cost_per_1k: 0.00015
        output_cost_per_1k: 0.0006
      'accounts/fireworks/models/qwen3-vl-30b-a3b-thinking':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/qwen3-vl-30b-a3b-thinking'
        mode: chat
        max_input_tokens: 262144
        max_output_tokens: 262144
        input_cost_per_1k: 0.00015
        output_cost_per_1k: 0.0006
      'accounts/fireworks/models/qwen3-vl-32b-instruct':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/qwen3-vl-32b-instruct'
        mode: chat
        max_input_tokens: 4096
        max_output_tokens: 4096
        input_cost_per_1k: 0.0009
        output_cost_per_1k: 0.0009
      'accounts/fireworks/models/qwen3-vl-8b-instruct':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/qwen3-vl-8b-instruct'
        mode: chat
        max_input_tokens: 4096
        max_output_tokens: 4096
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
      'accounts/fireworks/models/qwq-32b':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/qwq-32b'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.0009
        output_cost_per_1k: 0.0009
      'accounts/fireworks/models/rolm-ocr':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/rolm-ocr'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 128000
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
      'accounts/fireworks/models/snorkel-mistral-7b-pairrm-dpo':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/snorkel-mistral-7b-pairrm-dpo'
        mode: chat
        max_input_tokens: 32768
        max_output_tokens: 32768
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
      'accounts/fireworks/models/stable-diffusion-xl-1024-v1-0':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/stable-diffusion-xl-1024-v1-0'
        mode: image
        max_input_tokens: 4096
        max_output_tokens: 4096
        input_cost_per_1k: 1.3e-07
        output_cost_per_1k: 1.3e-07
      'accounts/fireworks/models/stablecode-3b':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/stablecode-3b'
        mode: chat
        max_input_tokens: 4096
        max_output_tokens: 4096
        input_cost_per_1k: 0.0001
        output_cost_per_1k: 0.0001
      'accounts/fireworks/models/starcoder-16b':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/starcoder-16b'
        mode: chat
        max_input_tokens: 8192
        max_output_tokens: 8192
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
      'accounts/fireworks/models/starcoder-7b':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/starcoder-7b'
        mode: chat
        max_input_tokens: 8192
        max_output_tokens: 8192
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
      'accounts/fireworks/models/starcoder2-15b':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/starcoder2-15b'
        mode: chat
        max_input_tokens: 16384
        max_output_tokens: 16384
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
      'accounts/fireworks/models/starcoder2-3b':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/starcoder2-3b'
        mode: chat
        max_input_tokens: 16384
        max_output_tokens: 16384
        input_cost_per_1k: 0.0001
        output_cost_per_1k: 0.0001
      'accounts/fireworks/models/starcoder2-7b':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/starcoder2-7b'
        mode: chat
        max_input_tokens: 16384
        max_output_tokens: 16384
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
      'accounts/fireworks/models/toppy-m-7b':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/toppy-m-7b'
        mode: chat
        max_input_tokens: 32768
        max_output_tokens: 32768
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
      'accounts/fireworks/models/whisper-v3':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/whisper-v3'
        mode: audio_transcription
        max_input_tokens: 4096
        max_output_tokens: 4096
      'accounts/fireworks/models/whisper-v3-turbo':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/whisper-v3-turbo'
        mode: audio_transcription
        max_input_tokens: 4096
        max_output_tokens: 4096
      'accounts/fireworks/models/yi-34b':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/yi-34b'
        mode: chat
        max_input_tokens: 4096
        max_output_tokens: 4096
        input_cost_per_1k: 0.0009
        output_cost_per_1k: 0.0009
      'accounts/fireworks/models/yi-34b-200k-capybara':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/yi-34b-200k-capybara'
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 200000
        input_cost_per_1k: 0.0009
        output_cost_per_1k: 0.0009
      'accounts/fireworks/models/yi-34b-chat':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/yi-34b-chat'
        mode: chat
        max_input_tokens: 4096
        max_output_tokens: 4096
        input_cost_per_1k: 0.0009
        output_cost_per_1k: 0.0009
      'accounts/fireworks/models/yi-6b':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/yi-6b'
        mode: chat
        max_input_tokens: 4096
        max_output_tokens: 4096
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
      'accounts/fireworks/models/yi-large':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/yi-large'
        mode: chat
        max_input_tokens: 32768
        max_output_tokens: 32768
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.003
        capabilities: [json_mode]
      'accounts/fireworks/models/zephyr-7b-beta':
        litellm_id: 'fireworks_ai/accounts/fireworks/models/zephyr-7b-beta'
        mode: chat
        max_input_tokens: 32768
        max_output_tokens: 32768
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
      'fireworks-ai-4.1b-to-16b':
        mode: chat
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
      'fireworks-ai-56b-to-176b':
        mode: chat
        input_cost_per_1k: 0.0012
        output_cost_per_1k: 0.0012
      'fireworks-ai-above-16b':
        mode: chat
        input_cost_per_1k: 0.0009
        output_cost_per_1k: 0.0009
      'fireworks-ai-default':
        mode: chat
      'fireworks-ai-moe-up-to-56b':
        mode: chat
        input_cost_per_1k: 0.0005
        output_cost_per_1k: 0.0005
      'fireworks-ai-up-to-4b':
        mode: chat
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
  fireworks_ai-embedding-models:
    model_count: 7
    models:
      'fireworks-ai-embedding-150m-to-350m':
        mode: chat
        input_cost_per_1k: 1.6e-05
      'fireworks-ai-embedding-up-to-150m':
        mode: chat
        input_cost_per_1k: 8e-06
      'fireworks_ai/WhereIsAI/UAE-Large-V1':
        mode: embedding
        max_input_tokens: 512
        input_cost_per_1k: 1.6e-05
      'fireworks_ai/nomic-ai/nomic-embed-text-v1':
        mode: embedding
        max_input_tokens: 8192
        input_cost_per_1k: 8e-06
      'fireworks_ai/nomic-ai/nomic-embed-text-v1.5':
        mode: embedding
        max_input_tokens: 8192
        input_cost_per_1k: 8e-06
      'fireworks_ai/thenlper/gte-base':
        mode: embedding
        max_input_tokens: 512
        input_cost_per_1k: 8e-06
      'fireworks_ai/thenlper/gte-large':
        mode: embedding
        max_input_tokens: 512
        input_cost_per_1k: 1.6e-05
  friendliai:
    model_count: 2
    models:
      'meta-llama-3.1-70b-instruct':
        litellm_id: 'friendliai/meta-llama-3.1-70b-instruct'
        mode: chat
        max_input_tokens: 8192
        max_output_tokens: 8192
        input_cost_per_1k: 0.0006
        output_cost_per_1k: 0.0006
        capabilities: [function_calling, parallel_function_calling, system_messages, json_mode]
      'meta-llama-3.1-8b-instruct':
        litellm_id: 'friendliai/meta-llama-3.1-8b-instruct'
        mode: chat
        max_input_tokens: 8192
        max_output_tokens: 8192
        input_cost_per_1k: 0.0001
        output_cost_per_1k: 0.0001
        capabilities: [function_calling, parallel_function_calling, system_messages, json_mode]
  github_copilot:
    model_count: 29
    models:
      'claude-haiku-4.5':
        litellm_id: 'github_copilot/claude-haiku-4.5'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 16000
        capabilities: [vision, function_calling, parallel_function_calling]
      'claude-opus-4.5':
        litellm_id: 'github_copilot/claude-opus-4.5'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 16000
        capabilities: [vision, function_calling, parallel_function_calling]
      'claude-opus-41':
        litellm_id: 'github_copilot/claude-opus-41'
        mode: chat
        max_input_tokens: 80000
        max_output_tokens: 16000
        capabilities: [vision]
      'claude-sonnet-4':
        litellm_id: 'github_copilot/claude-sonnet-4'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 16000
        capabilities: [vision, function_calling, parallel_function_calling]
      'claude-sonnet-4.5':
        litellm_id: 'github_copilot/claude-sonnet-4.5'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 16000
        capabilities: [vision, function_calling, parallel_function_calling]
      'gemini-2.5-pro':
        litellm_id: 'github_copilot/gemini-2.5-pro'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 64000
        capabilities: [vision, function_calling, parallel_function_calling]
      'gemini-3-pro-preview':
        litellm_id: 'github_copilot/gemini-3-pro-preview'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 64000
        capabilities: [vision, function_calling, parallel_function_calling]
      'gpt-3.5-turbo':
        litellm_id: 'github_copilot/gpt-3.5-turbo'
        mode: chat
        max_input_tokens: 16384
        max_output_tokens: 4096
        capabilities: [function_calling]
      'gpt-3.5-turbo-0613':
        litellm_id: 'github_copilot/gpt-3.5-turbo-0613'
        mode: chat
        max_input_tokens: 16384
        max_output_tokens: 4096
        capabilities: [function_calling]
      'gpt-4':
        litellm_id: 'github_copilot/gpt-4'
        mode: chat
        max_input_tokens: 32768
        max_output_tokens: 4096
        capabilities: [function_calling]
      'gpt-4-0613':
        litellm_id: 'github_copilot/gpt-4-0613'
        mode: chat
        max_input_tokens: 32768
        max_output_tokens: 4096
        capabilities: [function_calling]
      'gpt-4-o-preview':
        litellm_id: 'github_copilot/gpt-4-o-preview'
        mode: chat
        max_input_tokens: 64000
        max_output_tokens: 4096
        capabilities: [function_calling, parallel_function_calling]
      'gpt-4.1':
        litellm_id: 'github_copilot/gpt-4.1'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 16384
        capabilities: [vision, function_calling, parallel_function_calling, json_mode]
      'gpt-4.1-2025-04-14':
        litellm_id: 'github_copilot/gpt-4.1-2025-04-14'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 16384
        capabilities: [vision, function_calling, parallel_function_calling, json_mode]
      'gpt-41-copilot':
        litellm_id: 'github_copilot/gpt-41-copilot'
        mode: completion
      'gpt-4o':
        litellm_id: 'github_copilot/gpt-4o'
        mode: chat
        max_input_tokens: 64000
        max_output_tokens: 4096
        capabilities: [vision, function_calling, parallel_function_calling]
      'gpt-4o-2024-05-13':
        litellm_id: 'github_copilot/gpt-4o-2024-05-13'
        mode: chat
        max_input_tokens: 64000
        max_output_tokens: 4096
        capabilities: [vision, function_calling, parallel_function_calling]
      'gpt-4o-2024-08-06':
        litellm_id: 'github_copilot/gpt-4o-2024-08-06'
        mode: chat
        max_input_tokens: 64000
        max_output_tokens: 16384
        capabilities: [function_calling, parallel_function_calling]
      'gpt-4o-2024-11-20':
        litellm_id: 'github_copilot/gpt-4o-2024-11-20'
        mode: chat
        max_input_tokens: 64000
        max_output_tokens: 16384
        capabilities: [vision, function_calling, parallel_function_calling]
      'gpt-4o-mini':
        litellm_id: 'github_copilot/gpt-4o-mini'
        mode: chat
        max_input_tokens: 64000
        max_output_tokens: 4096
        capabilities: [function_calling, parallel_function_calling]
      'gpt-4o-mini-2024-07-18':
        litellm_id: 'github_copilot/gpt-4o-mini-2024-07-18'
        mode: chat
        max_input_tokens: 64000
        max_output_tokens: 4096
        capabilities: [function_calling, parallel_function_calling]
      'gpt-5':
        litellm_id: 'github_copilot/gpt-5'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 128000
        capabilities: [vision, function_calling, parallel_function_calling, json_mode]
      'gpt-5-mini':
        litellm_id: 'github_copilot/gpt-5-mini'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 64000
        capabilities: [vision, function_calling, parallel_function_calling, json_mode]
      'gpt-5.1':
        litellm_id: 'github_copilot/gpt-5.1'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 64000
        capabilities: [vision, function_calling, parallel_function_calling, json_mode]
      'gpt-5.1-codex-max':
        litellm_id: 'github_copilot/gpt-5.1-codex-max'
        mode: responses
        max_input_tokens: 128000
        max_output_tokens: 128000
        capabilities: [vision, function_calling, parallel_function_calling, json_mode]
      'gpt-5.2':
        litellm_id: 'github_copilot/gpt-5.2'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 64000
        capabilities: [vision, function_calling, parallel_function_calling, json_mode]
      'text-embedding-3-small':
        litellm_id: 'github_copilot/text-embedding-3-small'
        mode: embedding
        max_input_tokens: 8191
      'text-embedding-3-small-inference':
        litellm_id: 'github_copilot/text-embedding-3-small-inference'
        mode: embedding
        max_input_tokens: 8191
      'text-embedding-ada-002':
        litellm_id: 'github_copilot/text-embedding-ada-002'
        mode: embedding
        max_input_tokens: 8191
  google:
    model_count: 127
    models:
      'chirp':
        litellm_id: 'vertex_ai/chirp'
        mode: audio_speech
      'deepseek-ai/deepseek-ocr-maas':
        litellm_id: 'vertex_ai/deepseek-ai/deepseek-ocr-maas'
        mode: ocr
        input_cost_per_1k: 0.0003
        output_cost_per_1k: 0.0012
      'gemini-1.0-pro':
        mode: chat
        max_input_tokens: 32760
        max_output_tokens: 8192
        input_cost_per_1k: 0.0005
        output_cost_per_1k: 0.0015
        capabilities: [function_calling, parallel_function_calling]
      'gemini-1.0-pro-001':
        mode: chat
        max_input_tokens: 32760
        max_output_tokens: 8192
        input_cost_per_1k: 0.0005
        output_cost_per_1k: 0.0015
        capabilities: [function_calling, parallel_function_calling]
        deprecated: true
        deprecation_date: '2025-04-09'
      'gemini-1.0-pro-002':
        mode: chat
        max_input_tokens: 32760
        max_output_tokens: 8192
        input_cost_per_1k: 0.0005
        output_cost_per_1k: 0.0015
        capabilities: [function_calling, parallel_function_calling]
        deprecated: true
        deprecation_date: '2025-04-09'
      'gemini-1.0-ultra':
        mode: chat
        max_input_tokens: 8192
        max_output_tokens: 2048
        input_cost_per_1k: 0.0005
        output_cost_per_1k: 0.0015
        capabilities: [function_calling, parallel_function_calling]
      'gemini-1.0-ultra-001':
        mode: chat
        max_input_tokens: 8192
        max_output_tokens: 2048
        input_cost_per_1k: 0.0005
        output_cost_per_1k: 0.0015
        capabilities: [function_calling, parallel_function_calling]
      'gemini-1.5-flash':
        mode: chat
        max_input_tokens: 1000000
        max_output_tokens: 8192
        input_cost_per_1k: 7.5e-05
        output_cost_per_1k: 0.0003
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode]
      'gemini-1.5-flash':
        litellm_id: 'gemini/gemini-1.5-flash'
        mode: chat
        max_input_tokens: 1048576
        max_output_tokens: 8192
        input_cost_per_1k: 7.5e-05
        output_cost_per_1k: 0.0003
        capabilities: [vision, function_calling, system_messages, json_mode]
      'gemini-1.5-flash-001':
        mode: chat
        max_input_tokens: 1000000
        max_output_tokens: 8192
        input_cost_per_1k: 7.5e-05
        output_cost_per_1k: 0.0003
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode]
        deprecated: true
        deprecation_date: '2025-05-24'
      'gemini-1.5-flash-001':
        litellm_id: 'gemini/gemini-1.5-flash-001'
        mode: chat
        max_input_tokens: 1048576
        max_output_tokens: 8192
        input_cost_per_1k: 7.5e-05
        output_cost_per_1k: 0.0003
        capabilities: [vision, function_calling, system_messages, json_mode, prompt_caching]
        deprecated: true
        deprecation_date: '2025-05-24'
      'gemini-1.5-flash-002':
        mode: chat
        max_input_tokens: 1048576
        max_output_tokens: 8192
        input_cost_per_1k: 7.5e-05
        output_cost_per_1k: 0.0003
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode]
        deprecated: true
        deprecation_date: '2025-09-24'
      'gemini-1.5-flash-002':
        litellm_id: 'gemini/gemini-1.5-flash-002'
        mode: chat
        max_input_tokens: 1048576
        max_output_tokens: 8192
        input_cost_per_1k: 7.5e-05
        output_cost_per_1k: 0.0003
        capabilities: [vision, function_calling, system_messages, json_mode, prompt_caching]
        deprecated: true
        deprecation_date: '2025-09-24'
      'gemini-1.5-flash-8b':
        litellm_id: 'gemini/gemini-1.5-flash-8b'
        mode: chat
        max_input_tokens: 1048576
        max_output_tokens: 8192
        capabilities: [vision, function_calling, system_messages, json_mode, prompt_caching]
      'gemini-1.5-flash-8b-exp-0827':
        litellm_id: 'gemini/gemini-1.5-flash-8b-exp-0827'
        mode: chat
        max_input_tokens: 1000000
        max_output_tokens: 8192
        capabilities: [vision, function_calling, system_messages, json_mode]
      'gemini-1.5-flash-8b-exp-0924':
        litellm_id: 'gemini/gemini-1.5-flash-8b-exp-0924'
        mode: chat
        max_input_tokens: 1048576
        max_output_tokens: 8192
        capabilities: [vision, function_calling, system_messages, json_mode, prompt_caching]
      'gemini-1.5-flash-exp-0827':
        mode: chat
        max_input_tokens: 1000000
        max_output_tokens: 8192
        input_cost_per_1k: 4.69e-06
        output_cost_per_1k: 4.69e-06
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode]
      'gemini-1.5-flash-exp-0827':
        litellm_id: 'gemini/gemini-1.5-flash-exp-0827'
        mode: chat
        max_input_tokens: 1048576
        max_output_tokens: 8192
        capabilities: [vision, function_calling, system_messages, json_mode]
      'gemini-1.5-flash-latest':
        litellm_id: 'gemini/gemini-1.5-flash-latest'
        mode: chat
        max_input_tokens: 1048576
        max_output_tokens: 8192
        input_cost_per_1k: 7.5e-05
        output_cost_per_1k: 0.0003
        capabilities: [vision, function_calling, system_messages, json_mode, prompt_caching]
      'gemini-1.5-flash-preview-0514':
        mode: chat
        max_input_tokens: 1000000
        max_output_tokens: 8192
        input_cost_per_1k: 7.5e-05
        output_cost_per_1k: 4.69e-06
        capabilities: [vision, function_calling, parallel_function_calling, system_messages]
      'gemini-1.5-pro':
        mode: chat
        max_input_tokens: 2097152
        max_output_tokens: 8192
        input_cost_per_1k: 0.00125
        output_cost_per_1k: 0.005
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode]
      'gemini-1.5-pro':
        litellm_id: 'gemini/gemini-1.5-pro'
        mode: chat
        max_input_tokens: 2097152
        max_output_tokens: 8192
        input_cost_per_1k: 0.0035
        output_cost_per_1k: 0.0105
        capabilities: [vision, function_calling, system_messages, json_mode]
      'gemini-1.5-pro-001':
        mode: chat
        max_input_tokens: 1000000
        max_output_tokens: 8192
        input_cost_per_1k: 0.00125
        output_cost_per_1k: 0.005
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode]
        deprecated: true
        deprecation_date: '2025-05-24'
      'gemini-1.5-pro-001':
        litellm_id: 'gemini/gemini-1.5-pro-001'
        mode: chat
        max_input_tokens: 2097152
        max_output_tokens: 8192
        input_cost_per_1k: 0.0035
        output_cost_per_1k: 0.0105
        capabilities: [vision, function_calling, system_messages, json_mode, prompt_caching]
        deprecated: true
        deprecation_date: '2025-05-24'
      'gemini-1.5-pro-002':
        mode: chat
        max_input_tokens: 2097152
        max_output_tokens: 8192
        input_cost_per_1k: 0.00125
        output_cost_per_1k: 0.005
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode]
        deprecated: true
        deprecation_date: '2025-09-24'
      'gemini-1.5-pro-002':
        litellm_id: 'gemini/gemini-1.5-pro-002'
        mode: chat
        max_input_tokens: 2097152
        max_output_tokens: 8192
        input_cost_per_1k: 0.0035
        output_cost_per_1k: 0.0105
        capabilities: [vision, function_calling, system_messages, json_mode, prompt_caching]
        deprecated: true
        deprecation_date: '2025-09-24'
      'gemini-1.5-pro-exp-0801':
        litellm_id: 'gemini/gemini-1.5-pro-exp-0801'
        mode: chat
        max_input_tokens: 2097152
        max_output_tokens: 8192
        input_cost_per_1k: 0.0035
        output_cost_per_1k: 0.0105
        capabilities: [vision, function_calling, system_messages, json_mode]
      'gemini-1.5-pro-exp-0827':
        litellm_id: 'gemini/gemini-1.5-pro-exp-0827'
        mode: chat
        max_input_tokens: 2097152
        max_output_tokens: 8192
        capabilities: [vision, function_calling, system_messages, json_mode]
      'gemini-1.5-pro-latest':
        litellm_id: 'gemini/gemini-1.5-pro-latest'
        mode: chat
        max_input_tokens: 1048576
        max_output_tokens: 8192
        input_cost_per_1k: 0.0035
        output_cost_per_1k: 0.00105
        capabilities: [vision, function_calling, system_messages, json_mode]
      'gemini-1.5-pro-preview-0215':
        mode: chat
        max_input_tokens: 1000000
        max_output_tokens: 8192
        input_cost_per_1k: 7.813e-05
        output_cost_per_1k: 0.0003125
        capabilities: [function_calling, parallel_function_calling, system_messages, json_mode]
      'gemini-1.5-pro-preview-0409':
        mode: chat
        max_input_tokens: 1000000
        max_output_tokens: 8192
        input_cost_per_1k: 7.813e-05
        output_cost_per_1k: 0.0003125
        capabilities: [function_calling, parallel_function_calling, json_mode]
      'gemini-1.5-pro-preview-0514':
        mode: chat
        max_input_tokens: 1000000
        max_output_tokens: 8192
        input_cost_per_1k: 7.813e-05
        output_cost_per_1k: 0.0003125
        capabilities: [function_calling, parallel_function_calling, system_messages, json_mode]
      'gemini-2.0-flash':
        mode: chat
        max_input_tokens: 1048576
        max_output_tokens: 8192
        input_cost_per_1k: 0.0001
        output_cost_per_1k: 0.0004
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching, web_search, audio_input, audio_output]
      'gemini-2.0-flash':
        litellm_id: 'gemini/gemini-2.0-flash'
        mode: chat
        max_input_tokens: 1048576
        max_output_tokens: 8192
        input_cost_per_1k: 0.0001
        output_cost_per_1k: 0.0004
        capabilities: [vision, function_calling, system_messages, json_mode, prompt_caching, web_search, audio_input, audio_output]
      'gemini-2.0-flash-001':
        mode: chat
        max_input_tokens: 1048576
        max_output_tokens: 8192
        input_cost_per_1k: 0.00015
        output_cost_per_1k: 0.0006
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching, web_search, audio_output]
        deprecated: true
        deprecation_date: '2026-02-05'
      'gemini-2.0-flash-001':
        litellm_id: 'gemini/gemini-2.0-flash-001'
        mode: chat
        max_input_tokens: 1048576
        max_output_tokens: 8192
        input_cost_per_1k: 0.0001
        output_cost_per_1k: 0.0004
        capabilities: [vision, function_calling, system_messages, json_mode, prompt_caching, web_search]
      'gemini-2.0-flash-exp':
        mode: chat
        max_input_tokens: 1048576
        max_output_tokens: 8192
        input_cost_per_1k: 0.00015
        output_cost_per_1k: 0.0006
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching, web_search, audio_output]
      'gemini-2.0-flash-exp':
        litellm_id: 'gemini/gemini-2.0-flash-exp'
        mode: chat
        max_input_tokens: 1048576
        max_output_tokens: 8192
        capabilities: [vision, function_calling, system_messages, json_mode, prompt_caching, web_search, audio_output]
      'gemini-2.0-flash-lite':
        mode: chat
        max_input_tokens: 1048576
        max_output_tokens: 8192
        input_cost_per_1k: 7.5e-05
        output_cost_per_1k: 0.0003
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching, web_search, audio_output]
      'gemini-2.0-flash-lite':
        litellm_id: 'gemini/gemini-2.0-flash-lite'
        mode: chat
        max_input_tokens: 1048576
        max_output_tokens: 8192
        input_cost_per_1k: 7.5e-05
        output_cost_per_1k: 0.0003
        capabilities: [vision, function_calling, system_messages, json_mode, prompt_caching, web_search, audio_output]
      'gemini-2.0-flash-lite-001':
        mode: chat
        max_input_tokens: 1048576
        max_output_tokens: 8192
        input_cost_per_1k: 7.5e-05
        output_cost_per_1k: 0.0003
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching, web_search, audio_output]
        deprecated: true
        deprecation_date: '2026-02-25'
      'gemini-2.0-flash-lite-preview-02-05':
        litellm_id: 'gemini/gemini-2.0-flash-lite-preview-02-05'
        mode: chat
        max_input_tokens: 1048576
        max_output_tokens: 8192
        input_cost_per_1k: 7.5e-05
        output_cost_per_1k: 0.0003
        capabilities: [vision, function_calling, system_messages, json_mode, prompt_caching, web_search]
      'gemini-2.0-flash-live-001':
        litellm_id: 'gemini/gemini-2.0-flash-live-001'
        mode: chat
        max_input_tokens: 1048576
        max_output_tokens: 65535
        input_cost_per_1k: 0.00035
        output_cost_per_1k: 0.0015
        capabilities: [vision, function_calling, system_messages, json_mode, prompt_caching, reasoning, web_search, audio_output]
      'gemini-2.0-flash-live-preview-04-09':
        mode: chat
        max_input_tokens: 1048576
        max_output_tokens: 65535
        input_cost_per_1k: 0.0005
        output_cost_per_1k: 0.002
        capabilities: [vision, function_calling, system_messages, json_mode, prompt_caching, reasoning, web_search, audio_output]
      'gemini-2.0-flash-preview-image-generation':
        mode: chat
        max_input_tokens: 1048576
        max_output_tokens: 8192
        input_cost_per_1k: 0.0001
        output_cost_per_1k: 0.0004
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching, web_search, audio_input, audio_output]
      'gemini-2.0-flash-preview-image-generation':
        litellm_id: 'gemini/gemini-2.0-flash-preview-image-generation'
        mode: chat
        max_input_tokens: 1048576
        max_output_tokens: 8192
        input_cost_per_1k: 0.0001
        output_cost_per_1k: 0.0004
        capabilities: [vision, function_calling, system_messages, json_mode, prompt_caching, web_search, audio_input, audio_output]
      'gemini-2.0-flash-thinking-exp':
        mode: chat
        max_input_tokens: 1048576
        max_output_tokens: 8192
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching, web_search, audio_output]
      'gemini-2.0-flash-thinking-exp':
        litellm_id: 'gemini/gemini-2.0-flash-thinking-exp'
        mode: chat
        max_input_tokens: 1048576
        max_output_tokens: 65536
        capabilities: [vision, function_calling, system_messages, json_mode, prompt_caching, web_search, audio_output]
      'gemini-2.0-flash-thinking-exp-01-21':
        mode: chat
        max_input_tokens: 1048576
        max_output_tokens: 65536
        capabilities: [vision, parallel_function_calling, system_messages, prompt_caching, reasoning, web_search]
      'gemini-2.0-flash-thinking-exp-01-21':
        litellm_id: 'gemini/gemini-2.0-flash-thinking-exp-01-21'
        mode: chat
        max_input_tokens: 1048576
        max_output_tokens: 65536
        capabilities: [vision, function_calling, system_messages, json_mode, prompt_caching, reasoning, web_search, audio_output]
      'gemini-2.0-pro-exp-02-05':
        mode: chat
        max_input_tokens: 2097152
        max_output_tokens: 8192
        input_cost_per_1k: 0.00125
        output_cost_per_1k: 0.01
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching, web_search, audio_input]
      'gemini-2.0-pro-exp-02-05':
        litellm_id: 'gemini/gemini-2.0-pro-exp-02-05'
        mode: chat
        max_input_tokens: 2097152
        max_output_tokens: 8192
        capabilities: [vision, function_calling, system_messages, json_mode, prompt_caching, web_search, audio_input]
      'gemini-2.5-computer-use-preview-10-2025':
        litellm_id: 'gemini/gemini-2.5-computer-use-preview-10-2025'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 64000
        input_cost_per_1k: 0.00125
        output_cost_per_1k: 0.01
        capabilities: [vision, function_calling, system_messages]
      'gemini-2.5-flash':
        mode: chat
        max_input_tokens: 1048576
        max_output_tokens: 65535
        input_cost_per_1k: 0.0003
        output_cost_per_1k: 0.0025
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching, reasoning, web_search]
      'gemini-2.5-flash':
        litellm_id: 'gemini/gemini-2.5-flash'
        mode: chat
        max_input_tokens: 1048576
        max_output_tokens: 65535
        input_cost_per_1k: 0.0003
        output_cost_per_1k: 0.0025
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching, reasoning, web_search]
      'gemini-2.5-flash-image':
        mode: image
        max_input_tokens: 32768
        max_output_tokens: 32768
        input_cost_per_1k: 0.0003
        output_cost_per_1k: 0.0025
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching]
      'gemini-2.5-flash-image-preview':
        mode: image
        max_input_tokens: 1048576
        max_output_tokens: 65535
        input_cost_per_1k: 0.0003
        output_cost_per_1k: 0.03
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching, web_search]
      'gemini-2.5-flash-image-preview':
        litellm_id: 'gemini/gemini-2.5-flash-image-preview'
        mode: image
        max_input_tokens: 1048576
        max_output_tokens: 65535
        input_cost_per_1k: 0.0003
        output_cost_per_1k: 0.03
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching, web_search]
      'gemini-2.5-flash-lite':
        mode: chat
        max_input_tokens: 1048576
        max_output_tokens: 65535
        input_cost_per_1k: 0.0001
        output_cost_per_1k: 0.0004
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching, reasoning, web_search]
      'gemini-2.5-flash-lite':
        litellm_id: 'gemini/gemini-2.5-flash-lite'
        mode: chat
        max_input_tokens: 1048576
        max_output_tokens: 65535
        input_cost_per_1k: 0.0001
        output_cost_per_1k: 0.0004
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching, reasoning, web_search]
      'gemini-2.5-flash-lite-preview-06-17':
        mode: chat
        max_input_tokens: 1048576
        max_output_tokens: 65535
        input_cost_per_1k: 0.0001
        output_cost_per_1k: 0.0004
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching, reasoning, web_search]
      'gemini-2.5-flash-lite-preview-06-17':
        litellm_id: 'gemini/gemini-2.5-flash-lite-preview-06-17'
        mode: chat
        max_input_tokens: 1048576
        max_output_tokens: 65535
        input_cost_per_1k: 0.0001
        output_cost_per_1k: 0.0004
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching, reasoning, web_search]
      'gemini-2.5-flash-lite-preview-09-2025':
        mode: chat
        max_input_tokens: 1048576
        max_output_tokens: 65535
        input_cost_per_1k: 0.0001
        output_cost_per_1k: 0.0004
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching, reasoning, web_search]
      'gemini-2.5-flash-lite-preview-09-2025':
        litellm_id: 'gemini/gemini-2.5-flash-lite-preview-09-2025'
        mode: chat
        max_input_tokens: 1048576
        max_output_tokens: 65535
        input_cost_per_1k: 0.0001
        output_cost_per_1k: 0.0004
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching, reasoning, web_search]
      'gemini-2.5-flash-preview-04-17':
        mode: chat
        max_input_tokens: 1048576
        max_output_tokens: 65535
        input_cost_per_1k: 0.00015
        output_cost_per_1k: 0.0006
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching, reasoning, web_search]
      'gemini-2.5-flash-preview-04-17':
        litellm_id: 'gemini/gemini-2.5-flash-preview-04-17'
        mode: chat
        max_input_tokens: 1048576
        max_output_tokens: 65535
        input_cost_per_1k: 0.00015
        output_cost_per_1k: 0.0006
        capabilities: [vision, function_calling, system_messages, json_mode, prompt_caching, reasoning, web_search]
      'gemini-2.5-flash-preview-05-20':
        mode: chat
        max_input_tokens: 1048576
        max_output_tokens: 65535
        input_cost_per_1k: 0.0003
        output_cost_per_1k: 0.0025
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching, reasoning, web_search]
      'gemini-2.5-flash-preview-05-20':
        litellm_id: 'gemini/gemini-2.5-flash-preview-05-20'
        mode: chat
        max_input_tokens: 1048576
        max_output_tokens: 65535
        input_cost_per_1k: 0.0003
        output_cost_per_1k: 0.0025
        capabilities: [vision, function_calling, system_messages, json_mode, prompt_caching, reasoning, web_search]
      'gemini-2.5-flash-preview-09-2025':
        mode: chat
        max_input_tokens: 1048576
        max_output_tokens: 65535
        input_cost_per_1k: 0.0003
        output_cost_per_1k: 0.0025
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching, reasoning, web_search]
      'gemini-2.5-flash-preview-09-2025':
        litellm_id: 'gemini/gemini-2.5-flash-preview-09-2025'
        mode: chat
        max_input_tokens: 1048576
        max_output_tokens: 65535
        input_cost_per_1k: 0.0003
        output_cost_per_1k: 0.0025
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching, reasoning, web_search]
      'gemini-2.5-flash-preview-tts':
        litellm_id: 'gemini/gemini-2.5-flash-preview-tts'
        mode: chat
        max_input_tokens: 1048576
        max_output_tokens: 65535
        input_cost_per_1k: 0.00015
        output_cost_per_1k: 0.0006
        capabilities: [vision, function_calling, system_messages, json_mode, prompt_caching, reasoning, web_search]
      'gemini-2.5-pro':
        mode: chat
        max_input_tokens: 1048576
        max_output_tokens: 65535
        input_cost_per_1k: 0.00125
        output_cost_per_1k: 0.01
        capabilities: [vision, function_calling, system_messages, json_mode, prompt_caching, reasoning, web_search, audio_input]
      'gemini-2.5-pro':
        litellm_id: 'gemini/gemini-2.5-pro'
        mode: chat
        max_input_tokens: 1048576
        max_output_tokens: 65535
        input_cost_per_1k: 0.00125
        output_cost_per_1k: 0.01
        capabilities: [vision, function_calling, system_messages, json_mode, prompt_caching, reasoning, web_search, audio_input]
      'gemini-2.5-pro-exp-03-25':
        mode: chat
        max_input_tokens: 1048576
        max_output_tokens: 65535
        input_cost_per_1k: 0.00125
        output_cost_per_1k: 0.01
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching, web_search, audio_input]
      'gemini-2.5-pro-exp-03-25':
        litellm_id: 'gemini/gemini-2.5-pro-exp-03-25'
        mode: chat
        max_input_tokens: 1048576
        max_output_tokens: 65535
        capabilities: [vision, function_calling, system_messages, json_mode, prompt_caching, web_search, audio_input]
      'gemini-2.5-pro-preview-03-25':
        mode: chat
        max_input_tokens: 1048576
        max_output_tokens: 65535
        input_cost_per_1k: 0.00125
        output_cost_per_1k: 0.01
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching, reasoning, web_search]
      'gemini-2.5-pro-preview-03-25':
        litellm_id: 'gemini/gemini-2.5-pro-preview-03-25'
        mode: chat
        max_input_tokens: 1048576
        max_output_tokens: 65535
        input_cost_per_1k: 0.00125
        output_cost_per_1k: 0.01
        capabilities: [vision, function_calling, system_messages, json_mode, prompt_caching, web_search]
      'gemini-2.5-pro-preview-05-06':
        mode: chat
        max_input_tokens: 1048576
        max_output_tokens: 65535
        input_cost_per_1k: 0.00125
        output_cost_per_1k: 0.01
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching, reasoning, web_search]
      'gemini-2.5-pro-preview-05-06':
        litellm_id: 'gemini/gemini-2.5-pro-preview-05-06'
        mode: chat
        max_input_tokens: 1048576
        max_output_tokens: 65535
        input_cost_per_1k: 0.00125
        output_cost_per_1k: 0.01
        capabilities: [vision, function_calling, system_messages, json_mode, prompt_caching, web_search]
      'gemini-2.5-pro-preview-06-05':
        mode: chat
        max_input_tokens: 1048576
        max_output_tokens: 65535
        input_cost_per_1k: 0.00125
        output_cost_per_1k: 0.01
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching, reasoning, web_search]
      'gemini-2.5-pro-preview-06-05':
        litellm_id: 'gemini/gemini-2.5-pro-preview-06-05'
        mode: chat
        max_input_tokens: 1048576
        max_output_tokens: 65535
        input_cost_per_1k: 0.00125
        output_cost_per_1k: 0.01
        capabilities: [vision, function_calling, system_messages, json_mode, prompt_caching, web_search]
      'gemini-2.5-pro-preview-tts':
        mode: chat
        max_input_tokens: 1048576
        max_output_tokens: 65535
        input_cost_per_1k: 0.00125
        output_cost_per_1k: 0.01
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching, web_search]
      'gemini-2.5-pro-preview-tts':
        litellm_id: 'gemini/gemini-2.5-pro-preview-tts'
        mode: chat
        max_input_tokens: 1048576
        max_output_tokens: 65535
        input_cost_per_1k: 0.00125
        output_cost_per_1k: 0.01
        capabilities: [vision, function_calling, system_messages, json_mode, prompt_caching, web_search]
      'gemini-3-flash-preview':
        litellm_id: 'vertex_ai/gemini-3-flash-preview'
        mode: chat
        max_input_tokens: 1048576
        max_output_tokens: 65535
        input_cost_per_1k: 0.0005
        output_cost_per_1k: 0.003
        capabilities: [vision, function_calling, system_messages, json_mode, prompt_caching, reasoning, web_search, audio_input]
      'gemini-3-flash-preview':
        litellm_id: 'gemini/gemini-3-flash-preview'
        mode: chat
        max_input_tokens: 1048576
        max_output_tokens: 65535
        input_cost_per_1k: 0.0005
        output_cost_per_1k: 0.003
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching, reasoning, web_search]
      'gemini-3-flash-preview':
        mode: chat
        max_input_tokens: 1048576
        max_output_tokens: 65535
        input_cost_per_1k: 0.0005
        output_cost_per_1k: 0.003
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching, reasoning, web_search]
      'gemini-3-pro-image-preview':
        mode: image
        max_input_tokens: 65536
        max_output_tokens: 32768
        input_cost_per_1k: 0.002
        output_cost_per_1k: 0.012
        capabilities: [vision, system_messages, json_mode, prompt_caching, web_search]
      'gemini-3-pro-image-preview':
        litellm_id: 'gemini/gemini-3-pro-image-preview'
        mode: image
        max_input_tokens: 65536
        max_output_tokens: 32768
        input_cost_per_1k: 0.002
        output_cost_per_1k: 0.012
        capabilities: [vision, system_messages, json_mode, prompt_caching, web_search]
      'gemini-3-pro-preview':
        mode: chat
        max_input_tokens: 1048576
        max_output_tokens: 65535
        input_cost_per_1k: 0.002
        output_cost_per_1k: 0.012
        capabilities: [vision, function_calling, system_messages, json_mode, prompt_caching, reasoning, web_search, audio_input]
      'gemini-3-pro-preview':
        litellm_id: 'vertex_ai/gemini-3-pro-preview'
        mode: chat
        max_input_tokens: 1048576
        max_output_tokens: 65535
        input_cost_per_1k: 0.002
        output_cost_per_1k: 0.012
        capabilities: [vision, function_calling, system_messages, json_mode, prompt_caching, reasoning, web_search, audio_input]
      'gemini-3-pro-preview':
        litellm_id: 'gemini/gemini-3-pro-preview'
        mode: chat
        max_input_tokens: 1048576
        max_output_tokens: 65535
        input_cost_per_1k: 0.002
        output_cost_per_1k: 0.012
        capabilities: [vision, function_calling, system_messages, json_mode, prompt_caching, reasoning, web_search, audio_input]
      'gemini-embedding-001':
        litellm_id: 'gemini/gemini-embedding-001'
        mode: embedding
        max_input_tokens: 2048
        input_cost_per_1k: 0.00015
      'gemini-exp-1114':
        litellm_id: 'gemini/gemini-exp-1114'
        mode: chat
        max_input_tokens: 1048576
        max_output_tokens: 8192
        capabilities: [vision, function_calling, system_messages, json_mode]
      'gemini-exp-1206':
        litellm_id: 'gemini/gemini-exp-1206'
        mode: chat
        max_input_tokens: 2097152
        max_output_tokens: 8192
        capabilities: [vision, function_calling, system_messages, json_mode]
      'gemini-flash-experimental':
        mode: chat
        max_input_tokens: 1000000
        max_output_tokens: 8192
        capabilities: [parallel_function_calling]
      'gemini-flash-latest':
        litellm_id: 'gemini/gemini-flash-latest'
        mode: chat
        max_input_tokens: 1048576
        max_output_tokens: 65535
        input_cost_per_1k: 0.0003
        output_cost_per_1k: 0.0025
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching, reasoning, web_search]
      'gemini-flash-lite-latest':
        litellm_id: 'gemini/gemini-flash-lite-latest'
        mode: chat
        max_input_tokens: 1048576
        max_output_tokens: 65535
        input_cost_per_1k: 0.0001
        output_cost_per_1k: 0.0004
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching, reasoning, web_search]
      'gemini-gemma-2-27b-it':
        litellm_id: 'gemini/gemini-gemma-2-27b-it'
        mode: chat
        max_input_tokens: 8192
        max_output_tokens: 8192
        input_cost_per_1k: 0.00035
        output_cost_per_1k: 0.00105
        capabilities: [vision, function_calling]
      'gemini-gemma-2-9b-it':
        litellm_id: 'gemini/gemini-gemma-2-9b-it'
        mode: chat
        max_input_tokens: 8192
        max_output_tokens: 8192
        input_cost_per_1k: 0.00035
        output_cost_per_1k: 0.00105
        capabilities: [vision, function_calling]
      'gemini-live-2.5-flash-preview-native-audio-09-2025':
        mode: chat
        max_input_tokens: 1048576
        max_output_tokens: 65535
        input_cost_per_1k: 0.0003
        output_cost_per_1k: 0.002
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching, web_search, audio_input, audio_output]
      'gemini-live-2.5-flash-preview-native-audio-09-2025':
        litellm_id: 'gemini/gemini-live-2.5-flash-preview-native-audio-09-2025'
        mode: chat
        max_input_tokens: 1048576
        max_output_tokens: 65535
        input_cost_per_1k: 0.0003
        output_cost_per_1k: 0.002
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching, web_search, audio_input, audio_output]
      'gemini-pro':
        mode: chat
        max_input_tokens: 32760
        max_output_tokens: 8192
        input_cost_per_1k: 0.0005
        output_cost_per_1k: 0.0015
        capabilities: [function_calling, parallel_function_calling]
      'gemini-pro':
        litellm_id: 'gemini/gemini-pro'
        mode: chat
        max_input_tokens: 32760
        max_output_tokens: 8192
        input_cost_per_1k: 0.00035
        output_cost_per_1k: 0.00105
        capabilities: [function_calling]
      'gemini-pro-experimental':
        mode: chat
        max_input_tokens: 1000000
        max_output_tokens: 8192
        capabilities: [parallel_function_calling]
      'gemini-pro-vision':
        litellm_id: 'gemini/gemini-pro-vision'
        mode: chat
        max_input_tokens: 30720
        max_output_tokens: 2048
        input_cost_per_1k: 0.00035
        output_cost_per_1k: 0.00105
        capabilities: [vision, function_calling]
      'gemini/gemini-2.5-flash-image':
        mode: image
        max_input_tokens: 32768
        max_output_tokens: 32768
        input_cost_per_1k: 0.0003
        output_cost_per_1k: 0.0025
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching, web_search]
      'gemma-3-27b-it':
        litellm_id: 'gemini/gemma-3-27b-it'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 8192
        capabilities: [vision, function_calling, json_mode]
      'imagen-3.0-fast-generate-001':
        litellm_id: 'gemini/imagen-3.0-fast-generate-001'
        mode: image
      'imagen-3.0-generate-001':
        litellm_id: 'gemini/imagen-3.0-generate-001'
        mode: image
      'imagen-3.0-generate-002':
        litellm_id: 'gemini/imagen-3.0-generate-002'
        mode: image
      'imagen-4.0-fast-generate-001':
        litellm_id: 'gemini/imagen-4.0-fast-generate-001'
        mode: image
      'imagen-4.0-generate-001':
        litellm_id: 'gemini/imagen-4.0-generate-001'
        mode: image
      'imagen-4.0-ultra-generate-001':
        litellm_id: 'gemini/imagen-4.0-ultra-generate-001'
        mode: image
      'learnlm-1.5-pro-experimental':
        litellm_id: 'gemini/learnlm-1.5-pro-experimental'
        mode: chat
        max_input_tokens: 32767
        max_output_tokens: 8192
        capabilities: [vision, function_calling, system_messages, json_mode]
      'medlm-large':
        mode: chat
        max_input_tokens: 8192
        max_output_tokens: 1024
      'medlm-medium':
        mode: chat
        max_input_tokens: 32768
        max_output_tokens: 8192
      'mistral-ocr-2505':
        litellm_id: 'vertex_ai/mistral-ocr-2505'
        mode: ocr
      'search_api':
        litellm_id: 'vertex_ai/search_api'
        mode: vector_store
      'veo-2.0-generate-001':
        litellm_id: 'gemini/veo-2.0-generate-001'
        mode: video_generation
        max_input_tokens: 1024
      'veo-3.0-fast-generate-preview':
        litellm_id: 'gemini/veo-3.0-fast-generate-preview'
        mode: video_generation
        max_input_tokens: 1024
      'veo-3.0-generate-preview':
        litellm_id: 'gemini/veo-3.0-generate-preview'
        mode: video_generation
        max_input_tokens: 1024
      'veo-3.1-fast-generate-001':
        litellm_id: 'gemini/veo-3.1-fast-generate-001'
        mode: video_generation
        max_input_tokens: 1024
      'veo-3.1-fast-generate-preview':
        litellm_id: 'gemini/veo-3.1-fast-generate-preview'
        mode: video_generation
        max_input_tokens: 1024
      'veo-3.1-generate-001':
        litellm_id: 'gemini/veo-3.1-generate-001'
        mode: video_generation
        max_input_tokens: 1024
      'veo-3.1-generate-preview':
        litellm_id: 'gemini/veo-3.1-generate-preview'
        mode: video_generation
        max_input_tokens: 1024
      'vertex_ai/gemini-2.5-flash-image':
        mode: image
        max_input_tokens: 32768
        max_output_tokens: 32768
        input_cost_per_1k: 0.0003
        output_cost_per_1k: 0.0025
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching]
      'vertex_ai/gemini-3-pro-image-preview':
        mode: image
        max_input_tokens: 65536
        max_output_tokens: 32768
        input_cost_per_1k: 0.002
        output_cost_per_1k: 0.012
  google_pse:
    model_count: 1
    models:
      'search':
        litellm_id: 'google_pse/search'
        mode: search
  gradient_ai:
    model_count: 13
    models:
      'alibaba-qwen3-32b':
        litellm_id: 'gradient_ai/alibaba-qwen3-32b'
        mode: chat
        max_input_tokens: 2048
      'anthropic-claude-3-opus':
        litellm_id: 'gradient_ai/anthropic-claude-3-opus'
        mode: chat
        max_input_tokens: 1024
        input_cost_per_1k: 0.015
        output_cost_per_1k: 0.075
      'anthropic-claude-3.5-haiku':
        litellm_id: 'gradient_ai/anthropic-claude-3.5-haiku'
        mode: chat
        max_input_tokens: 1024
        input_cost_per_1k: 0.0008
        output_cost_per_1k: 0.004
      'anthropic-claude-3.5-sonnet':
        litellm_id: 'gradient_ai/anthropic-claude-3.5-sonnet'
        mode: chat
        max_input_tokens: 1024
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.015
      'anthropic-claude-3.7-sonnet':
        litellm_id: 'gradient_ai/anthropic-claude-3.7-sonnet'
        mode: chat
        max_input_tokens: 1024
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.015
      'deepseek-r1-distill-llama-70b':
        litellm_id: 'gradient_ai/deepseek-r1-distill-llama-70b'
        mode: chat
        max_input_tokens: 8000
        input_cost_per_1k: 0.00099
        output_cost_per_1k: 0.00099
      'llama3-8b-instruct':
        litellm_id: 'gradient_ai/llama3-8b-instruct'
        mode: chat
        max_input_tokens: 512
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
      'llama3.3-70b-instruct':
        litellm_id: 'gradient_ai/llama3.3-70b-instruct'
        mode: chat
        max_input_tokens: 2048
        input_cost_per_1k: 0.00065
        output_cost_per_1k: 0.00065
      'mistral-nemo-instruct-2407':
        litellm_id: 'gradient_ai/mistral-nemo-instruct-2407'
        mode: chat
        max_input_tokens: 512
        input_cost_per_1k: 0.0003
        output_cost_per_1k: 0.0003
      'openai-gpt-4o':
        litellm_id: 'gradient_ai/openai-gpt-4o'
        mode: chat
        max_input_tokens: 16384
      'openai-gpt-4o-mini':
        litellm_id: 'gradient_ai/openai-gpt-4o-mini'
        mode: chat
        max_input_tokens: 16384
      'openai-o3':
        litellm_id: 'gradient_ai/openai-o3'
        mode: chat
        max_input_tokens: 100000
        input_cost_per_1k: 0.002
        output_cost_per_1k: 0.008
      'openai-o3-mini':
        litellm_id: 'gradient_ai/openai-o3-mini'
        mode: chat
        max_input_tokens: 100000
        input_cost_per_1k: 0.0011
        output_cost_per_1k: 0.0044
  groq:
    model_count: 12
    models:
      'llama-3.1-8b-instant':
        litellm_id: 'groq/llama-3.1-8b-instant'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 8192
        input_cost_per_1k: 5e-05
        output_cost_per_1k: 8e-05
        capabilities: [function_calling]
      'llama-3.3-70b-versatile':
        litellm_id: 'groq/llama-3.3-70b-versatile'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 32768
        input_cost_per_1k: 0.00059
        output_cost_per_1k: 0.00079
        capabilities: [function_calling]
      'meta-llama/llama-4-maverick-17b-128e-instruct':
        litellm_id: 'groq/meta-llama/llama-4-maverick-17b-128e-instruct'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 8192
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0006
        capabilities: [vision, function_calling, json_mode]
      'meta-llama/llama-4-scout-17b-16e-instruct':
        litellm_id: 'groq/meta-llama/llama-4-scout-17b-16e-instruct'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 8192
        input_cost_per_1k: 0.00011
        output_cost_per_1k: 0.00034
        capabilities: [vision, function_calling, json_mode]
      'meta-llama/llama-guard-4-12b':
        litellm_id: 'groq/meta-llama/llama-guard-4-12b'
        mode: chat
        max_input_tokens: 8192
        max_output_tokens: 8192
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
      'moonshotai/kimi-k2-instruct-0905':
        litellm_id: 'groq/moonshotai/kimi-k2-instruct-0905'
        mode: chat
        max_input_tokens: 262144
        max_output_tokens: 16384
        input_cost_per_1k: 0.001
        output_cost_per_1k: 0.003
        capabilities: [function_calling, json_mode]
      'openai/gpt-oss-120b':
        litellm_id: 'groq/openai/gpt-oss-120b'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 32766
        input_cost_per_1k: 0.00015
        output_cost_per_1k: 0.00075
        capabilities: [function_calling, parallel_function_calling, json_mode, reasoning, web_search]
      'openai/gpt-oss-20b':
        litellm_id: 'groq/openai/gpt-oss-20b'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 32768
        input_cost_per_1k: 0.0001
        output_cost_per_1k: 0.0005
        capabilities: [function_calling, parallel_function_calling, json_mode, reasoning, web_search]
      'playai-tts':
        litellm_id: 'groq/playai-tts'
        mode: audio_speech
        max_input_tokens: 10000
        max_output_tokens: 10000
      'qwen/qwen3-32b':
        litellm_id: 'groq/qwen/qwen3-32b'
        mode: chat
        max_input_tokens: 131000
        max_output_tokens: 131000
        input_cost_per_1k: 0.00029
        output_cost_per_1k: 0.00059
        capabilities: [function_calling, reasoning]
      'whisper-large-v3':
        litellm_id: 'groq/whisper-large-v3'
        mode: audio_transcription
      'whisper-large-v3-turbo':
        litellm_id: 'groq/whisper-large-v3-turbo'
        mode: audio_transcription
  heroku:
    model_count: 4
    models:
      'claude-3-5-haiku':
        litellm_id: 'heroku/claude-3-5-haiku'
        mode: chat
        max_input_tokens: 4096
        capabilities: [function_calling, system_messages]
      'claude-3-5-sonnet-latest':
        litellm_id: 'heroku/claude-3-5-sonnet-latest'
        mode: chat
        max_input_tokens: 8192
        capabilities: [function_calling, system_messages]
      'claude-3-7-sonnet':
        litellm_id: 'heroku/claude-3-7-sonnet'
        mode: chat
        max_input_tokens: 8192
        capabilities: [function_calling, system_messages]
      'claude-4-sonnet':
        litellm_id: 'heroku/claude-4-sonnet'
        mode: chat
        max_input_tokens: 8192
        capabilities: [function_calling, system_messages]
  hyperbolic:
    model_count: 16
    models:
      'NousResearch/Hermes-3-Llama-3.1-70B':
        litellm_id: 'hyperbolic/NousResearch/Hermes-3-Llama-3.1-70B'
        mode: chat
        max_input_tokens: 32768
        max_output_tokens: 32768
        input_cost_per_1k: 0.00012
        output_cost_per_1k: 0.0003
        capabilities: [function_calling, parallel_function_calling, system_messages]
      'Qwen/QwQ-32B':
        litellm_id: 'hyperbolic/Qwen/QwQ-32B'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
        capabilities: [function_calling, parallel_function_calling, system_messages]
      'Qwen/Qwen2.5-72B-Instruct':
        litellm_id: 'hyperbolic/Qwen/Qwen2.5-72B-Instruct'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.00012
        output_cost_per_1k: 0.0003
        capabilities: [function_calling, parallel_function_calling, system_messages]
      'Qwen/Qwen2.5-Coder-32B-Instruct':
        litellm_id: 'hyperbolic/Qwen/Qwen2.5-Coder-32B-Instruct'
        mode: chat
        max_input_tokens: 32768
        max_output_tokens: 32768
        input_cost_per_1k: 0.00012
        output_cost_per_1k: 0.0003
        capabilities: [function_calling, parallel_function_calling, system_messages]
      'Qwen/Qwen3-235B-A22B':
        litellm_id: 'hyperbolic/Qwen/Qwen3-235B-A22B'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.002
        output_cost_per_1k: 0.002
        capabilities: [function_calling, parallel_function_calling, system_messages]
      'deepseek-ai/DeepSeek-R1':
        litellm_id: 'hyperbolic/deepseek-ai/DeepSeek-R1'
        mode: chat
        max_input_tokens: 32768
        max_output_tokens: 32768
        input_cost_per_1k: 0.0004
        output_cost_per_1k: 0.0004
        capabilities: [function_calling, parallel_function_calling, system_messages]
      'deepseek-ai/DeepSeek-R1-0528':
        litellm_id: 'hyperbolic/deepseek-ai/DeepSeek-R1-0528'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.00025
        output_cost_per_1k: 0.00025
        capabilities: [function_calling, parallel_function_calling, system_messages]
      'deepseek-ai/DeepSeek-V3':
        litellm_id: 'hyperbolic/deepseek-ai/DeepSeek-V3'
        mode: chat
        max_input_tokens: 32768
        max_output_tokens: 32768
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
        capabilities: [function_calling, parallel_function_calling, system_messages]
      'deepseek-ai/DeepSeek-V3-0324':
        litellm_id: 'hyperbolic/deepseek-ai/DeepSeek-V3-0324'
        mode: chat
        max_input_tokens: 32768
        max_output_tokens: 32768
        input_cost_per_1k: 0.0004
        output_cost_per_1k: 0.0004
        capabilities: [function_calling, parallel_function_calling, system_messages]
      'meta-llama/Llama-3.2-3B-Instruct':
        litellm_id: 'hyperbolic/meta-llama/Llama-3.2-3B-Instruct'
        mode: chat
        max_input_tokens: 32768
        max_output_tokens: 32768
        input_cost_per_1k: 0.00012
        output_cost_per_1k: 0.0003
        capabilities: [function_calling, parallel_function_calling, system_messages]
      'meta-llama/Llama-3.3-70B-Instruct':
        litellm_id: 'hyperbolic/meta-llama/Llama-3.3-70B-Instruct'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.00012
        output_cost_per_1k: 0.0003
        capabilities: [function_calling, parallel_function_calling, system_messages]
      'meta-llama/Meta-Llama-3-70B-Instruct':
        litellm_id: 'hyperbolic/meta-llama/Meta-Llama-3-70B-Instruct'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.00012
        output_cost_per_1k: 0.0003
        capabilities: [function_calling, parallel_function_calling, system_messages]
      'meta-llama/Meta-Llama-3.1-405B-Instruct':
        litellm_id: 'hyperbolic/meta-llama/Meta-Llama-3.1-405B-Instruct'
        mode: chat
        max_input_tokens: 32768
        max_output_tokens: 32768
        input_cost_per_1k: 0.00012
        output_cost_per_1k: 0.0003
        capabilities: [function_calling, parallel_function_calling, system_messages]
      'meta-llama/Meta-Llama-3.1-70B-Instruct':
        litellm_id: 'hyperbolic/meta-llama/Meta-Llama-3.1-70B-Instruct'
        mode: chat
        max_input_tokens: 32768
        max_output_tokens: 32768
        input_cost_per_1k: 0.00012
        output_cost_per_1k: 0.0003
        capabilities: [function_calling, parallel_function_calling, system_messages]
      'meta-llama/Meta-Llama-3.1-8B-Instruct':
        litellm_id: 'hyperbolic/meta-llama/Meta-Llama-3.1-8B-Instruct'
        mode: chat
        max_input_tokens: 32768
        max_output_tokens: 32768
        input_cost_per_1k: 0.00012
        output_cost_per_1k: 0.0003
        capabilities: [function_calling, parallel_function_calling, system_messages]
      'moonshotai/Kimi-K2-Instruct':
        litellm_id: 'hyperbolic/moonshotai/Kimi-K2-Instruct'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.002
        output_cost_per_1k: 0.002
        capabilities: [function_calling, parallel_function_calling, system_messages]
  jina_ai:
    model_count: 1
    models:
      'jina-reranker-v2-base-multilingual':
        mode: rerank
        max_input_tokens: 1024
        max_output_tokens: 1024
        input_cost_per_1k: 1.8e-05
        output_cost_per_1k: 1.8e-05
  lambda_ai:
    model_count: 20
    models:
      'deepseek-llama3.3-70b':
        litellm_id: 'lambda_ai/deepseek-llama3.3-70b'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0006
        capabilities: [function_calling, parallel_function_calling, system_messages, reasoning]
      'deepseek-r1-0528':
        litellm_id: 'lambda_ai/deepseek-r1-0528'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0006
        capabilities: [function_calling, parallel_function_calling, system_messages, reasoning]
      'deepseek-r1-671b':
        litellm_id: 'lambda_ai/deepseek-r1-671b'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.0008
        output_cost_per_1k: 0.0008
        capabilities: [function_calling, parallel_function_calling, system_messages, reasoning]
      'deepseek-v3-0324':
        litellm_id: 'lambda_ai/deepseek-v3-0324'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0006
        capabilities: [function_calling, parallel_function_calling, system_messages]
      'hermes3-405b':
        litellm_id: 'lambda_ai/hermes3-405b'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.0008
        output_cost_per_1k: 0.0008
        capabilities: [function_calling, parallel_function_calling, system_messages]
      'hermes3-70b':
        litellm_id: 'lambda_ai/hermes3-70b'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.00012
        output_cost_per_1k: 0.0003
        capabilities: [function_calling, parallel_function_calling, system_messages]
      'hermes3-8b':
        litellm_id: 'lambda_ai/hermes3-8b'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 2.5e-05
        output_cost_per_1k: 4e-05
        capabilities: [function_calling, parallel_function_calling, system_messages]
      'lfm-40b':
        litellm_id: 'lambda_ai/lfm-40b'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.0001
        output_cost_per_1k: 0.0002
        capabilities: [function_calling, parallel_function_calling, system_messages]
      'lfm-7b':
        litellm_id: 'lambda_ai/lfm-7b'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 2.5e-05
        output_cost_per_1k: 4e-05
        capabilities: [function_calling, parallel_function_calling, system_messages]
      'llama-4-maverick-17b-128e-instruct-fp8':
        litellm_id: 'lambda_ai/llama-4-maverick-17b-128e-instruct-fp8'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 8192
        input_cost_per_1k: 5e-05
        output_cost_per_1k: 0.0001
        capabilities: [function_calling, parallel_function_calling, system_messages]
      'llama-4-scout-17b-16e-instruct':
        litellm_id: 'lambda_ai/llama-4-scout-17b-16e-instruct'
        mode: chat
        max_input_tokens: 16384
        max_output_tokens: 8192
        input_cost_per_1k: 5e-05
        output_cost_per_1k: 0.0001
        capabilities: [function_calling, parallel_function_calling, system_messages]
      'llama3.1-405b-instruct-fp8':
        litellm_id: 'lambda_ai/llama3.1-405b-instruct-fp8'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.0008
        output_cost_per_1k: 0.0008
        capabilities: [function_calling, parallel_function_calling, system_messages]
      'llama3.1-70b-instruct-fp8':
        litellm_id: 'lambda_ai/llama3.1-70b-instruct-fp8'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.00012
        output_cost_per_1k: 0.0003
        capabilities: [function_calling, parallel_function_calling, system_messages]
      'llama3.1-8b-instruct':
        litellm_id: 'lambda_ai/llama3.1-8b-instruct'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 2.5e-05
        output_cost_per_1k: 4e-05
        capabilities: [function_calling, parallel_function_calling, system_messages]
      'llama3.1-nemotron-70b-instruct-fp8':
        litellm_id: 'lambda_ai/llama3.1-nemotron-70b-instruct-fp8'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.00012
        output_cost_per_1k: 0.0003
        capabilities: [function_calling, parallel_function_calling, system_messages]
      'llama3.2-11b-vision-instruct':
        litellm_id: 'lambda_ai/llama3.2-11b-vision-instruct'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 1.5e-05
        output_cost_per_1k: 2.5e-05
        capabilities: [vision, function_calling, parallel_function_calling, system_messages]
      'llama3.2-3b-instruct':
        litellm_id: 'lambda_ai/llama3.2-3b-instruct'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 1.5e-05
        output_cost_per_1k: 2.5e-05
        capabilities: [function_calling, parallel_function_calling, system_messages]
      'llama3.3-70b-instruct-fp8':
        litellm_id: 'lambda_ai/llama3.3-70b-instruct-fp8'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.00012
        output_cost_per_1k: 0.0003
        capabilities: [function_calling, parallel_function_calling, system_messages]
      'qwen25-coder-32b-instruct':
        litellm_id: 'lambda_ai/qwen25-coder-32b-instruct'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 5e-05
        output_cost_per_1k: 0.0001
        capabilities: [function_calling, parallel_function_calling, system_messages]
      'qwen3-32b-fp8':
        litellm_id: 'lambda_ai/qwen3-32b-fp8'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 5e-05
        output_cost_per_1k: 0.0001
        capabilities: [function_calling, parallel_function_calling, system_messages, reasoning]
  lemonade:
    model_count: 5
    models:
      'Gemma-3-4b-it-GGUF':
        litellm_id: 'lemonade/Gemma-3-4b-it-GGUF'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 8192
        capabilities: [function_calling, json_mode]
      'Qwen3-4B-Instruct-2507-GGUF':
        litellm_id: 'lemonade/Qwen3-4B-Instruct-2507-GGUF'
        mode: chat
        max_input_tokens: 262144
        max_output_tokens: 32768
        capabilities: [function_calling, json_mode]
      'Qwen3-Coder-30B-A3B-Instruct-GGUF':
        litellm_id: 'lemonade/Qwen3-Coder-30B-A3B-Instruct-GGUF'
        mode: chat
        max_input_tokens: 262144
        max_output_tokens: 32768
        capabilities: [function_calling, json_mode]
      'gpt-oss-120b-mxfp-GGUF':
        litellm_id: 'lemonade/gpt-oss-120b-mxfp-GGUF'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 32768
        capabilities: [function_calling, json_mode]
      'gpt-oss-20b-mxfp4-GGUF':
        litellm_id: 'lemonade/gpt-oss-20b-mxfp4-GGUF'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 32768
        capabilities: [function_calling, json_mode]
  linkup:
    model_count: 2
    models:
      'search':
        litellm_id: 'linkup/search'
        mode: search
      'search-deep':
        litellm_id: 'linkup/search-deep'
        mode: search
  meta_llama:
    model_count: 4
    models:
      'Llama-3.3-70B-Instruct':
        litellm_id: 'meta_llama/Llama-3.3-70B-Instruct'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 4028
        capabilities: [function_calling]
      'Llama-3.3-8B-Instruct':
        litellm_id: 'meta_llama/Llama-3.3-8B-Instruct'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 4028
        capabilities: [function_calling]
      'Llama-4-Maverick-17B-128E-Instruct-FP8':
        litellm_id: 'meta_llama/Llama-4-Maverick-17B-128E-Instruct-FP8'
        mode: chat
        max_input_tokens: 1000000
        max_output_tokens: 4028
        capabilities: [function_calling]
      'Llama-4-Scout-17B-16E-Instruct-FP8':
        litellm_id: 'meta_llama/Llama-4-Scout-17B-16E-Instruct-FP8'
        mode: chat
        max_input_tokens: 10000000
        max_output_tokens: 4028
        capabilities: [function_calling]
  minimax:
    model_count: 7
    models:
      'MiniMax-M2':
        litellm_id: 'minimax/MiniMax-M2'
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 8192
        input_cost_per_1k: 0.0003
        output_cost_per_1k: 0.0012
        capabilities: [function_calling, system_messages, prompt_caching]
      'MiniMax-M2.1':
        litellm_id: 'minimax/MiniMax-M2.1'
        mode: chat
        max_input_tokens: 1000000
        max_output_tokens: 8192
        input_cost_per_1k: 0.0003
        output_cost_per_1k: 0.0012
        capabilities: [function_calling, system_messages, prompt_caching]
      'MiniMax-M2.1-lightning':
        litellm_id: 'minimax/MiniMax-M2.1-lightning'
        mode: chat
        max_input_tokens: 1000000
        max_output_tokens: 8192
        input_cost_per_1k: 0.0003
        output_cost_per_1k: 0.0024
        capabilities: [function_calling, system_messages, prompt_caching]
      'speech-02-hd':
        litellm_id: 'minimax/speech-02-hd'
        mode: audio_speech
      'speech-02-turbo':
        litellm_id: 'minimax/speech-02-turbo'
        mode: audio_speech
      'speech-2.6-hd':
        litellm_id: 'minimax/speech-2.6-hd'
        mode: audio_speech
      'speech-2.6-turbo':
        litellm_id: 'minimax/speech-2.6-turbo'
        mode: audio_speech
  mistral:
    model_count: 40
    models:
      'codestral-2405':
        litellm_id: 'mistral/codestral-2405'
        mode: chat
        max_input_tokens: 32000
        max_output_tokens: 8191
        input_cost_per_1k: 0.001
        output_cost_per_1k: 0.003
        capabilities: [json_mode]
      'codestral-2508':
        litellm_id: 'mistral/codestral-2508'
        mode: chat
        max_input_tokens: 256000
        max_output_tokens: 256000
        input_cost_per_1k: 0.0003
        output_cost_per_1k: 0.0009
        capabilities: [function_calling, json_mode]
      'codestral-embed':
        litellm_id: 'mistral/codestral-embed'
        mode: embedding
        max_input_tokens: 8192
        input_cost_per_1k: 0.00015
      'codestral-embed-2505':
        litellm_id: 'mistral/codestral-embed-2505'
        mode: embedding
        max_input_tokens: 8192
        input_cost_per_1k: 0.00015
      'codestral-latest':
        litellm_id: 'mistral/codestral-latest'
        mode: chat
        max_input_tokens: 32000
        max_output_tokens: 8191
        input_cost_per_1k: 0.001
        output_cost_per_1k: 0.003
        capabilities: [json_mode]
      'codestral-mamba-latest':
        litellm_id: 'mistral/codestral-mamba-latest'
        mode: chat
        max_input_tokens: 256000
        max_output_tokens: 256000
        input_cost_per_1k: 0.00025
        output_cost_per_1k: 0.00025
      'devstral-2512':
        litellm_id: 'mistral/devstral-2512'
        mode: chat
        max_input_tokens: 256000
        max_output_tokens: 256000
        input_cost_per_1k: 0.0004
        output_cost_per_1k: 0.002
        capabilities: [function_calling, json_mode]
      'devstral-medium-2507':
        litellm_id: 'mistral/devstral-medium-2507'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 128000
        input_cost_per_1k: 0.0004
        output_cost_per_1k: 0.002
        capabilities: [function_calling, json_mode]
      'devstral-small-2505':
        litellm_id: 'mistral/devstral-small-2505'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 128000
        input_cost_per_1k: 0.0001
        output_cost_per_1k: 0.0003
        capabilities: [function_calling, json_mode]
      'devstral-small-2507':
        litellm_id: 'mistral/devstral-small-2507'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 128000
        input_cost_per_1k: 0.0001
        output_cost_per_1k: 0.0003
        capabilities: [function_calling, json_mode]
      'labs-devstral-small-2512':
        litellm_id: 'mistral/labs-devstral-small-2512'
        mode: chat
        max_input_tokens: 256000
        max_output_tokens: 256000
        input_cost_per_1k: 0.0001
        output_cost_per_1k: 0.0003
        capabilities: [function_calling, json_mode]
      'magistral-medium-2506':
        litellm_id: 'mistral/magistral-medium-2506'
        mode: chat
        max_input_tokens: 40000
        max_output_tokens: 40000
        input_cost_per_1k: 0.002
        output_cost_per_1k: 0.005
        capabilities: [function_calling, json_mode, reasoning]
      'magistral-medium-2509':
        litellm_id: 'mistral/magistral-medium-2509'
        mode: chat
        max_input_tokens: 40000
        max_output_tokens: 40000
        input_cost_per_1k: 0.002
        output_cost_per_1k: 0.005
        capabilities: [function_calling, json_mode, reasoning]
      'magistral-medium-latest':
        litellm_id: 'mistral/magistral-medium-latest'
        mode: chat
        max_input_tokens: 40000
        max_output_tokens: 40000
        input_cost_per_1k: 0.002
        output_cost_per_1k: 0.005
        capabilities: [function_calling, json_mode, reasoning]
      'magistral-small-2506':
        litellm_id: 'mistral/magistral-small-2506'
        mode: chat
        max_input_tokens: 40000
        max_output_tokens: 40000
        input_cost_per_1k: 0.0005
        output_cost_per_1k: 0.0015
        capabilities: [function_calling, json_mode, reasoning]
      'magistral-small-latest':
        litellm_id: 'mistral/magistral-small-latest'
        mode: chat
        max_input_tokens: 40000
        max_output_tokens: 40000
        input_cost_per_1k: 0.0005
        output_cost_per_1k: 0.0015
        capabilities: [function_calling, json_mode, reasoning]
      'mistral-embed':
        litellm_id: 'mistral/mistral-embed'
        mode: embedding
        max_input_tokens: 8192
        input_cost_per_1k: 0.0001
      'mistral-large-2402':
        litellm_id: 'mistral/mistral-large-2402'
        mode: chat
        max_input_tokens: 32000
        max_output_tokens: 8191
        input_cost_per_1k: 0.004
        output_cost_per_1k: 0.012
        capabilities: [function_calling, json_mode]
      'mistral-large-2407':
        litellm_id: 'mistral/mistral-large-2407'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 128000
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.009
        capabilities: [function_calling, json_mode]
      'mistral-large-2411':
        litellm_id: 'mistral/mistral-large-2411'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 128000
        input_cost_per_1k: 0.002
        output_cost_per_1k: 0.006
        capabilities: [function_calling, json_mode]
      'mistral-large-3':
        litellm_id: 'mistral/mistral-large-3'
        mode: chat
        max_input_tokens: 256000
        max_output_tokens: 8191
        input_cost_per_1k: 0.0005
        output_cost_per_1k: 0.0015
        capabilities: [vision, function_calling, json_mode]
      'mistral-large-latest':
        litellm_id: 'mistral/mistral-large-latest'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 128000
        input_cost_per_1k: 0.002
        output_cost_per_1k: 0.006
        capabilities: [function_calling, json_mode]
      'mistral-medium':
        litellm_id: 'mistral/mistral-medium'
        mode: chat
        max_input_tokens: 32000
        max_output_tokens: 8191
        input_cost_per_1k: 0.0027
        output_cost_per_1k: 0.0081
        capabilities: [json_mode]
      'mistral-medium-2312':
        litellm_id: 'mistral/mistral-medium-2312'
        mode: chat
        max_input_tokens: 32000
        max_output_tokens: 8191
        input_cost_per_1k: 0.0027
        output_cost_per_1k: 0.0081
        capabilities: [json_mode]
      'mistral-medium-2505':
        litellm_id: 'mistral/mistral-medium-2505'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 8191
        input_cost_per_1k: 0.0004
        output_cost_per_1k: 0.002
        capabilities: [function_calling, json_mode]
      'mistral-medium-latest':
        litellm_id: 'mistral/mistral-medium-latest'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 8191
        input_cost_per_1k: 0.0004
        output_cost_per_1k: 0.002
        capabilities: [function_calling, json_mode]
      'mistral-ocr-2505-completion':
        litellm_id: 'mistral/mistral-ocr-2505-completion'
        mode: ocr
      'mistral-ocr-latest':
        litellm_id: 'mistral/mistral-ocr-latest'
        mode: ocr
      'mistral-small':
        litellm_id: 'mistral/mistral-small'
        mode: chat
        max_input_tokens: 32000
        max_output_tokens: 8191
        input_cost_per_1k: 0.0001
        output_cost_per_1k: 0.0003
        capabilities: [function_calling, json_mode]
      'mistral-small-latest':
        litellm_id: 'mistral/mistral-small-latest'
        mode: chat
        max_input_tokens: 32000
        max_output_tokens: 8191
        input_cost_per_1k: 0.0001
        output_cost_per_1k: 0.0003
        capabilities: [function_calling, json_mode]
      'mistral-tiny':
        litellm_id: 'mistral/mistral-tiny'
        mode: chat
        max_input_tokens: 32000
        max_output_tokens: 8191
        input_cost_per_1k: 0.00025
        output_cost_per_1k: 0.00025
        capabilities: [json_mode]
      'open-codestral-mamba':
        litellm_id: 'mistral/open-codestral-mamba'
        mode: chat
        max_input_tokens: 256000
        max_output_tokens: 256000
        input_cost_per_1k: 0.00025
        output_cost_per_1k: 0.00025
      'open-mistral-7b':
        litellm_id: 'mistral/open-mistral-7b'
        mode: chat
        max_input_tokens: 32000
        max_output_tokens: 8191
        input_cost_per_1k: 0.00025
        output_cost_per_1k: 0.00025
        capabilities: [json_mode]
      'open-mistral-nemo':
        litellm_id: 'mistral/open-mistral-nemo'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 128000
        input_cost_per_1k: 0.0003
        output_cost_per_1k: 0.0003
        capabilities: [json_mode]
      'open-mistral-nemo-2407':
        litellm_id: 'mistral/open-mistral-nemo-2407'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 128000
        input_cost_per_1k: 0.0003
        output_cost_per_1k: 0.0003
        capabilities: [json_mode]
      'open-mixtral-8x22b':
        litellm_id: 'mistral/open-mixtral-8x22b'
        mode: chat
        max_input_tokens: 65336
        max_output_tokens: 8191
        input_cost_per_1k: 0.002
        output_cost_per_1k: 0.006
        capabilities: [function_calling, json_mode]
      'open-mixtral-8x7b':
        litellm_id: 'mistral/open-mixtral-8x7b'
        mode: chat
        max_input_tokens: 32000
        max_output_tokens: 8191
        input_cost_per_1k: 0.0007
        output_cost_per_1k: 0.0007
        capabilities: [function_calling, json_mode]
      'pixtral-12b-2409':
        litellm_id: 'mistral/pixtral-12b-2409'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 128000
        input_cost_per_1k: 0.00015
        output_cost_per_1k: 0.00015
        capabilities: [vision, function_calling, json_mode]
      'pixtral-large-2411':
        litellm_id: 'mistral/pixtral-large-2411'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 128000
        input_cost_per_1k: 0.002
        output_cost_per_1k: 0.006
        capabilities: [vision, function_calling, json_mode]
      'pixtral-large-latest':
        litellm_id: 'mistral/pixtral-large-latest'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 128000
        input_cost_per_1k: 0.002
        output_cost_per_1k: 0.006
        capabilities: [vision, function_calling, json_mode]
  moonshot:
    model_count: 20
    models:
      'kimi-k2-0711-preview':
        litellm_id: 'moonshot/kimi-k2-0711-preview'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.0006
        output_cost_per_1k: 0.0025
        capabilities: [function_calling, web_search]
      'kimi-k2-0905-preview':
        litellm_id: 'moonshot/kimi-k2-0905-preview'
        mode: chat
        max_input_tokens: 262144
        max_output_tokens: 262144
        input_cost_per_1k: 0.0006
        output_cost_per_1k: 0.0025
        capabilities: [function_calling, web_search]
      'kimi-k2-thinking':
        litellm_id: 'moonshot/kimi-k2-thinking'
        mode: chat
        max_input_tokens: 262144
        max_output_tokens: 262144
        input_cost_per_1k: 0.0006
        output_cost_per_1k: 0.0025
        capabilities: [function_calling, web_search]
      'kimi-k2-thinking-turbo':
        litellm_id: 'moonshot/kimi-k2-thinking-turbo'
        mode: chat
        max_input_tokens: 262144
        max_output_tokens: 262144
        input_cost_per_1k: 0.00115
        output_cost_per_1k: 0.008
        capabilities: [function_calling, web_search]
      'kimi-k2-turbo-preview':
        litellm_id: 'moonshot/kimi-k2-turbo-preview'
        mode: chat
        max_input_tokens: 262144
        max_output_tokens: 262144
        input_cost_per_1k: 0.00115
        output_cost_per_1k: 0.008
        capabilities: [function_calling, web_search]
      'kimi-latest':
        litellm_id: 'moonshot/kimi-latest'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.002
        output_cost_per_1k: 0.005
        capabilities: [vision, function_calling]
      'kimi-latest-128k':
        litellm_id: 'moonshot/kimi-latest-128k'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.002
        output_cost_per_1k: 0.005
        capabilities: [vision, function_calling]
      'kimi-latest-32k':
        litellm_id: 'moonshot/kimi-latest-32k'
        mode: chat
        max_input_tokens: 32768
        max_output_tokens: 32768
        input_cost_per_1k: 0.001
        output_cost_per_1k: 0.003
        capabilities: [vision, function_calling]
      'kimi-latest-8k':
        litellm_id: 'moonshot/kimi-latest-8k'
        mode: chat
        max_input_tokens: 8192
        max_output_tokens: 8192
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.002
        capabilities: [vision, function_calling]
      'kimi-thinking-preview':
        litellm_id: 'moonshot/kimi-thinking-preview'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.0006
        output_cost_per_1k: 0.0025
        capabilities: [vision]
      'moonshot-v1-128k':
        litellm_id: 'moonshot/moonshot-v1-128k'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.002
        output_cost_per_1k: 0.005
        capabilities: [function_calling]
      'moonshot-v1-128k-0430':
        litellm_id: 'moonshot/moonshot-v1-128k-0430'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.002
        output_cost_per_1k: 0.005
        capabilities: [function_calling]
      'moonshot-v1-128k-vision-preview':
        litellm_id: 'moonshot/moonshot-v1-128k-vision-preview'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.002
        output_cost_per_1k: 0.005
        capabilities: [vision, function_calling]
      'moonshot-v1-32k':
        litellm_id: 'moonshot/moonshot-v1-32k'
        mode: chat
        max_input_tokens: 32768
        max_output_tokens: 32768
        input_cost_per_1k: 0.001
        output_cost_per_1k: 0.003
        capabilities: [function_calling]
      'moonshot-v1-32k-0430':
        litellm_id: 'moonshot/moonshot-v1-32k-0430'
        mode: chat
        max_input_tokens: 32768
        max_output_tokens: 32768
        input_cost_per_1k: 0.001
        output_cost_per_1k: 0.003
        capabilities: [function_calling]
      'moonshot-v1-32k-vision-preview':
        litellm_id: 'moonshot/moonshot-v1-32k-vision-preview'
        mode: chat
        max_input_tokens: 32768
        max_output_tokens: 32768
        input_cost_per_1k: 0.001
        output_cost_per_1k: 0.003
        capabilities: [vision, function_calling]
      'moonshot-v1-8k':
        litellm_id: 'moonshot/moonshot-v1-8k'
        mode: chat
        max_input_tokens: 8192
        max_output_tokens: 8192
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.002
        capabilities: [function_calling]
      'moonshot-v1-8k-0430':
        litellm_id: 'moonshot/moonshot-v1-8k-0430'
        mode: chat
        max_input_tokens: 8192
        max_output_tokens: 8192
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.002
        capabilities: [function_calling]
      'moonshot-v1-8k-vision-preview':
        litellm_id: 'moonshot/moonshot-v1-8k-vision-preview'
        mode: chat
        max_input_tokens: 8192
        max_output_tokens: 8192
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.002
        capabilities: [vision, function_calling]
      'moonshot-v1-auto':
        litellm_id: 'moonshot/moonshot-v1-auto'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.002
        output_cost_per_1k: 0.005
        capabilities: [function_calling]
  morph:
    model_count: 2
    models:
      'morph-v3-fast':
        litellm_id: 'morph/morph-v3-fast'
        mode: chat
        max_input_tokens: 16000
        max_output_tokens: 16000
        input_cost_per_1k: 0.0008
        output_cost_per_1k: 0.0012
        capabilities: [system_messages]
      'morph-v3-large':
        litellm_id: 'morph/morph-v3-large'
        mode: chat
        max_input_tokens: 16000
        max_output_tokens: 16000
        input_cost_per_1k: 0.0009
        output_cost_per_1k: 0.0019
        capabilities: [system_messages]
  nlp_cloud:
    model_count: 2
    models:
      'chatdolphin':
        mode: chat
        max_input_tokens: 16384
        max_output_tokens: 16384
        input_cost_per_1k: 0.0005
        output_cost_per_1k: 0.0005
      'dolphin':
        mode: completion
        max_input_tokens: 16384
        max_output_tokens: 16384
        input_cost_per_1k: 0.0005
        output_cost_per_1k: 0.0005
  nscale:
    model_count: 16
    models:
      'Qwen/QwQ-32B':
        litellm_id: 'nscale/Qwen/QwQ-32B'
        mode: chat
        input_cost_per_1k: 0.00018
        output_cost_per_1k: 0.0002
      'Qwen/Qwen2.5-Coder-32B-Instruct':
        litellm_id: 'nscale/Qwen/Qwen2.5-Coder-32B-Instruct'
        mode: chat
        input_cost_per_1k: 6e-05
        output_cost_per_1k: 0.0002
      'Qwen/Qwen2.5-Coder-3B-Instruct':
        litellm_id: 'nscale/Qwen/Qwen2.5-Coder-3B-Instruct'
        mode: chat
        input_cost_per_1k: 1e-05
        output_cost_per_1k: 3e-05
      'Qwen/Qwen2.5-Coder-7B-Instruct':
        litellm_id: 'nscale/Qwen/Qwen2.5-Coder-7B-Instruct'
        mode: chat
        input_cost_per_1k: 1e-05
        output_cost_per_1k: 3e-05
      'black-forest-labs/FLUX.1-schnell':
        litellm_id: 'nscale/black-forest-labs/FLUX.1-schnell'
        mode: image
      'deepseek-ai/DeepSeek-R1-Distill-Llama-70B':
        litellm_id: 'nscale/deepseek-ai/DeepSeek-R1-Distill-Llama-70B'
        mode: chat
        input_cost_per_1k: 0.000375
        output_cost_per_1k: 0.000375
      'deepseek-ai/DeepSeek-R1-Distill-Llama-8B':
        litellm_id: 'nscale/deepseek-ai/DeepSeek-R1-Distill-Llama-8B'
        mode: chat
        input_cost_per_1k: 2.5e-05
        output_cost_per_1k: 2.5e-05
      'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B':
        litellm_id: 'nscale/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B'
        mode: chat
        input_cost_per_1k: 9e-05
        output_cost_per_1k: 9e-05
      'deepseek-ai/DeepSeek-R1-Distill-Qwen-14B':
        litellm_id: 'nscale/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B'
        mode: chat
        input_cost_per_1k: 7e-05
        output_cost_per_1k: 7e-05
      'deepseek-ai/DeepSeek-R1-Distill-Qwen-32B':
        litellm_id: 'nscale/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B'
        mode: chat
        input_cost_per_1k: 0.00015
        output_cost_per_1k: 0.00015
      'deepseek-ai/DeepSeek-R1-Distill-Qwen-7B':
        litellm_id: 'nscale/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B'
        mode: chat
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
      'meta-llama/Llama-3.1-8B-Instruct':
        litellm_id: 'nscale/meta-llama/Llama-3.1-8B-Instruct'
        mode: chat
        input_cost_per_1k: 3e-05
        output_cost_per_1k: 3e-05
      'meta-llama/Llama-3.3-70B-Instruct':
        litellm_id: 'nscale/meta-llama/Llama-3.3-70B-Instruct'
        mode: chat
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
      'meta-llama/Llama-4-Scout-17B-16E-Instruct':
        litellm_id: 'nscale/meta-llama/Llama-4-Scout-17B-16E-Instruct'
        mode: chat
        input_cost_per_1k: 9e-05
        output_cost_per_1k: 0.00029
      'mistralai/mixtral-8x22b-instruct-v0.1':
        litellm_id: 'nscale/mistralai/mixtral-8x22b-instruct-v0.1'
        mode: chat
        input_cost_per_1k: 0.0006
        output_cost_per_1k: 0.0006
      'stabilityai/stable-diffusion-xl-base-1.0':
        litellm_id: 'nscale/stabilityai/stable-diffusion-xl-base-1.0'
        mode: image
  nvidia_nim:
    model_count: 3
    models:
      'nvidia/llama-3_2-nv-rerankqa-1b-v2':
        litellm_id: 'nvidia_nim/nvidia/llama-3_2-nv-rerankqa-1b-v2'
        mode: rerank
      'nvidia/nv-rerankqa-mistral-4b-v3':
        litellm_id: 'nvidia_nim/nvidia/nv-rerankqa-mistral-4b-v3'
        mode: rerank
      'ranking/nvidia/llama-3.2-nv-rerankqa-1b-v2':
        litellm_id: 'nvidia_nim/ranking/nvidia/llama-3.2-nv-rerankqa-1b-v2'
        mode: rerank
  oci:
    model_count: 13
    models:
      'cohere.command-a-03-2025':
        litellm_id: 'oci/cohere.command-a-03-2025'
        mode: chat
        max_input_tokens: 256000
        max_output_tokens: 4000
        input_cost_per_1k: 0.00156
        output_cost_per_1k: 0.00156
        capabilities: [function_calling]
      'cohere.command-latest':
        litellm_id: 'oci/cohere.command-latest'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 4000
        input_cost_per_1k: 0.00156
        output_cost_per_1k: 0.00156
        capabilities: [function_calling]
      'cohere.command-plus-latest':
        litellm_id: 'oci/cohere.command-plus-latest'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 4000
        input_cost_per_1k: 0.00156
        output_cost_per_1k: 0.00156
        capabilities: [function_calling]
      'meta.llama-3.1-405b-instruct':
        litellm_id: 'oci/meta.llama-3.1-405b-instruct'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 4000
        input_cost_per_1k: 0.01068
        output_cost_per_1k: 0.01068
        capabilities: [function_calling]
      'meta.llama-3.2-90b-vision-instruct':
        litellm_id: 'oci/meta.llama-3.2-90b-vision-instruct'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 4000
        input_cost_per_1k: 0.002
        output_cost_per_1k: 0.002
        capabilities: [function_calling]
      'meta.llama-3.3-70b-instruct':
        litellm_id: 'oci/meta.llama-3.3-70b-instruct'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 4000
        input_cost_per_1k: 0.00072
        output_cost_per_1k: 0.00072
        capabilities: [function_calling]
      'meta.llama-4-maverick-17b-128e-instruct-fp8':
        litellm_id: 'oci/meta.llama-4-maverick-17b-128e-instruct-fp8'
        mode: chat
        max_input_tokens: 512000
        max_output_tokens: 4000
        input_cost_per_1k: 0.00072
        output_cost_per_1k: 0.00072
        capabilities: [function_calling]
      'meta.llama-4-scout-17b-16e-instruct':
        litellm_id: 'oci/meta.llama-4-scout-17b-16e-instruct'
        mode: chat
        max_input_tokens: 192000
        max_output_tokens: 4000
        input_cost_per_1k: 0.00072
        output_cost_per_1k: 0.00072
        capabilities: [function_calling]
      'xai.grok-3':
        litellm_id: 'oci/xai.grok-3'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.00015
        capabilities: [function_calling]
      'xai.grok-3-fast':
        litellm_id: 'oci/xai.grok-3-fast'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.005
        output_cost_per_1k: 0.025
        capabilities: [function_calling]
      'xai.grok-3-mini':
        litellm_id: 'oci/xai.grok-3-mini'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.0003
        output_cost_per_1k: 0.0005
        capabilities: [function_calling]
      'xai.grok-3-mini-fast':
        litellm_id: 'oci/xai.grok-3-mini-fast'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.0006
        output_cost_per_1k: 0.004
        capabilities: [function_calling]
      'xai.grok-4':
        litellm_id: 'oci/xai.grok-4'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 128000
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.00015
        capabilities: [function_calling]
  ollama:
    model_count: 29
    models:
      'codegeex4':
        litellm_id: 'ollama/codegeex4'
        mode: chat
        max_input_tokens: 32768
        max_output_tokens: 8192
      'codegemma':
        litellm_id: 'ollama/codegemma'
        mode: completion
        max_input_tokens: 8192
        max_output_tokens: 8192
      'codellama':
        litellm_id: 'ollama/codellama'
        mode: completion
        max_input_tokens: 4096
        max_output_tokens: 4096
      'deepseek-coder-v2-base':
        litellm_id: 'ollama/deepseek-coder-v2-base'
        mode: completion
        max_input_tokens: 8192
        max_output_tokens: 8192
        capabilities: [function_calling]
      'deepseek-coder-v2-instruct':
        litellm_id: 'ollama/deepseek-coder-v2-instruct'
        mode: chat
        max_input_tokens: 32768
        max_output_tokens: 8192
        capabilities: [function_calling]
      'deepseek-coder-v2-lite-base':
        litellm_id: 'ollama/deepseek-coder-v2-lite-base'
        mode: completion
        max_input_tokens: 8192
        max_output_tokens: 8192
        capabilities: [function_calling]
      'deepseek-coder-v2-lite-instruct':
        litellm_id: 'ollama/deepseek-coder-v2-lite-instruct'
        mode: chat
        max_input_tokens: 32768
        max_output_tokens: 8192
        capabilities: [function_calling]
      'deepseek-v3.1:671b-cloud':
        litellm_id: 'ollama/deepseek-v3.1:671b-cloud'
        mode: chat
        max_input_tokens: 163840
        max_output_tokens: 163840
        capabilities: [function_calling]
      'gpt-oss:120b-cloud':
        litellm_id: 'ollama/gpt-oss:120b-cloud'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        capabilities: [function_calling]
      'gpt-oss:20b-cloud':
        litellm_id: 'ollama/gpt-oss:20b-cloud'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        capabilities: [function_calling]
      'internlm2_5-20b-chat':
        litellm_id: 'ollama/internlm2_5-20b-chat'
        mode: chat
        max_input_tokens: 32768
        max_output_tokens: 8192
        capabilities: [function_calling]
      'llama2':
        litellm_id: 'ollama/llama2'
        mode: chat
        max_input_tokens: 4096
        max_output_tokens: 4096
      'llama2-uncensored':
        litellm_id: 'ollama/llama2-uncensored'
        mode: completion
        max_input_tokens: 4096
        max_output_tokens: 4096
      'llama2:13b':
        litellm_id: 'ollama/llama2:13b'
        mode: chat
        max_input_tokens: 4096
        max_output_tokens: 4096
      'llama2:70b':
        litellm_id: 'ollama/llama2:70b'
        mode: chat
        max_input_tokens: 4096
        max_output_tokens: 4096
      'llama2:7b':
        litellm_id: 'ollama/llama2:7b'
        mode: chat
        max_input_tokens: 4096
        max_output_tokens: 4096
      'llama3':
        litellm_id: 'ollama/llama3'
        mode: chat
        max_input_tokens: 8192
        max_output_tokens: 8192
      'llama3.1':
        litellm_id: 'ollama/llama3.1'
        mode: chat
        max_input_tokens: 8192
        max_output_tokens: 8192
        capabilities: [function_calling]
      'llama3:70b':
        litellm_id: 'ollama/llama3:70b'
        mode: chat
        max_input_tokens: 8192
        max_output_tokens: 8192
      'llama3:8b':
        litellm_id: 'ollama/llama3:8b'
        mode: chat
        max_input_tokens: 8192
        max_output_tokens: 8192
      'mistral':
        litellm_id: 'ollama/mistral'
        mode: completion
        max_input_tokens: 8192
        max_output_tokens: 8192
        capabilities: [function_calling]
      'mistral-7B-Instruct-v0.1':
        litellm_id: 'ollama/mistral-7B-Instruct-v0.1'
        mode: chat
        max_input_tokens: 8192
        max_output_tokens: 8192
        capabilities: [function_calling]
      'mistral-7B-Instruct-v0.2':
        litellm_id: 'ollama/mistral-7B-Instruct-v0.2'
        mode: chat
        max_input_tokens: 32768
        max_output_tokens: 32768
        capabilities: [function_calling]
      'mistral-large-instruct-2407':
        litellm_id: 'ollama/mistral-large-instruct-2407'
        mode: chat
        max_input_tokens: 65536
        max_output_tokens: 8192
        capabilities: [function_calling]
      'mixtral-8x22B-Instruct-v0.1':
        litellm_id: 'ollama/mixtral-8x22B-Instruct-v0.1'
        mode: chat
        max_input_tokens: 65536
        max_output_tokens: 65536
        capabilities: [function_calling]
      'mixtral-8x7B-Instruct-v0.1':
        litellm_id: 'ollama/mixtral-8x7B-Instruct-v0.1'
        mode: chat
        max_input_tokens: 32768
        max_output_tokens: 32768
        capabilities: [function_calling]
      'orca-mini':
        litellm_id: 'ollama/orca-mini'
        mode: completion
        max_input_tokens: 4096
        max_output_tokens: 4096
      'qwen3-coder:480b-cloud':
        litellm_id: 'ollama/qwen3-coder:480b-cloud'
        mode: chat
        max_input_tokens: 262144
        max_output_tokens: 262144
        capabilities: [function_calling]
      'vicuna':
        litellm_id: 'ollama/vicuna'
        mode: completion
        max_input_tokens: 2048
        max_output_tokens: 2048
  openai:
    model_count: 154
    models:
      'chatgpt-4o-latest':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 4096
        input_cost_per_1k: 0.005
        output_cost_per_1k: 0.015
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, prompt_caching]
      'codex-mini-latest':
        mode: responses
        max_input_tokens: 200000
        max_output_tokens: 100000
        input_cost_per_1k: 0.0015
        output_cost_per_1k: 0.006
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching, reasoning]
      'container':
        litellm_id: 'openai/container'
        mode: chat
      'ft:gpt-3.5-turbo':
        mode: chat
        max_input_tokens: 16385
        max_output_tokens: 4096
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.006
        capabilities: [system_messages]
      'ft:gpt-3.5-turbo-0125':
        mode: chat
        max_input_tokens: 16385
        max_output_tokens: 4096
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.006
        capabilities: [system_messages]
      'ft:gpt-3.5-turbo-0613':
        mode: chat
        max_input_tokens: 4096
        max_output_tokens: 4096
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.006
        capabilities: [system_messages]
      'ft:gpt-3.5-turbo-1106':
        mode: chat
        max_input_tokens: 16385
        max_output_tokens: 4096
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.006
        capabilities: [system_messages]
      'ft:gpt-4-0613':
        mode: chat
        max_input_tokens: 8192
        max_output_tokens: 4096
        input_cost_per_1k: 0.03
        output_cost_per_1k: 0.06
        capabilities: [function_calling, system_messages]
      'ft:gpt-4.1-2025-04-14':
        mode: chat
        max_input_tokens: 1047576
        max_output_tokens: 32768
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.012
        capabilities: [function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching]
      'ft:gpt-4.1-mini-2025-04-14':
        mode: chat
        max_input_tokens: 1047576
        max_output_tokens: 32768
        input_cost_per_1k: 0.0008
        output_cost_per_1k: 0.0032
        capabilities: [function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching]
      'ft:gpt-4.1-nano-2025-04-14':
        mode: chat
        max_input_tokens: 1047576
        max_output_tokens: 32768
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0008
        capabilities: [function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching]
      'ft:gpt-4o-2024-08-06':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 16384
        input_cost_per_1k: 0.00375
        output_cost_per_1k: 0.015
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching]
      'ft:gpt-4o-2024-11-20':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 16384
        input_cost_per_1k: 0.00375
        output_cost_per_1k: 0.015
        capabilities: [function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching]
      'ft:gpt-4o-mini-2024-07-18':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 16384
        input_cost_per_1k: 0.0003
        output_cost_per_1k: 0.0012
        capabilities: [function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching]
      'ft:o4-mini-2025-04-16':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 100000
        input_cost_per_1k: 0.004
        output_cost_per_1k: 0.016
        capabilities: [function_calling, json_mode, prompt_caching, reasoning]
      'gpt-3.5-turbo':
        mode: chat
        max_input_tokens: 16385
        max_output_tokens: 4096
        input_cost_per_1k: 0.0005
        output_cost_per_1k: 0.0015
        capabilities: [function_calling, system_messages, prompt_caching]
      'gpt-3.5-turbo-0125':
        mode: chat
        max_input_tokens: 16385
        max_output_tokens: 4096
        input_cost_per_1k: 0.0005
        output_cost_per_1k: 0.0015
        capabilities: [function_calling, parallel_function_calling, system_messages, prompt_caching]
      'gpt-3.5-turbo-0301':
        mode: chat
        max_input_tokens: 4097
        max_output_tokens: 4096
        input_cost_per_1k: 0.0015
        output_cost_per_1k: 0.002
        capabilities: [system_messages, prompt_caching]
      'gpt-3.5-turbo-0613':
        mode: chat
        max_input_tokens: 4097
        max_output_tokens: 4096
        input_cost_per_1k: 0.0015
        output_cost_per_1k: 0.002
        capabilities: [function_calling, system_messages, prompt_caching]
      'gpt-3.5-turbo-1106':
        mode: chat
        max_input_tokens: 16385
        max_output_tokens: 4096
        input_cost_per_1k: 0.001
        output_cost_per_1k: 0.002
        capabilities: [function_calling, parallel_function_calling, system_messages, prompt_caching]
        deprecated: true
        deprecation_date: '2026-09-28'
      'gpt-3.5-turbo-16k':
        mode: chat
        max_input_tokens: 16385
        max_output_tokens: 4096
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.004
        capabilities: [system_messages, prompt_caching]
      'gpt-3.5-turbo-16k-0613':
        mode: chat
        max_input_tokens: 16385
        max_output_tokens: 4096
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.004
        capabilities: [system_messages, prompt_caching]
      'gpt-4':
        mode: chat
        max_input_tokens: 8192
        max_output_tokens: 4096
        input_cost_per_1k: 0.03
        output_cost_per_1k: 0.06
        capabilities: [function_calling, system_messages, prompt_caching]
      'gpt-4-0125-preview':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 4096
        input_cost_per_1k: 0.01
        output_cost_per_1k: 0.03
        capabilities: [function_calling, parallel_function_calling, system_messages, prompt_caching]
        deprecated: true
        deprecation_date: '2026-03-26'
      'gpt-4-0314':
        mode: chat
        max_input_tokens: 8192
        max_output_tokens: 4096
        input_cost_per_1k: 0.03
        output_cost_per_1k: 0.06
        capabilities: [system_messages, prompt_caching]
      'gpt-4-0613':
        mode: chat
        max_input_tokens: 8192
        max_output_tokens: 4096
        input_cost_per_1k: 0.03
        output_cost_per_1k: 0.06
        capabilities: [function_calling, system_messages, prompt_caching]
        deprecated: true
        deprecation_date: '2025-06-06'
      'gpt-4-1106-preview':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 4096
        input_cost_per_1k: 0.01
        output_cost_per_1k: 0.03
        capabilities: [function_calling, parallel_function_calling, system_messages, prompt_caching]
        deprecated: true
        deprecation_date: '2026-03-26'
      'gpt-4-1106-vision-preview':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 4096
        input_cost_per_1k: 0.01
        output_cost_per_1k: 0.03
        capabilities: [vision, system_messages, prompt_caching]
        deprecated: true
        deprecation_date: '2024-12-06'
      'gpt-4-32k':
        mode: chat
        max_input_tokens: 32768
        max_output_tokens: 4096
        input_cost_per_1k: 0.06
        output_cost_per_1k: 0.12
        capabilities: [system_messages, prompt_caching]
      'gpt-4-32k-0314':
        mode: chat
        max_input_tokens: 32768
        max_output_tokens: 4096
        input_cost_per_1k: 0.06
        output_cost_per_1k: 0.12
        capabilities: [system_messages, prompt_caching]
      'gpt-4-32k-0613':
        mode: chat
        max_input_tokens: 32768
        max_output_tokens: 4096
        input_cost_per_1k: 0.06
        output_cost_per_1k: 0.12
        capabilities: [system_messages, prompt_caching]
      'gpt-4-turbo':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 4096
        input_cost_per_1k: 0.01
        output_cost_per_1k: 0.03
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, prompt_caching]
      'gpt-4-turbo-2024-04-09':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 4096
        input_cost_per_1k: 0.01
        output_cost_per_1k: 0.03
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, prompt_caching]
      'gpt-4-turbo-preview':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 4096
        input_cost_per_1k: 0.01
        output_cost_per_1k: 0.03
        capabilities: [function_calling, parallel_function_calling, system_messages, prompt_caching]
      'gpt-4-vision-preview':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 4096
        input_cost_per_1k: 0.01
        output_cost_per_1k: 0.03
        capabilities: [vision, system_messages, prompt_caching]
        deprecated: true
        deprecation_date: '2024-12-06'
      'gpt-4.1':
        mode: chat
        max_input_tokens: 1047576
        max_output_tokens: 32768
        input_cost_per_1k: 0.002
        output_cost_per_1k: 0.008
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching]
      'gpt-4.1-2025-04-14':
        mode: chat
        max_input_tokens: 1047576
        max_output_tokens: 32768
        input_cost_per_1k: 0.002
        output_cost_per_1k: 0.008
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching]
      'gpt-4.1-mini':
        mode: chat
        max_input_tokens: 1047576
        max_output_tokens: 32768
        input_cost_per_1k: 0.0004
        output_cost_per_1k: 0.0016
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching]
      'gpt-4.1-mini-2025-04-14':
        mode: chat
        max_input_tokens: 1047576
        max_output_tokens: 32768
        input_cost_per_1k: 0.0004
        output_cost_per_1k: 0.0016
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching]
      'gpt-4.1-nano':
        mode: chat
        max_input_tokens: 1047576
        max_output_tokens: 32768
        input_cost_per_1k: 0.0001
        output_cost_per_1k: 0.0004
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching]
      'gpt-4.1-nano-2025-04-14':
        mode: chat
        max_input_tokens: 1047576
        max_output_tokens: 32768
        input_cost_per_1k: 0.0001
        output_cost_per_1k: 0.0004
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching]
      'gpt-4.5-preview':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 16384
        input_cost_per_1k: 0.075
        output_cost_per_1k: 0.15
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching]
      'gpt-4.5-preview-2025-02-27':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 16384
        input_cost_per_1k: 0.075
        output_cost_per_1k: 0.15
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching]
        deprecated: true
        deprecation_date: '2025-07-14'
      'gpt-4o':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 16384
        input_cost_per_1k: 0.0025
        output_cost_per_1k: 0.01
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching]
      'gpt-4o-2024-05-13':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 4096
        input_cost_per_1k: 0.005
        output_cost_per_1k: 0.015
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, prompt_caching]
      'gpt-4o-2024-08-06':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 16384
        input_cost_per_1k: 0.0025
        output_cost_per_1k: 0.01
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching]
      'gpt-4o-2024-11-20':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 16384
        input_cost_per_1k: 0.0025
        output_cost_per_1k: 0.01
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching]
      'gpt-4o-audio-preview':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 16384
        input_cost_per_1k: 0.0025
        output_cost_per_1k: 0.01
        capabilities: [function_calling, parallel_function_calling, system_messages, audio_input, audio_output]
      'gpt-4o-audio-preview-2024-10-01':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 16384
        input_cost_per_1k: 0.0025
        output_cost_per_1k: 0.01
        capabilities: [function_calling, parallel_function_calling, system_messages, audio_input, audio_output]
      'gpt-4o-audio-preview-2024-12-17':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 16384
        input_cost_per_1k: 0.0025
        output_cost_per_1k: 0.01
        capabilities: [function_calling, parallel_function_calling, system_messages, audio_input, audio_output]
      'gpt-4o-audio-preview-2025-06-03':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 16384
        input_cost_per_1k: 0.0025
        output_cost_per_1k: 0.01
        capabilities: [function_calling, parallel_function_calling, system_messages, audio_input, audio_output]
      'gpt-4o-mini':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 16384
        input_cost_per_1k: 0.00015
        output_cost_per_1k: 0.0006
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching]
      'gpt-4o-mini-2024-07-18':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 16384
        input_cost_per_1k: 0.00015
        output_cost_per_1k: 0.0006
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching]
      'gpt-4o-mini-audio-preview':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 16384
        input_cost_per_1k: 0.00015
        output_cost_per_1k: 0.0006
        capabilities: [function_calling, parallel_function_calling, system_messages, audio_input, audio_output]
      'gpt-4o-mini-audio-preview-2024-12-17':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 16384
        input_cost_per_1k: 0.00015
        output_cost_per_1k: 0.0006
        capabilities: [function_calling, parallel_function_calling, system_messages, audio_input, audio_output]
      'gpt-4o-mini-realtime-preview':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 4096
        input_cost_per_1k: 0.0006
        output_cost_per_1k: 0.0024
        capabilities: [function_calling, parallel_function_calling, system_messages, audio_input, audio_output]
      'gpt-4o-mini-realtime-preview-2024-12-17':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 4096
        input_cost_per_1k: 0.0006
        output_cost_per_1k: 0.0024
        capabilities: [function_calling, parallel_function_calling, system_messages, audio_input, audio_output]
      'gpt-4o-mini-search-preview':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 16384
        input_cost_per_1k: 0.00015
        output_cost_per_1k: 0.0006
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching, web_search]
      'gpt-4o-mini-search-preview-2025-03-11':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 16384
        input_cost_per_1k: 0.00015
        output_cost_per_1k: 0.0006
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching]
      'gpt-4o-mini-transcribe':
        mode: audio_transcription
        max_input_tokens: 16000
        max_output_tokens: 2000
        input_cost_per_1k: 0.00125
        output_cost_per_1k: 0.005
      'gpt-4o-mini-tts':
        mode: audio_speech
        input_cost_per_1k: 0.0025
        output_cost_per_1k: 0.01
      'gpt-4o-realtime-preview':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 4096
        input_cost_per_1k: 0.005
        output_cost_per_1k: 0.02
        capabilities: [function_calling, parallel_function_calling, system_messages, audio_input, audio_output]
      'gpt-4o-realtime-preview-2024-10-01':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 4096
        input_cost_per_1k: 0.005
        output_cost_per_1k: 0.02
        capabilities: [function_calling, parallel_function_calling, system_messages, audio_input, audio_output]
      'gpt-4o-realtime-preview-2024-12-17':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 4096
        input_cost_per_1k: 0.005
        output_cost_per_1k: 0.02
        capabilities: [function_calling, parallel_function_calling, system_messages, audio_input, audio_output]
      'gpt-4o-realtime-preview-2025-06-03':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 4096
        input_cost_per_1k: 0.005
        output_cost_per_1k: 0.02
        capabilities: [function_calling, parallel_function_calling, system_messages, audio_input, audio_output]
      'gpt-4o-search-preview':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 16384
        input_cost_per_1k: 0.0025
        output_cost_per_1k: 0.01
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching, web_search]
      'gpt-4o-search-preview-2025-03-11':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 16384
        input_cost_per_1k: 0.0025
        output_cost_per_1k: 0.01
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching]
      'gpt-4o-transcribe':
        mode: audio_transcription
        max_input_tokens: 16000
        max_output_tokens: 2000
        input_cost_per_1k: 0.0025
        output_cost_per_1k: 0.01
      'gpt-4o-transcribe-diarize':
        mode: audio_transcription
        max_input_tokens: 16000
        max_output_tokens: 2000
        input_cost_per_1k: 0.0025
        output_cost_per_1k: 0.01
      'gpt-5':
        mode: chat
        max_input_tokens: 272000
        max_output_tokens: 128000
        input_cost_per_1k: 0.00125
        output_cost_per_1k: 0.01
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching, reasoning]
      'gpt-5-2025-08-07':
        mode: chat
        max_input_tokens: 272000
        max_output_tokens: 128000
        input_cost_per_1k: 0.00125
        output_cost_per_1k: 0.01
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching, reasoning]
      'gpt-5-chat':
        mode: chat
        max_input_tokens: 272000
        max_output_tokens: 128000
        input_cost_per_1k: 0.00125
        output_cost_per_1k: 0.01
        capabilities: [vision, system_messages, json_mode, prompt_caching, reasoning]
      'gpt-5-chat-latest':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 16384
        input_cost_per_1k: 0.00125
        output_cost_per_1k: 0.01
        capabilities: [vision, system_messages, json_mode, prompt_caching, reasoning]
      'gpt-5-codex':
        mode: responses
        max_input_tokens: 272000
        max_output_tokens: 128000
        input_cost_per_1k: 0.00125
        output_cost_per_1k: 0.01
        capabilities: [vision, function_calling, parallel_function_calling, json_mode, prompt_caching, reasoning]
      'gpt-5-mini':
        mode: chat
        max_input_tokens: 272000
        max_output_tokens: 128000
        input_cost_per_1k: 0.00025
        output_cost_per_1k: 0.002
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching, reasoning]
      'gpt-5-mini-2025-08-07':
        mode: chat
        max_input_tokens: 272000
        max_output_tokens: 128000
        input_cost_per_1k: 0.00025
        output_cost_per_1k: 0.002
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching, reasoning]
      'gpt-5-nano':
        mode: chat
        max_input_tokens: 272000
        max_output_tokens: 128000
        input_cost_per_1k: 5e-05
        output_cost_per_1k: 0.0004
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching, reasoning]
      'gpt-5-nano-2025-08-07':
        mode: chat
        max_input_tokens: 272000
        max_output_tokens: 128000
        input_cost_per_1k: 5e-05
        output_cost_per_1k: 0.0004
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching, reasoning]
      'gpt-5-pro':
        mode: responses
        max_input_tokens: 400000
        max_output_tokens: 272000
        input_cost_per_1k: 0.015
        output_cost_per_1k: 0.12
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching, reasoning, web_search]
      'gpt-5-pro-2025-10-06':
        mode: responses
        max_input_tokens: 400000
        max_output_tokens: 272000
        input_cost_per_1k: 0.015
        output_cost_per_1k: 0.12
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching, reasoning, web_search]
      'gpt-5.1':
        mode: chat
        max_input_tokens: 272000
        max_output_tokens: 128000
        input_cost_per_1k: 0.00125
        output_cost_per_1k: 0.01
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching, reasoning]
      'gpt-5.1-2025-11-13':
        mode: chat
        max_input_tokens: 272000
        max_output_tokens: 128000
        input_cost_per_1k: 0.00125
        output_cost_per_1k: 0.01
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching, reasoning]
      'gpt-5.1-chat-latest':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 16384
        input_cost_per_1k: 0.00125
        output_cost_per_1k: 0.01
        capabilities: [vision, system_messages, json_mode, prompt_caching, reasoning]
      'gpt-5.1-codex':
        mode: responses
        max_input_tokens: 272000
        max_output_tokens: 128000
        input_cost_per_1k: 0.00125
        output_cost_per_1k: 0.01
        capabilities: [vision, function_calling, parallel_function_calling, json_mode, prompt_caching, reasoning]
      'gpt-5.1-codex-max':
        mode: responses
        max_input_tokens: 400000
        max_output_tokens: 128000
        input_cost_per_1k: 0.00125
        output_cost_per_1k: 0.01
        capabilities: [vision, function_calling, parallel_function_calling, json_mode, prompt_caching, reasoning]
      'gpt-5.1-codex-mini':
        mode: responses
        max_input_tokens: 272000
        max_output_tokens: 128000
        input_cost_per_1k: 0.00025
        output_cost_per_1k: 0.002
        capabilities: [vision, function_calling, parallel_function_calling, json_mode, prompt_caching, reasoning]
      'gpt-5.2':
        mode: chat
        max_input_tokens: 400000
        max_output_tokens: 128000
        input_cost_per_1k: 0.00175
        output_cost_per_1k: 0.014
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching, reasoning]
      'gpt-5.2-2025-12-11':
        mode: chat
        max_input_tokens: 400000
        max_output_tokens: 128000
        input_cost_per_1k: 0.00175
        output_cost_per_1k: 0.014
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching, reasoning]
      'gpt-5.2-chat-latest':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 16384
        input_cost_per_1k: 0.00175
        output_cost_per_1k: 0.014
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching, reasoning]
      'gpt-5.2-pro':
        mode: responses
        max_input_tokens: 400000
        max_output_tokens: 128000
        input_cost_per_1k: 0.021
        output_cost_per_1k: 0.168
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching, reasoning, web_search]
      'gpt-5.2-pro-2025-12-11':
        mode: responses
        max_input_tokens: 400000
        max_output_tokens: 128000
        input_cost_per_1k: 0.021
        output_cost_per_1k: 0.168
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching, reasoning, web_search]
      'gpt-image-1':
        mode: image
        input_cost_per_1k: 0.005
        output_cost_per_1k: 0.04
      'gpt-image-1-mini':
        mode: image
        input_cost_per_1k: 0.002
      'gpt-image-1.5':
        mode: image
        input_cost_per_1k: 0.005
        output_cost_per_1k: 0.01
        capabilities: [vision]
      'gpt-image-1.5-2025-12-16':
        mode: image
        input_cost_per_1k: 0.005
        output_cost_per_1k: 0.01
        capabilities: [vision]
      'gpt-realtime':
        mode: chat
        max_input_tokens: 32000
        max_output_tokens: 4096
        input_cost_per_1k: 0.004
        output_cost_per_1k: 0.016
        capabilities: [function_calling, parallel_function_calling, system_messages, audio_input, audio_output]
      'gpt-realtime-2025-08-28':
        mode: chat
        max_input_tokens: 32000
        max_output_tokens: 4096
        input_cost_per_1k: 0.004
        output_cost_per_1k: 0.016
        capabilities: [function_calling, parallel_function_calling, system_messages, audio_input, audio_output]
      'gpt-realtime-mini':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 4096
        input_cost_per_1k: 0.0006
        output_cost_per_1k: 0.0024
        capabilities: [function_calling, parallel_function_calling, system_messages, audio_input, audio_output]
      'hd/1024-x-1024/dall-e-3':
        mode: image
      'hd/1024-x-1792/dall-e-3':
        mode: image
      'hd/1792-x-1024/dall-e-3':
        mode: image
      'high/1024-x-1024/gpt-image-1':
        mode: image
      'high/1024-x-1536/gpt-image-1':
        mode: image
      'high/1536-x-1024/gpt-image-1':
        mode: image
      'low/1024-x-1024/gpt-image-1':
        mode: image
      'low/1024-x-1024/gpt-image-1-mini':
        mode: image
      'low/1024-x-1536/gpt-image-1':
        mode: image
      'low/1024-x-1536/gpt-image-1-mini':
        mode: image
      'low/1536-x-1024/gpt-image-1':
        mode: image
      'low/1536-x-1024/gpt-image-1-mini':
        mode: image
      'medium/1024-x-1024/gpt-image-1':
        mode: image
      'medium/1024-x-1024/gpt-image-1-mini':
        mode: image
      'medium/1024-x-1536/gpt-image-1':
        mode: image
      'medium/1024-x-1536/gpt-image-1-mini':
        mode: image
      'medium/1536-x-1024/gpt-image-1':
        mode: image
      'medium/1536-x-1024/gpt-image-1-mini':
        mode: image
      'o1':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 100000
        input_cost_per_1k: 0.015
        output_cost_per_1k: 0.06
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching, reasoning]
      'o1-2024-12-17':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 100000
        input_cost_per_1k: 0.015
        output_cost_per_1k: 0.06
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching, reasoning]
      'o1-mini':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 65536
        input_cost_per_1k: 0.0011
        output_cost_per_1k: 0.0044
        capabilities: [vision, prompt_caching]
      'o1-mini-2024-09-12':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 65536
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.012
        capabilities: [vision, prompt_caching, reasoning]
        deprecated: true
        deprecation_date: '2025-10-27'
      'o1-preview':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 32768
        input_cost_per_1k: 0.015
        output_cost_per_1k: 0.06
        capabilities: [vision, prompt_caching, reasoning]
      'o1-preview-2024-09-12':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 32768
        input_cost_per_1k: 0.015
        output_cost_per_1k: 0.06
        capabilities: [vision, prompt_caching, reasoning]
      'o1-pro':
        mode: responses
        max_input_tokens: 200000
        max_output_tokens: 100000
        input_cost_per_1k: 0.15
        output_cost_per_1k: 0.6
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching, reasoning]
      'o1-pro-2025-03-19':
        mode: responses
        max_input_tokens: 200000
        max_output_tokens: 100000
        input_cost_per_1k: 0.15
        output_cost_per_1k: 0.6
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching, reasoning]
      'o3':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 100000
        input_cost_per_1k: 0.002
        output_cost_per_1k: 0.008
        capabilities: [vision, function_calling, json_mode, prompt_caching, reasoning]
      'o3-2025-04-16':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 100000
        input_cost_per_1k: 0.002
        output_cost_per_1k: 0.008
        capabilities: [vision, function_calling, json_mode, prompt_caching, reasoning]
      'o3-deep-research':
        mode: responses
        max_input_tokens: 200000
        max_output_tokens: 100000
        input_cost_per_1k: 0.01
        output_cost_per_1k: 0.04
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching]
      'o3-deep-research-2025-06-26':
        mode: responses
        max_input_tokens: 200000
        max_output_tokens: 100000
        input_cost_per_1k: 0.01
        output_cost_per_1k: 0.04
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching]
      'o3-mini':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 100000
        input_cost_per_1k: 0.0011
        output_cost_per_1k: 0.0044
        capabilities: [function_calling, json_mode, prompt_caching, reasoning]
      'o3-mini-2025-01-31':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 100000
        input_cost_per_1k: 0.0011
        output_cost_per_1k: 0.0044
        capabilities: [function_calling, json_mode, prompt_caching, reasoning]
      'o3-pro':
        mode: responses
        max_input_tokens: 200000
        max_output_tokens: 100000
        input_cost_per_1k: 0.02
        output_cost_per_1k: 0.08
        capabilities: [vision, function_calling, json_mode, prompt_caching, reasoning]
      'o3-pro-2025-06-10':
        mode: responses
        max_input_tokens: 200000
        max_output_tokens: 100000
        input_cost_per_1k: 0.02
        output_cost_per_1k: 0.08
        capabilities: [vision, function_calling, json_mode, prompt_caching, reasoning]
      'o4-mini':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 100000
        input_cost_per_1k: 0.0011
        output_cost_per_1k: 0.0044
        capabilities: [vision, function_calling, json_mode, prompt_caching, reasoning]
      'o4-mini-2025-04-16':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 100000
        input_cost_per_1k: 0.0011
        output_cost_per_1k: 0.0044
        capabilities: [vision, function_calling, json_mode, prompt_caching, reasoning]
      'o4-mini-deep-research':
        mode: responses
        max_input_tokens: 200000
        max_output_tokens: 100000
        input_cost_per_1k: 0.002
        output_cost_per_1k: 0.008
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching]
      'o4-mini-deep-research-2025-06-26':
        mode: responses
        max_input_tokens: 200000
        max_output_tokens: 100000
        input_cost_per_1k: 0.002
        output_cost_per_1k: 0.008
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching]
      'omni-moderation-2024-09-26':
        mode: moderation
        max_input_tokens: 32768
        max_output_tokens: 0
      'omni-moderation-latest':
        mode: moderation
        max_input_tokens: 32768
        max_output_tokens: 0
      'omni-moderation-latest-intents':
        mode: moderation
        max_input_tokens: 32768
        max_output_tokens: 0
      'sora-2':
        litellm_id: 'openai/sora-2'
        mode: video_generation
      'sora-2-pro':
        litellm_id: 'openai/sora-2-pro'
        mode: video_generation
      'standard/1024-x-1024/dall-e-3':
        mode: image
      'standard/1024-x-1792/dall-e-3':
        mode: image
      'standard/1792-x-1024/dall-e-3':
        mode: image
      'text-embedding-3-large':
        mode: embedding
        max_input_tokens: 8191
        input_cost_per_1k: 0.00013
      'text-embedding-3-small':
        mode: embedding
        max_input_tokens: 8191
        input_cost_per_1k: 2e-05
      'text-embedding-ada-002':
        mode: embedding
        max_input_tokens: 8191
        input_cost_per_1k: 0.0001
      'text-embedding-ada-002-v2':
        mode: embedding
        max_input_tokens: 8191
        input_cost_per_1k: 0.0001
      'text-moderation-007':
        mode: moderation
        max_input_tokens: 32768
        max_output_tokens: 0
      'text-moderation-latest':
        mode: moderation
        max_input_tokens: 32768
        max_output_tokens: 0
      'text-moderation-stable':
        mode: moderation
        max_input_tokens: 32768
        max_output_tokens: 0
      'tts-1':
        mode: audio_speech
      'tts-1-hd':
        mode: audio_speech
      'whisper-1':
        mode: audio_transcription
  openrouter:
    model_count: 103
    models:
      'anthropic/claude-2':
        litellm_id: 'openrouter/anthropic/claude-2'
        mode: chat
        max_input_tokens: 100000
        max_output_tokens: 8191
        input_cost_per_1k: 0.01102
        output_cost_per_1k: 0.03268
      'anthropic/claude-3-5-haiku':
        litellm_id: 'openrouter/anthropic/claude-3-5-haiku'
        mode: chat
        max_input_tokens: 200000
        input_cost_per_1k: 0.001
        output_cost_per_1k: 0.005
        capabilities: [function_calling]
      'anthropic/claude-3-5-haiku-20241022':
        litellm_id: 'openrouter/anthropic/claude-3-5-haiku-20241022'
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 8192
        input_cost_per_1k: 0.001
        output_cost_per_1k: 0.005
        capabilities: [function_calling]
      'anthropic/claude-3-haiku':
        litellm_id: 'openrouter/anthropic/claude-3-haiku'
        mode: chat
        max_input_tokens: 200000
        input_cost_per_1k: 0.00025
        output_cost_per_1k: 0.00125
        capabilities: [vision, function_calling]
      'anthropic/claude-3-haiku-20240307':
        litellm_id: 'openrouter/anthropic/claude-3-haiku-20240307'
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 4096
        input_cost_per_1k: 0.00025
        output_cost_per_1k: 0.00125
        capabilities: [vision, function_calling]
      'anthropic/claude-3-opus':
        litellm_id: 'openrouter/anthropic/claude-3-opus'
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 4096
        input_cost_per_1k: 0.015
        output_cost_per_1k: 0.075
        capabilities: [vision, function_calling]
      'anthropic/claude-3-sonnet':
        litellm_id: 'openrouter/anthropic/claude-3-sonnet'
        mode: chat
        max_input_tokens: 200000
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.015
        capabilities: [vision, function_calling]
      'anthropic/claude-3.5-sonnet':
        litellm_id: 'openrouter/anthropic/claude-3.5-sonnet'
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 8192
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.015
        capabilities: [vision, function_calling]
      'anthropic/claude-3.5-sonnet:beta':
        litellm_id: 'openrouter/anthropic/claude-3.5-sonnet:beta'
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 8192
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.015
        capabilities: [vision, function_calling]
      'anthropic/claude-3.7-sonnet':
        litellm_id: 'openrouter/anthropic/claude-3.7-sonnet'
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 128000
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.015
        capabilities: [vision, function_calling, reasoning]
      'anthropic/claude-3.7-sonnet:beta':
        litellm_id: 'openrouter/anthropic/claude-3.7-sonnet:beta'
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 128000
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.015
        capabilities: [vision, function_calling, reasoning]
      'anthropic/claude-haiku-4.5':
        litellm_id: 'openrouter/anthropic/claude-haiku-4.5'
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 200000
        input_cost_per_1k: 0.001
        output_cost_per_1k: 0.005
        capabilities: [vision, function_calling, prompt_caching, reasoning]
      'anthropic/claude-instant-v1':
        litellm_id: 'openrouter/anthropic/claude-instant-v1'
        mode: chat
        max_input_tokens: 100000
        max_output_tokens: 8191
        input_cost_per_1k: 0.00163
        output_cost_per_1k: 0.00551
      'anthropic/claude-opus-4':
        litellm_id: 'openrouter/anthropic/claude-opus-4'
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 32000
        input_cost_per_1k: 0.015
        output_cost_per_1k: 0.075
        capabilities: [vision, function_calling, prompt_caching, reasoning]
      'anthropic/claude-opus-4.1':
        litellm_id: 'openrouter/anthropic/claude-opus-4.1'
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 32000
        input_cost_per_1k: 0.015
        output_cost_per_1k: 0.075
        capabilities: [vision, function_calling, prompt_caching, reasoning]
      'anthropic/claude-opus-4.5':
        litellm_id: 'openrouter/anthropic/claude-opus-4.5'
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 32000
        input_cost_per_1k: 0.005
        output_cost_per_1k: 0.025
        capabilities: [vision, function_calling, prompt_caching, reasoning]
      'anthropic/claude-sonnet-4':
        litellm_id: 'openrouter/anthropic/claude-sonnet-4'
        mode: chat
        max_input_tokens: 1000000
        max_output_tokens: 64000
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.015
        capabilities: [vision, function_calling, prompt_caching, reasoning]
      'anthropic/claude-sonnet-4.5':
        litellm_id: 'openrouter/anthropic/claude-sonnet-4.5'
        mode: chat
        max_input_tokens: 1000000
        max_output_tokens: 1000000
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.015
        capabilities: [vision, function_calling, prompt_caching, reasoning]
      'bytedance/ui-tars-1.5-7b':
        litellm_id: 'openrouter/bytedance/ui-tars-1.5-7b'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 2048
        input_cost_per_1k: 0.0001
        output_cost_per_1k: 0.0002
      'cognitivecomputations/dolphin-mixtral-8x7b':
        litellm_id: 'openrouter/cognitivecomputations/dolphin-mixtral-8x7b'
        mode: chat
        max_input_tokens: 32769
        input_cost_per_1k: 0.0005
        output_cost_per_1k: 0.0005
      'cohere/command-r-plus':
        litellm_id: 'openrouter/cohere/command-r-plus'
        mode: chat
        max_input_tokens: 128000
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.015
      'databricks/dbrx-instruct':
        litellm_id: 'openrouter/databricks/dbrx-instruct'
        mode: chat
        max_input_tokens: 32768
        input_cost_per_1k: 0.0006
        output_cost_per_1k: 0.0006
      'deepseek/deepseek-chat':
        litellm_id: 'openrouter/deepseek/deepseek-chat'
        mode: chat
        max_input_tokens: 65536
        max_output_tokens: 8192
        input_cost_per_1k: 0.00014
        output_cost_per_1k: 0.00028
        capabilities: [prompt_caching]
      'deepseek/deepseek-chat-v3-0324':
        litellm_id: 'openrouter/deepseek/deepseek-chat-v3-0324'
        mode: chat
        max_input_tokens: 65536
        max_output_tokens: 8192
        input_cost_per_1k: 0.00014
        output_cost_per_1k: 0.00028
        capabilities: [prompt_caching]
      'deepseek/deepseek-chat-v3.1':
        litellm_id: 'openrouter/deepseek/deepseek-chat-v3.1'
        mode: chat
        max_input_tokens: 163840
        max_output_tokens: 163840
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0008
        capabilities: [function_calling, prompt_caching, reasoning]
      'deepseek/deepseek-coder':
        litellm_id: 'openrouter/deepseek/deepseek-coder'
        mode: chat
        max_input_tokens: 66000
        max_output_tokens: 4096
        input_cost_per_1k: 0.00014
        output_cost_per_1k: 0.00028
        capabilities: [prompt_caching]
      'deepseek/deepseek-r1':
        litellm_id: 'openrouter/deepseek/deepseek-r1'
        mode: chat
        max_input_tokens: 65336
        max_output_tokens: 8192
        input_cost_per_1k: 0.00055
        output_cost_per_1k: 0.00219
        capabilities: [function_calling, prompt_caching, reasoning]
      'deepseek/deepseek-r1-0528':
        litellm_id: 'openrouter/deepseek/deepseek-r1-0528'
        mode: chat
        max_input_tokens: 65336
        max_output_tokens: 8192
        input_cost_per_1k: 0.0005
        output_cost_per_1k: 0.00215
        capabilities: [function_calling, prompt_caching, reasoning]
      'deepseek/deepseek-v3.2':
        litellm_id: 'openrouter/deepseek/deepseek-v3.2'
        mode: chat
        max_input_tokens: 163840
        max_output_tokens: 163840
        input_cost_per_1k: 0.00028
        output_cost_per_1k: 0.0004
        capabilities: [function_calling, prompt_caching, reasoning]
      'deepseek/deepseek-v3.2-exp':
        litellm_id: 'openrouter/deepseek/deepseek-v3.2-exp'
        mode: chat
        max_input_tokens: 163840
        max_output_tokens: 163840
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0004
        capabilities: [function_calling, prompt_caching]
      'fireworks/firellava-13b':
        litellm_id: 'openrouter/fireworks/firellava-13b'
        mode: chat
        max_input_tokens: 4096
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
      'google/gemini-2.0-flash-001':
        litellm_id: 'openrouter/google/gemini-2.0-flash-001'
        mode: chat
        max_input_tokens: 1048576
        max_output_tokens: 8192
        input_cost_per_1k: 0.0001
        output_cost_per_1k: 0.0004
        capabilities: [vision, function_calling, system_messages, json_mode, audio_output]
      'google/gemini-2.5-flash':
        litellm_id: 'openrouter/google/gemini-2.5-flash'
        mode: chat
        max_input_tokens: 1048576
        max_output_tokens: 8192
        input_cost_per_1k: 0.0003
        output_cost_per_1k: 0.0025
        capabilities: [vision, function_calling, system_messages, json_mode, audio_output]
      'google/gemini-2.5-pro':
        litellm_id: 'openrouter/google/gemini-2.5-pro'
        mode: chat
        max_input_tokens: 1048576
        max_output_tokens: 8192
        input_cost_per_1k: 0.00125
        output_cost_per_1k: 0.01
        capabilities: [vision, function_calling, system_messages, json_mode, audio_output]
      'google/gemini-3-pro-preview':
        litellm_id: 'openrouter/google/gemini-3-pro-preview'
        mode: chat
        max_input_tokens: 1048576
        max_output_tokens: 65535
        input_cost_per_1k: 0.002
        output_cost_per_1k: 0.012
        capabilities: [vision, function_calling, system_messages, json_mode, prompt_caching, reasoning, web_search, audio_input]
      'google/gemini-pro-1.5':
        litellm_id: 'openrouter/google/gemini-pro-1.5'
        mode: chat
        max_input_tokens: 1000000
        max_output_tokens: 8192
        input_cost_per_1k: 0.0025
        output_cost_per_1k: 0.0075
        capabilities: [vision, function_calling]
      'google/gemini-pro-vision':
        litellm_id: 'openrouter/google/gemini-pro-vision'
        mode: chat
        max_input_tokens: 45875
        input_cost_per_1k: 0.000125
        output_cost_per_1k: 0.000375
        capabilities: [vision, function_calling]
      'google/palm-2-chat-bison':
        litellm_id: 'openrouter/google/palm-2-chat-bison'
        mode: chat
        max_input_tokens: 25804
        input_cost_per_1k: 0.0005
        output_cost_per_1k: 0.0005
      'google/palm-2-codechat-bison':
        litellm_id: 'openrouter/google/palm-2-codechat-bison'
        mode: chat
        max_input_tokens: 20070
        input_cost_per_1k: 0.0005
        output_cost_per_1k: 0.0005
      'gryphe/mythomax-l2-13b':
        litellm_id: 'openrouter/gryphe/mythomax-l2-13b'
        mode: chat
        max_input_tokens: 8192
        input_cost_per_1k: 0.001875
        output_cost_per_1k: 0.001875
      'jondurbin/airoboros-l2-70b-2.1':
        litellm_id: 'openrouter/jondurbin/airoboros-l2-70b-2.1'
        mode: chat
        max_input_tokens: 4096
        input_cost_per_1k: 0.013875
        output_cost_per_1k: 0.013875
      'mancer/weaver':
        litellm_id: 'openrouter/mancer/weaver'
        mode: chat
        max_input_tokens: 8000
        input_cost_per_1k: 0.005625
        output_cost_per_1k: 0.005625
      'meta-llama/codellama-34b-instruct':
        litellm_id: 'openrouter/meta-llama/codellama-34b-instruct'
        mode: chat
        max_input_tokens: 8192
        input_cost_per_1k: 0.0005
        output_cost_per_1k: 0.0005
      'meta-llama/llama-2-13b-chat':
        litellm_id: 'openrouter/meta-llama/llama-2-13b-chat'
        mode: chat
        max_input_tokens: 4096
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
      'meta-llama/llama-2-70b-chat':
        litellm_id: 'openrouter/meta-llama/llama-2-70b-chat'
        mode: chat
        max_input_tokens: 4096
        input_cost_per_1k: 0.0015
        output_cost_per_1k: 0.0015
      'meta-llama/llama-3-70b-instruct':
        litellm_id: 'openrouter/meta-llama/llama-3-70b-instruct'
        mode: chat
        max_input_tokens: 8192
        input_cost_per_1k: 0.00059
        output_cost_per_1k: 0.00079
      'meta-llama/llama-3-70b-instruct:nitro':
        litellm_id: 'openrouter/meta-llama/llama-3-70b-instruct:nitro'
        mode: chat
        max_input_tokens: 8192
        input_cost_per_1k: 0.0009
        output_cost_per_1k: 0.0009
      'meta-llama/llama-3-8b-instruct:extended':
        litellm_id: 'openrouter/meta-llama/llama-3-8b-instruct:extended'
        mode: chat
        max_input_tokens: 16384
        input_cost_per_1k: 0.000225
        output_cost_per_1k: 0.00225
      'meta-llama/llama-3-8b-instruct:free':
        litellm_id: 'openrouter/meta-llama/llama-3-8b-instruct:free'
        mode: chat
        max_input_tokens: 8192
      'microsoft/wizardlm-2-8x22b:nitro':
        litellm_id: 'openrouter/microsoft/wizardlm-2-8x22b:nitro'
        mode: chat
        max_input_tokens: 65536
        input_cost_per_1k: 0.001
        output_cost_per_1k: 0.001
      'minimax/minimax-m2':
        litellm_id: 'openrouter/minimax/minimax-m2'
        mode: chat
        max_input_tokens: 204800
        max_output_tokens: 204800
        input_cost_per_1k: 0.000255
        output_cost_per_1k: 0.00102
        capabilities: [function_calling, reasoning]
      'mistralai/devstral-2512':
        litellm_id: 'openrouter/mistralai/devstral-2512'
        mode: chat
        max_input_tokens: 262144
        max_output_tokens: 65536
        input_cost_per_1k: 0.00015
        output_cost_per_1k: 0.0006
        capabilities: [function_calling]
      'mistralai/devstral-2512:free':
        litellm_id: 'openrouter/mistralai/devstral-2512:free'
        mode: chat
        max_input_tokens: 262144
        max_output_tokens: 262144
        capabilities: [function_calling]
      'mistralai/ministral-14b-2512':
        litellm_id: 'openrouter/mistralai/ministral-14b-2512'
        mode: chat
        max_input_tokens: 262144
        max_output_tokens: 262144
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
        capabilities: [vision, function_calling]
      'mistralai/ministral-3b-2512':
        litellm_id: 'openrouter/mistralai/ministral-3b-2512'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.0001
        output_cost_per_1k: 0.0001
        capabilities: [vision, function_calling]
      'mistralai/ministral-8b-2512':
        litellm_id: 'openrouter/mistralai/ministral-8b-2512'
        mode: chat
        max_input_tokens: 262144
        max_output_tokens: 262144
        input_cost_per_1k: 0.00015
        output_cost_per_1k: 0.00015
        capabilities: [vision, function_calling]
      'mistralai/mistral-7b-instruct':
        litellm_id: 'openrouter/mistralai/mistral-7b-instruct'
        mode: chat
        max_input_tokens: 8192
        input_cost_per_1k: 0.00013
        output_cost_per_1k: 0.00013
      'mistralai/mistral-7b-instruct:free':
        litellm_id: 'openrouter/mistralai/mistral-7b-instruct:free'
        mode: chat
        max_input_tokens: 8192
      'mistralai/mistral-large':
        litellm_id: 'openrouter/mistralai/mistral-large'
        mode: chat
        max_input_tokens: 32000
        input_cost_per_1k: 0.008
        output_cost_per_1k: 0.024
      'mistralai/mistral-large-2512':
        litellm_id: 'openrouter/mistralai/mistral-large-2512'
        mode: chat
        max_input_tokens: 262144
        max_output_tokens: 262144
        input_cost_per_1k: 0.0005
        output_cost_per_1k: 0.0015
        capabilities: [vision, function_calling]
      'mistralai/mistral-small-3.1-24b-instruct':
        litellm_id: 'openrouter/mistralai/mistral-small-3.1-24b-instruct'
        mode: chat
        max_input_tokens: 32000
        input_cost_per_1k: 0.0001
        output_cost_per_1k: 0.0003
      'mistralai/mistral-small-3.2-24b-instruct':
        litellm_id: 'openrouter/mistralai/mistral-small-3.2-24b-instruct'
        mode: chat
        max_input_tokens: 32000
        input_cost_per_1k: 0.0001
        output_cost_per_1k: 0.0003
      'mistralai/mixtral-8x22b-instruct':
        litellm_id: 'openrouter/mistralai/mixtral-8x22b-instruct'
        mode: chat
        max_input_tokens: 65536
        input_cost_per_1k: 0.00065
        output_cost_per_1k: 0.00065
      'nousresearch/nous-hermes-llama2-13b':
        litellm_id: 'openrouter/nousresearch/nous-hermes-llama2-13b'
        mode: chat
        max_input_tokens: 4096
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
      'openai/gpt-3.5-turbo':
        litellm_id: 'openrouter/openai/gpt-3.5-turbo'
        mode: chat
        max_input_tokens: 4095
        input_cost_per_1k: 0.0015
        output_cost_per_1k: 0.002
      'openai/gpt-3.5-turbo-16k':
        litellm_id: 'openrouter/openai/gpt-3.5-turbo-16k'
        mode: chat
        max_input_tokens: 16383
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.004
      'openai/gpt-4':
        litellm_id: 'openrouter/openai/gpt-4'
        mode: chat
        max_input_tokens: 8192
        input_cost_per_1k: 0.03
        output_cost_per_1k: 0.06
      'openai/gpt-4-vision-preview':
        litellm_id: 'openrouter/openai/gpt-4-vision-preview'
        mode: chat
        max_input_tokens: 130000
        input_cost_per_1k: 0.01
        output_cost_per_1k: 0.03
        capabilities: [vision, function_calling]
      'openai/gpt-4.1':
        litellm_id: 'openrouter/openai/gpt-4.1'
        mode: chat
        max_input_tokens: 1047576
        max_output_tokens: 32768
        input_cost_per_1k: 0.002
        output_cost_per_1k: 0.008
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching]
      'openai/gpt-4.1-2025-04-14':
        litellm_id: 'openrouter/openai/gpt-4.1-2025-04-14'
        mode: chat
        max_input_tokens: 1047576
        max_output_tokens: 32768
        input_cost_per_1k: 0.002
        output_cost_per_1k: 0.008
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching]
      'openai/gpt-4.1-mini':
        litellm_id: 'openrouter/openai/gpt-4.1-mini'
        mode: chat
        max_input_tokens: 1047576
        max_output_tokens: 32768
        input_cost_per_1k: 0.0004
        output_cost_per_1k: 0.0016
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching]
      'openai/gpt-4.1-mini-2025-04-14':
        litellm_id: 'openrouter/openai/gpt-4.1-mini-2025-04-14'
        mode: chat
        max_input_tokens: 1047576
        max_output_tokens: 32768
        input_cost_per_1k: 0.0004
        output_cost_per_1k: 0.0016
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching]
      'openai/gpt-4.1-nano':
        litellm_id: 'openrouter/openai/gpt-4.1-nano'
        mode: chat
        max_input_tokens: 1047576
        max_output_tokens: 32768
        input_cost_per_1k: 0.0001
        output_cost_per_1k: 0.0004
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching]
      'openai/gpt-4.1-nano-2025-04-14':
        litellm_id: 'openrouter/openai/gpt-4.1-nano-2025-04-14'
        mode: chat
        max_input_tokens: 1047576
        max_output_tokens: 32768
        input_cost_per_1k: 0.0001
        output_cost_per_1k: 0.0004
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching]
      'openai/gpt-4o':
        litellm_id: 'openrouter/openai/gpt-4o'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 4096
        input_cost_per_1k: 0.0025
        output_cost_per_1k: 0.01
        capabilities: [vision, function_calling, parallel_function_calling]
      'openai/gpt-4o-2024-05-13':
        litellm_id: 'openrouter/openai/gpt-4o-2024-05-13'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 4096
        input_cost_per_1k: 0.005
        output_cost_per_1k: 0.015
        capabilities: [vision, function_calling, parallel_function_calling]
      'openai/gpt-5':
        litellm_id: 'openrouter/openai/gpt-5'
        mode: chat
        max_input_tokens: 272000
        max_output_tokens: 128000
        input_cost_per_1k: 0.00125
        output_cost_per_1k: 0.01
        capabilities: [reasoning]
      'openai/gpt-5-chat':
        litellm_id: 'openrouter/openai/gpt-5-chat'
        mode: chat
        max_input_tokens: 272000
        max_output_tokens: 128000
        input_cost_per_1k: 0.00125
        output_cost_per_1k: 0.01
        capabilities: [reasoning]
      'openai/gpt-5-codex':
        litellm_id: 'openrouter/openai/gpt-5-codex'
        mode: chat
        max_input_tokens: 272000
        max_output_tokens: 128000
        input_cost_per_1k: 0.00125
        output_cost_per_1k: 0.01
        capabilities: [reasoning]
      'openai/gpt-5-mini':
        litellm_id: 'openrouter/openai/gpt-5-mini'
        mode: chat
        max_input_tokens: 272000
        max_output_tokens: 128000
        input_cost_per_1k: 0.00025
        output_cost_per_1k: 0.002
        capabilities: [reasoning]
      'openai/gpt-5-nano':
        litellm_id: 'openrouter/openai/gpt-5-nano'
        mode: chat
        max_input_tokens: 272000
        max_output_tokens: 128000
        input_cost_per_1k: 5e-05
        output_cost_per_1k: 0.0004
        capabilities: [reasoning]
      'openai/gpt-5.2':
        litellm_id: 'openrouter/openai/gpt-5.2'
        mode: chat
        max_input_tokens: 400000
        max_output_tokens: 128000
        input_cost_per_1k: 0.00175
        output_cost_per_1k: 0.014
        capabilities: [vision, function_calling, prompt_caching, reasoning]
      'openai/gpt-5.2-chat':
        litellm_id: 'openrouter/openai/gpt-5.2-chat'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 16384
        input_cost_per_1k: 0.00175
        output_cost_per_1k: 0.014
        capabilities: [vision, function_calling, prompt_caching]
      'openai/gpt-5.2-pro':
        litellm_id: 'openrouter/openai/gpt-5.2-pro'
        mode: chat
        max_input_tokens: 400000
        max_output_tokens: 128000
        input_cost_per_1k: 0.021
        output_cost_per_1k: 0.168
        capabilities: [vision, function_calling, reasoning]
      'openai/gpt-oss-120b':
        litellm_id: 'openrouter/openai/gpt-oss-120b'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 32768
        input_cost_per_1k: 0.00018
        output_cost_per_1k: 0.0008
        capabilities: [function_calling, parallel_function_calling, json_mode, reasoning]
      'openai/gpt-oss-20b':
        litellm_id: 'openrouter/openai/gpt-oss-20b'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 32768
        input_cost_per_1k: 0.00018
        output_cost_per_1k: 0.0008
        capabilities: [function_calling, parallel_function_calling, json_mode, reasoning]
      'openai/o1':
        litellm_id: 'openrouter/openai/o1'
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 100000
        input_cost_per_1k: 0.015
        output_cost_per_1k: 0.06
        capabilities: [vision, function_calling, parallel_function_calling, system_messages, json_mode, prompt_caching]
      'openai/o1-mini':
        litellm_id: 'openrouter/openai/o1-mini'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 65536
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.012
        capabilities: [function_calling, parallel_function_calling]
      'openai/o1-mini-2024-09-12':
        litellm_id: 'openrouter/openai/o1-mini-2024-09-12'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 65536
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.012
        capabilities: [function_calling, parallel_function_calling]
      'openai/o1-preview':
        litellm_id: 'openrouter/openai/o1-preview'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 32768
        input_cost_per_1k: 0.015
        output_cost_per_1k: 0.06
        capabilities: [function_calling, parallel_function_calling]
      'openai/o1-preview-2024-09-12':
        litellm_id: 'openrouter/openai/o1-preview-2024-09-12'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 32768
        input_cost_per_1k: 0.015
        output_cost_per_1k: 0.06
        capabilities: [function_calling, parallel_function_calling]
      'openai/o3-mini':
        litellm_id: 'openrouter/openai/o3-mini'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 65536
        input_cost_per_1k: 0.0011
        output_cost_per_1k: 0.0044
        capabilities: [function_calling, parallel_function_calling, reasoning]
      'openai/o3-mini-high':
        litellm_id: 'openrouter/openai/o3-mini-high'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 65536
        input_cost_per_1k: 0.0011
        output_cost_per_1k: 0.0044
        capabilities: [function_calling, parallel_function_calling, reasoning]
      'pygmalionai/mythalion-13b':
        litellm_id: 'openrouter/pygmalionai/mythalion-13b'
        mode: chat
        max_input_tokens: 4096
        input_cost_per_1k: 0.001875
        output_cost_per_1k: 0.001875
      'qwen/qwen-2.5-coder-32b-instruct':
        litellm_id: 'openrouter/qwen/qwen-2.5-coder-32b-instruct'
        mode: chat
        max_input_tokens: 33792
        max_output_tokens: 33792
        input_cost_per_1k: 0.00018
        output_cost_per_1k: 0.00018
      'qwen/qwen-vl-plus':
        litellm_id: 'openrouter/qwen/qwen-vl-plus'
        mode: chat
        max_input_tokens: 8192
        max_output_tokens: 2048
        input_cost_per_1k: 0.00021
        output_cost_per_1k: 0.00063
        capabilities: [vision]
      'qwen/qwen3-coder':
        litellm_id: 'openrouter/qwen/qwen3-coder'
        mode: chat
        max_input_tokens: 262100
        max_output_tokens: 262100
        input_cost_per_1k: 0.00022
        output_cost_per_1k: 0.00095
        capabilities: [function_calling]
      'switchpoint/router':
        litellm_id: 'openrouter/switchpoint/router'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.00085
        output_cost_per_1k: 0.0034
      'undi95/remm-slerp-l2-13b':
        litellm_id: 'openrouter/undi95/remm-slerp-l2-13b'
        mode: chat
        max_input_tokens: 6144
        input_cost_per_1k: 0.001875
        output_cost_per_1k: 0.001875
      'x-ai/grok-4':
        litellm_id: 'openrouter/x-ai/grok-4'
        mode: chat
        max_input_tokens: 256000
        max_output_tokens: 256000
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.015
        capabilities: [function_calling, reasoning, web_search]
      'x-ai/grok-4-fast:free':
        litellm_id: 'openrouter/x-ai/grok-4-fast:free'
        mode: chat
        max_input_tokens: 2000000
        max_output_tokens: 30000
        capabilities: [function_calling, reasoning]
      'z-ai/glm-4.6':
        litellm_id: 'openrouter/z-ai/glm-4.6'
        mode: chat
        max_input_tokens: 202800
        max_output_tokens: 131000
        input_cost_per_1k: 0.0004
        output_cost_per_1k: 0.00175
        capabilities: [function_calling, reasoning]
      'z-ai/glm-4.6:exacto':
        litellm_id: 'openrouter/z-ai/glm-4.6:exacto'
        mode: chat
        max_input_tokens: 202800
        max_output_tokens: 131000
        input_cost_per_1k: 0.00045
        output_cost_per_1k: 0.0019
        capabilities: [function_calling, reasoning]
  ovhcloud:
    model_count: 15
    models:
      'DeepSeek-R1-Distill-Llama-70B':
        litellm_id: 'ovhcloud/DeepSeek-R1-Distill-Llama-70B'
        mode: chat
        max_input_tokens: 131000
        max_output_tokens: 131000
        input_cost_per_1k: 0.00067
        output_cost_per_1k: 0.00067
        capabilities: [function_calling, json_mode, reasoning]
      'Llama-3.1-8B-Instruct':
        litellm_id: 'ovhcloud/Llama-3.1-8B-Instruct'
        mode: chat
        max_input_tokens: 131000
        max_output_tokens: 131000
        input_cost_per_1k: 0.0001
        output_cost_per_1k: 0.0001
        capabilities: [function_calling, json_mode]
      'Meta-Llama-3_1-70B-Instruct':
        litellm_id: 'ovhcloud/Meta-Llama-3_1-70B-Instruct'
        mode: chat
        max_input_tokens: 131000
        max_output_tokens: 131000
        input_cost_per_1k: 0.00067
        output_cost_per_1k: 0.00067
      'Meta-Llama-3_3-70B-Instruct':
        litellm_id: 'ovhcloud/Meta-Llama-3_3-70B-Instruct'
        mode: chat
        max_input_tokens: 131000
        max_output_tokens: 131000
        input_cost_per_1k: 0.00067
        output_cost_per_1k: 0.00067
        capabilities: [function_calling, json_mode]
      'Mistral-7B-Instruct-v0.3':
        litellm_id: 'ovhcloud/Mistral-7B-Instruct-v0.3'
        mode: chat
        max_input_tokens: 127000
        max_output_tokens: 127000
        input_cost_per_1k: 0.0001
        output_cost_per_1k: 0.0001
        capabilities: [function_calling, json_mode]
      'Mistral-Nemo-Instruct-2407':
        litellm_id: 'ovhcloud/Mistral-Nemo-Instruct-2407'
        mode: chat
        max_input_tokens: 118000
        max_output_tokens: 118000
        input_cost_per_1k: 0.00013
        output_cost_per_1k: 0.00013
        capabilities: [function_calling, json_mode]
      'Mistral-Small-3.2-24B-Instruct-2506':
        litellm_id: 'ovhcloud/Mistral-Small-3.2-24B-Instruct-2506'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 128000
        input_cost_per_1k: 9e-05
        output_cost_per_1k: 0.00028
        capabilities: [vision, function_calling, json_mode]
      'Mixtral-8x7B-Instruct-v0.1':
        litellm_id: 'ovhcloud/Mixtral-8x7B-Instruct-v0.1'
        mode: chat
        max_input_tokens: 32000
        max_output_tokens: 32000
        input_cost_per_1k: 0.00063
        output_cost_per_1k: 0.00063
        capabilities: [json_mode]
      'Qwen2.5-Coder-32B-Instruct':
        litellm_id: 'ovhcloud/Qwen2.5-Coder-32B-Instruct'
        mode: chat
        max_input_tokens: 32000
        max_output_tokens: 32000
        input_cost_per_1k: 0.00087
        output_cost_per_1k: 0.00087
        capabilities: [json_mode]
      'Qwen2.5-VL-72B-Instruct':
        litellm_id: 'ovhcloud/Qwen2.5-VL-72B-Instruct'
        mode: chat
        max_input_tokens: 32000
        max_output_tokens: 32000
        input_cost_per_1k: 0.00091
        output_cost_per_1k: 0.00091
        capabilities: [vision, json_mode]
      'Qwen3-32B':
        litellm_id: 'ovhcloud/Qwen3-32B'
        mode: chat
        max_input_tokens: 32000
        max_output_tokens: 32000
        input_cost_per_1k: 8e-05
        output_cost_per_1k: 0.00023
        capabilities: [function_calling, json_mode, reasoning]
      'gpt-oss-120b':
        litellm_id: 'ovhcloud/gpt-oss-120b'
        mode: chat
        max_input_tokens: 131000
        max_output_tokens: 131000
        input_cost_per_1k: 8e-05
        output_cost_per_1k: 0.0004
        capabilities: [json_mode, reasoning]
      'gpt-oss-20b':
        litellm_id: 'ovhcloud/gpt-oss-20b'
        mode: chat
        max_input_tokens: 131000
        max_output_tokens: 131000
        input_cost_per_1k: 4e-05
        output_cost_per_1k: 0.00015
        capabilities: [json_mode, reasoning]
      'llava-v1.6-mistral-7b-hf':
        litellm_id: 'ovhcloud/llava-v1.6-mistral-7b-hf'
        mode: chat
        max_input_tokens: 32000
        max_output_tokens: 32000
        input_cost_per_1k: 0.00029
        output_cost_per_1k: 0.00029
        capabilities: [vision, json_mode]
      'mamba-codestral-7B-v0.1':
        litellm_id: 'ovhcloud/mamba-codestral-7B-v0.1'
        mode: chat
        max_input_tokens: 256000
        max_output_tokens: 256000
        input_cost_per_1k: 0.00019
        output_cost_per_1k: 0.00019
        capabilities: [json_mode]
  palm:
    model_count: 6
    models:
      'chat-bison':
        litellm_id: 'palm/chat-bison'
        mode: chat
        max_input_tokens: 8192
        max_output_tokens: 4096
        input_cost_per_1k: 0.000125
        output_cost_per_1k: 0.000125
      'chat-bison-001':
        litellm_id: 'palm/chat-bison-001'
        mode: chat
        max_input_tokens: 8192
        max_output_tokens: 4096
        input_cost_per_1k: 0.000125
        output_cost_per_1k: 0.000125
      'text-bison':
        litellm_id: 'palm/text-bison'
        mode: completion
        max_input_tokens: 8192
        max_output_tokens: 1024
        input_cost_per_1k: 0.000125
        output_cost_per_1k: 0.000125
      'text-bison-001':
        litellm_id: 'palm/text-bison-001'
        mode: completion
        max_input_tokens: 8192
        max_output_tokens: 1024
        input_cost_per_1k: 0.000125
        output_cost_per_1k: 0.000125
      'text-bison-safety-off':
        litellm_id: 'palm/text-bison-safety-off'
        mode: completion
        max_input_tokens: 8192
        max_output_tokens: 1024
        input_cost_per_1k: 0.000125
        output_cost_per_1k: 0.000125
      'text-bison-safety-recitation-off':
        litellm_id: 'palm/text-bison-safety-recitation-off'
        mode: completion
        max_input_tokens: 8192
        max_output_tokens: 1024
        input_cost_per_1k: 0.000125
        output_cost_per_1k: 0.000125
  parallel_ai:
    model_count: 2
    models:
      'search':
        litellm_id: 'parallel_ai/search'
        mode: search
      'search-pro':
        litellm_id: 'parallel_ai/search-pro'
        mode: search
  perplexity:
    model_count: 26
    models:
      'codellama-34b-instruct':
        litellm_id: 'perplexity/codellama-34b-instruct'
        mode: chat
        max_input_tokens: 16384
        max_output_tokens: 16384
        input_cost_per_1k: 0.00035
        output_cost_per_1k: 0.0014
      'codellama-70b-instruct':
        litellm_id: 'perplexity/codellama-70b-instruct'
        mode: chat
        max_input_tokens: 16384
        max_output_tokens: 16384
        input_cost_per_1k: 0.0007
        output_cost_per_1k: 0.0028
      'llama-2-70b-chat':
        litellm_id: 'perplexity/llama-2-70b-chat'
        mode: chat
        max_input_tokens: 4096
        max_output_tokens: 4096
        input_cost_per_1k: 0.0007
        output_cost_per_1k: 0.0028
      'llama-3.1-70b-instruct':
        litellm_id: 'perplexity/llama-3.1-70b-instruct'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.001
        output_cost_per_1k: 0.001
      'llama-3.1-8b-instruct':
        litellm_id: 'perplexity/llama-3.1-8b-instruct'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
      'llama-3.1-sonar-huge-128k-online':
        litellm_id: 'perplexity/llama-3.1-sonar-huge-128k-online'
        mode: chat
        max_input_tokens: 127072
        max_output_tokens: 127072
        input_cost_per_1k: 0.005
        output_cost_per_1k: 0.005
        deprecated: true
        deprecation_date: '2025-02-22'
      'llama-3.1-sonar-large-128k-chat':
        litellm_id: 'perplexity/llama-3.1-sonar-large-128k-chat'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.001
        output_cost_per_1k: 0.001
        deprecated: true
        deprecation_date: '2025-02-22'
      'llama-3.1-sonar-large-128k-online':
        litellm_id: 'perplexity/llama-3.1-sonar-large-128k-online'
        mode: chat
        max_input_tokens: 127072
        max_output_tokens: 127072
        input_cost_per_1k: 0.001
        output_cost_per_1k: 0.001
        deprecated: true
        deprecation_date: '2025-02-22'
      'llama-3.1-sonar-small-128k-chat':
        litellm_id: 'perplexity/llama-3.1-sonar-small-128k-chat'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
        deprecated: true
        deprecation_date: '2025-02-22'
      'llama-3.1-sonar-small-128k-online':
        litellm_id: 'perplexity/llama-3.1-sonar-small-128k-online'
        mode: chat
        max_input_tokens: 127072
        max_output_tokens: 127072
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
        deprecated: true
        deprecation_date: '2025-02-22'
      'mistral-7b-instruct':
        litellm_id: 'perplexity/mistral-7b-instruct'
        mode: chat
        max_input_tokens: 4096
        max_output_tokens: 4096
        input_cost_per_1k: 7e-05
        output_cost_per_1k: 0.00028
      'mixtral-8x7b-instruct':
        litellm_id: 'perplexity/mixtral-8x7b-instruct'
        mode: chat
        max_input_tokens: 4096
        max_output_tokens: 4096
        input_cost_per_1k: 7e-05
        output_cost_per_1k: 0.00028
      'pplx-70b-chat':
        litellm_id: 'perplexity/pplx-70b-chat'
        mode: chat
        max_input_tokens: 4096
        max_output_tokens: 4096
        input_cost_per_1k: 0.0007
        output_cost_per_1k: 0.0028
      'pplx-70b-online':
        litellm_id: 'perplexity/pplx-70b-online'
        mode: chat
        max_input_tokens: 4096
        max_output_tokens: 4096
        output_cost_per_1k: 0.0028
      'pplx-7b-chat':
        litellm_id: 'perplexity/pplx-7b-chat'
        mode: chat
        max_input_tokens: 8192
        max_output_tokens: 8192
        input_cost_per_1k: 7e-05
        output_cost_per_1k: 0.00028
      'pplx-7b-online':
        litellm_id: 'perplexity/pplx-7b-online'
        mode: chat
        max_input_tokens: 4096
        max_output_tokens: 4096
        output_cost_per_1k: 0.00028
      'search':
        litellm_id: 'perplexity/search'
        mode: search
      'sonar':
        litellm_id: 'perplexity/sonar'
        mode: chat
        max_input_tokens: 128000
        input_cost_per_1k: 0.001
        output_cost_per_1k: 0.001
        capabilities: [web_search]
      'sonar-deep-research':
        litellm_id: 'perplexity/sonar-deep-research'
        mode: chat
        max_input_tokens: 128000
        input_cost_per_1k: 0.002
        output_cost_per_1k: 0.008
        capabilities: [reasoning, web_search]
      'sonar-medium-chat':
        litellm_id: 'perplexity/sonar-medium-chat'
        mode: chat
        max_input_tokens: 16384
        max_output_tokens: 16384
        input_cost_per_1k: 0.0006
        output_cost_per_1k: 0.0018
      'sonar-medium-online':
        litellm_id: 'perplexity/sonar-medium-online'
        mode: chat
        max_input_tokens: 12000
        max_output_tokens: 12000
        output_cost_per_1k: 0.0018
      'sonar-pro':
        litellm_id: 'perplexity/sonar-pro'
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 8000
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.015
        capabilities: [web_search]
      'sonar-reasoning':
        litellm_id: 'perplexity/sonar-reasoning'
        mode: chat
        max_input_tokens: 128000
        input_cost_per_1k: 0.001
        output_cost_per_1k: 0.005
        capabilities: [reasoning, web_search]
      'sonar-reasoning-pro':
        litellm_id: 'perplexity/sonar-reasoning-pro'
        mode: chat
        max_input_tokens: 128000
        input_cost_per_1k: 0.002
        output_cost_per_1k: 0.008
        capabilities: [reasoning, web_search]
      'sonar-small-chat':
        litellm_id: 'perplexity/sonar-small-chat'
        mode: chat
        max_input_tokens: 16384
        max_output_tokens: 16384
        input_cost_per_1k: 7e-05
        output_cost_per_1k: 0.00028
      'sonar-small-online':
        litellm_id: 'perplexity/sonar-small-online'
        mode: chat
        max_input_tokens: 12000
        max_output_tokens: 12000
        output_cost_per_1k: 0.00028
  publicai:
    model_count: 9
    models:
      'BSC-LT/ALIA-40b-instruct_Q8_0':
        litellm_id: 'publicai/BSC-LT/ALIA-40b-instruct_Q8_0'
        mode: chat
        max_input_tokens: 8192
        max_output_tokens: 4096
        capabilities: [function_calling]
      'BSC-LT/salamandra-7b-instruct-tools-16k':
        litellm_id: 'publicai/BSC-LT/salamandra-7b-instruct-tools-16k'
        mode: chat
        max_input_tokens: 16384
        max_output_tokens: 4096
        capabilities: [function_calling]
      'aisingapore/Gemma-SEA-LION-v4-27B-IT':
        litellm_id: 'publicai/aisingapore/Gemma-SEA-LION-v4-27B-IT'
        mode: chat
        max_input_tokens: 8192
        max_output_tokens: 4096
        capabilities: [function_calling]
      'aisingapore/Qwen-SEA-LION-v4-32B-IT':
        litellm_id: 'publicai/aisingapore/Qwen-SEA-LION-v4-32B-IT'
        mode: chat
        max_input_tokens: 32768
        max_output_tokens: 4096
        capabilities: [function_calling]
      'allenai/Olmo-3-32B-Think':
        litellm_id: 'publicai/allenai/Olmo-3-32B-Think'
        mode: chat
        max_input_tokens: 32768
        max_output_tokens: 4096
        capabilities: [function_calling, reasoning]
      'allenai/Olmo-3-7B-Instruct':
        litellm_id: 'publicai/allenai/Olmo-3-7B-Instruct'
        mode: chat
        max_input_tokens: 32768
        max_output_tokens: 4096
        capabilities: [function_calling]
      'allenai/Olmo-3-7B-Think':
        litellm_id: 'publicai/allenai/Olmo-3-7B-Think'
        mode: chat
        max_input_tokens: 32768
        max_output_tokens: 4096
        capabilities: [function_calling, reasoning]
      'swiss-ai/apertus-70b-instruct':
        litellm_id: 'publicai/swiss-ai/apertus-70b-instruct'
        mode: chat
        max_input_tokens: 8192
        max_output_tokens: 4096
        capabilities: [function_calling]
      'swiss-ai/apertus-8b-instruct':
        litellm_id: 'publicai/swiss-ai/apertus-8b-instruct'
        mode: chat
        max_input_tokens: 8192
        max_output_tokens: 4096
        capabilities: [function_calling]
  recraft:
    model_count: 2
    models:
      'recraftv2':
        litellm_id: 'recraft/recraftv2'
        mode: image
      'recraftv3':
        litellm_id: 'recraft/recraftv3'
        mode: image
  replicate:
    model_count: 13
    models:
      'meta/llama-2-13b':
        litellm_id: 'replicate/meta/llama-2-13b'
        mode: chat
        max_input_tokens: 4096
        max_output_tokens: 4096
        input_cost_per_1k: 0.0001
        output_cost_per_1k: 0.0005
      'meta/llama-2-13b-chat':
        litellm_id: 'replicate/meta/llama-2-13b-chat'
        mode: chat
        max_input_tokens: 4096
        max_output_tokens: 4096
        input_cost_per_1k: 0.0001
        output_cost_per_1k: 0.0005
      'meta/llama-2-70b':
        litellm_id: 'replicate/meta/llama-2-70b'
        mode: chat
        max_input_tokens: 4096
        max_output_tokens: 4096
        input_cost_per_1k: 0.00065
        output_cost_per_1k: 0.00275
      'meta/llama-2-70b-chat':
        litellm_id: 'replicate/meta/llama-2-70b-chat'
        mode: chat
        max_input_tokens: 4096
        max_output_tokens: 4096
        input_cost_per_1k: 0.00065
        output_cost_per_1k: 0.00275
      'meta/llama-2-7b':
        litellm_id: 'replicate/meta/llama-2-7b'
        mode: chat
        max_input_tokens: 4096
        max_output_tokens: 4096
        input_cost_per_1k: 5e-05
        output_cost_per_1k: 0.00025
      'meta/llama-2-7b-chat':
        litellm_id: 'replicate/meta/llama-2-7b-chat'
        mode: chat
        max_input_tokens: 4096
        max_output_tokens: 4096
        input_cost_per_1k: 5e-05
        output_cost_per_1k: 0.00025
      'meta/llama-3-70b':
        litellm_id: 'replicate/meta/llama-3-70b'
        mode: chat
        max_input_tokens: 8192
        max_output_tokens: 8192
        input_cost_per_1k: 0.00065
        output_cost_per_1k: 0.00275
      'meta/llama-3-70b-instruct':
        litellm_id: 'replicate/meta/llama-3-70b-instruct'
        mode: chat
        max_input_tokens: 8192
        max_output_tokens: 8192
        input_cost_per_1k: 0.00065
        output_cost_per_1k: 0.00275
      'meta/llama-3-8b':
        litellm_id: 'replicate/meta/llama-3-8b'
        mode: chat
        max_input_tokens: 8086
        max_output_tokens: 8086
        input_cost_per_1k: 5e-05
        output_cost_per_1k: 0.00025
      'meta/llama-3-8b-instruct':
        litellm_id: 'replicate/meta/llama-3-8b-instruct'
        mode: chat
        max_input_tokens: 8086
        max_output_tokens: 8086
        input_cost_per_1k: 5e-05
        output_cost_per_1k: 0.00025
      'mistralai/mistral-7b-instruct-v0.2':
        litellm_id: 'replicate/mistralai/mistral-7b-instruct-v0.2'
        mode: chat
        max_input_tokens: 4096
        max_output_tokens: 4096
        input_cost_per_1k: 5e-05
        output_cost_per_1k: 0.00025
      'mistralai/mistral-7b-v0.1':
        litellm_id: 'replicate/mistralai/mistral-7b-v0.1'
        mode: chat
        max_input_tokens: 4096
        max_output_tokens: 4096
        input_cost_per_1k: 5e-05
        output_cost_per_1k: 0.00025
      'mistralai/mixtral-8x7b-instruct-v0.1':
        litellm_id: 'replicate/mistralai/mixtral-8x7b-instruct-v0.1'
        mode: chat
        max_input_tokens: 4096
        max_output_tokens: 4096
        input_cost_per_1k: 0.0003
        output_cost_per_1k: 0.001
  runwayml:
    model_count: 6
    models:
      'eleven_multilingual_v2':
        litellm_id: 'runwayml/eleven_multilingual_v2'
        mode: audio_speech
      'gen3a_turbo':
        litellm_id: 'runwayml/gen3a_turbo'
        mode: video_generation
      'gen4_aleph':
        litellm_id: 'runwayml/gen4_aleph'
        mode: video_generation
      'gen4_image':
        litellm_id: 'runwayml/gen4_image'
        mode: image
      'gen4_image_turbo':
        litellm_id: 'runwayml/gen4_image_turbo'
        mode: image
      'gen4_turbo':
        litellm_id: 'runwayml/gen4_turbo'
        mode: video_generation
  sambanova:
    model_count: 16
    models:
      'DeepSeek-R1':
        litellm_id: 'sambanova/DeepSeek-R1'
        mode: chat
        max_input_tokens: 32768
        max_output_tokens: 32768
        input_cost_per_1k: 0.005
        output_cost_per_1k: 0.007
      'DeepSeek-R1-Distill-Llama-70B':
        litellm_id: 'sambanova/DeepSeek-R1-Distill-Llama-70B'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.0007
        output_cost_per_1k: 0.0014
      'DeepSeek-V3-0324':
        litellm_id: 'sambanova/DeepSeek-V3-0324'
        mode: chat
        max_input_tokens: 32768
        max_output_tokens: 32768
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.0045
        capabilities: [function_calling, reasoning]
      'DeepSeek-V3.1':
        litellm_id: 'sambanova/DeepSeek-V3.1'
        mode: chat
        max_input_tokens: 32768
        max_output_tokens: 32768
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.0045
        capabilities: [function_calling, reasoning]
      'Llama-4-Maverick-17B-128E-Instruct':
        litellm_id: 'sambanova/Llama-4-Maverick-17B-128E-Instruct'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.00063
        output_cost_per_1k: 0.0018
        capabilities: [vision, function_calling, json_mode]
      'Llama-4-Scout-17B-16E-Instruct':
        litellm_id: 'sambanova/Llama-4-Scout-17B-16E-Instruct'
        mode: chat
        max_input_tokens: 8192
        max_output_tokens: 8192
        input_cost_per_1k: 0.0004
        output_cost_per_1k: 0.0007
        capabilities: [function_calling, json_mode]
      'Meta-Llama-3.1-405B-Instruct':
        litellm_id: 'sambanova/Meta-Llama-3.1-405B-Instruct'
        mode: chat
        max_input_tokens: 16384
        max_output_tokens: 16384
        input_cost_per_1k: 0.005
        output_cost_per_1k: 0.01
        capabilities: [function_calling, json_mode]
      'Meta-Llama-3.1-8B-Instruct':
        litellm_id: 'sambanova/Meta-Llama-3.1-8B-Instruct'
        mode: chat
        max_input_tokens: 16384
        max_output_tokens: 16384
        input_cost_per_1k: 0.0001
        output_cost_per_1k: 0.0002
        capabilities: [function_calling, json_mode]
      'Meta-Llama-3.2-1B-Instruct':
        litellm_id: 'sambanova/Meta-Llama-3.2-1B-Instruct'
        mode: chat
        max_input_tokens: 16384
        max_output_tokens: 16384
        input_cost_per_1k: 4e-05
        output_cost_per_1k: 8e-05
      'Meta-Llama-3.2-3B-Instruct':
        litellm_id: 'sambanova/Meta-Llama-3.2-3B-Instruct'
        mode: chat
        max_input_tokens: 4096
        max_output_tokens: 4096
        input_cost_per_1k: 8e-05
        output_cost_per_1k: 0.00016
      'Meta-Llama-3.3-70B-Instruct':
        litellm_id: 'sambanova/Meta-Llama-3.3-70B-Instruct'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.0006
        output_cost_per_1k: 0.0012
        capabilities: [function_calling, json_mode]
      'Meta-Llama-Guard-3-8B':
        litellm_id: 'sambanova/Meta-Llama-Guard-3-8B'
        mode: chat
        max_input_tokens: 16384
        max_output_tokens: 16384
        input_cost_per_1k: 0.0003
        output_cost_per_1k: 0.0003
      'QwQ-32B':
        litellm_id: 'sambanova/QwQ-32B'
        mode: chat
        max_input_tokens: 16384
        max_output_tokens: 16384
        input_cost_per_1k: 0.0005
        output_cost_per_1k: 0.001
      'Qwen2-Audio-7B-Instruct':
        litellm_id: 'sambanova/Qwen2-Audio-7B-Instruct'
        mode: chat
        max_input_tokens: 4096
        max_output_tokens: 4096
        input_cost_per_1k: 0.0005
        output_cost_per_1k: 0.1
        capabilities: [audio_input]
      'Qwen3-32B':
        litellm_id: 'sambanova/Qwen3-32B'
        mode: chat
        max_input_tokens: 8192
        max_output_tokens: 8192
        input_cost_per_1k: 0.0004
        output_cost_per_1k: 0.0008
        capabilities: [function_calling, reasoning]
      'gpt-oss-120b':
        litellm_id: 'sambanova/gpt-oss-120b'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.0045
        capabilities: [function_calling, reasoning]
  searxng:
    model_count: 1
    models:
      'search':
        litellm_id: 'searxng/search'
        mode: search
  snowflake:
    model_count: 24
    models:
      'claude-3-5-sonnet':
        litellm_id: 'snowflake/claude-3-5-sonnet'
        mode: chat
        max_input_tokens: 18000
        max_output_tokens: 8192
      'deepseek-r1':
        litellm_id: 'snowflake/deepseek-r1'
        mode: chat
        max_input_tokens: 32768
        max_output_tokens: 8192
        capabilities: [reasoning]
      'gemma-7b':
        litellm_id: 'snowflake/gemma-7b'
        mode: chat
        max_input_tokens: 8000
        max_output_tokens: 8192
      'jamba-1.5-large':
        litellm_id: 'snowflake/jamba-1.5-large'
        mode: chat
        max_input_tokens: 256000
        max_output_tokens: 8192
      'jamba-1.5-mini':
        litellm_id: 'snowflake/jamba-1.5-mini'
        mode: chat
        max_input_tokens: 256000
        max_output_tokens: 8192
      'jamba-instruct':
        litellm_id: 'snowflake/jamba-instruct'
        mode: chat
        max_input_tokens: 256000
        max_output_tokens: 8192
      'llama2-70b-chat':
        litellm_id: 'snowflake/llama2-70b-chat'
        mode: chat
        max_input_tokens: 4096
        max_output_tokens: 8192
      'llama3-70b':
        litellm_id: 'snowflake/llama3-70b'
        mode: chat
        max_input_tokens: 8000
        max_output_tokens: 8192
      'llama3-8b':
        litellm_id: 'snowflake/llama3-8b'
        mode: chat
        max_input_tokens: 8000
        max_output_tokens: 8192
      'llama3.1-405b':
        litellm_id: 'snowflake/llama3.1-405b'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 8192
      'llama3.1-70b':
        litellm_id: 'snowflake/llama3.1-70b'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 8192
      'llama3.1-8b':
        litellm_id: 'snowflake/llama3.1-8b'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 8192
      'llama3.2-1b':
        litellm_id: 'snowflake/llama3.2-1b'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 8192
      'llama3.2-3b':
        litellm_id: 'snowflake/llama3.2-3b'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 8192
      'llama3.3-70b':
        litellm_id: 'snowflake/llama3.3-70b'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 8192
      'mistral-7b':
        litellm_id: 'snowflake/mistral-7b'
        mode: chat
        max_input_tokens: 32000
        max_output_tokens: 8192
      'mistral-large':
        litellm_id: 'snowflake/mistral-large'
        mode: chat
        max_input_tokens: 32000
        max_output_tokens: 8192
      'mistral-large2':
        litellm_id: 'snowflake/mistral-large2'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 8192
      'mixtral-8x7b':
        litellm_id: 'snowflake/mixtral-8x7b'
        mode: chat
        max_input_tokens: 32000
        max_output_tokens: 8192
      'reka-core':
        litellm_id: 'snowflake/reka-core'
        mode: chat
        max_input_tokens: 32000
        max_output_tokens: 8192
      'reka-flash':
        litellm_id: 'snowflake/reka-flash'
        mode: chat
        max_input_tokens: 100000
        max_output_tokens: 8192
      'snowflake-arctic':
        litellm_id: 'snowflake/snowflake-arctic'
        mode: chat
        max_input_tokens: 4096
        max_output_tokens: 8192
      'snowflake-llama-3.1-405b':
        litellm_id: 'snowflake/snowflake-llama-3.1-405b'
        mode: chat
        max_input_tokens: 8000
        max_output_tokens: 8192
      'snowflake-llama-3.3-70b':
        litellm_id: 'snowflake/snowflake-llama-3.3-70b'
        mode: chat
        max_input_tokens: 8000
        max_output_tokens: 8192
  stability:
    model_count: 23
    models:
      'conservative':
        litellm_id: 'stability/conservative'
        mode: image_edit
      'creative':
        litellm_id: 'stability/creative'
        mode: image_edit
      'erase':
        litellm_id: 'stability/erase'
        mode: image_edit
      'fast':
        litellm_id: 'stability/fast'
        mode: image_edit
      'inpaint':
        litellm_id: 'stability/inpaint'
        mode: image_edit
      'outpaint':
        litellm_id: 'stability/outpaint'
        mode: image_edit
      'remove-background':
        litellm_id: 'stability/remove-background'
        mode: image_edit
      'replace-background-and-relight':
        litellm_id: 'stability/replace-background-and-relight'
        mode: image_edit
      'sd3':
        litellm_id: 'stability/sd3'
        mode: image
      'sd3-large':
        litellm_id: 'stability/sd3-large'
        mode: image
      'sd3-large-turbo':
        litellm_id: 'stability/sd3-large-turbo'
        mode: image
      'sd3-medium':
        litellm_id: 'stability/sd3-medium'
        mode: image
      'sd3.5-large':
        litellm_id: 'stability/sd3.5-large'
        mode: image
      'sd3.5-large-turbo':
        litellm_id: 'stability/sd3.5-large-turbo'
        mode: image
      'sd3.5-medium':
        litellm_id: 'stability/sd3.5-medium'
        mode: image
      'search-and-recolor':
        litellm_id: 'stability/search-and-recolor'
        mode: image_edit
      'search-and-replace':
        litellm_id: 'stability/search-and-replace'
        mode: image_edit
      'sketch':
        litellm_id: 'stability/sketch'
        mode: image_edit
      'stable-image-core':
        litellm_id: 'stability/stable-image-core'
        mode: image
      'stable-image-ultra':
        litellm_id: 'stability/stable-image-ultra'
        mode: image
      'structure':
        litellm_id: 'stability/structure'
        mode: image_edit
      'style':
        litellm_id: 'stability/style'
        mode: image_edit
      'style-transfer':
        litellm_id: 'stability/style-transfer'
        mode: image_edit
  tavily:
    model_count: 2
    models:
      'search':
        litellm_id: 'tavily/search'
        mode: search
      'search-advanced':
        litellm_id: 'tavily/search-advanced'
        mode: search
  text-completion-codestral:
    model_count: 2
    models:
      'codestral-2405':
        litellm_id: 'text-completion-codestral/codestral-2405'
        mode: completion
        max_input_tokens: 32000
        max_output_tokens: 8191
      'codestral-latest':
        litellm_id: 'text-completion-codestral/codestral-latest'
        mode: completion
        max_input_tokens: 32000
        max_output_tokens: 8191
  text-completion-openai:
    model_count: 6
    models:
      'babbage-002':
        mode: completion
        max_input_tokens: 16384
        max_output_tokens: 4096
        input_cost_per_1k: 0.0004
        output_cost_per_1k: 0.0004
      'davinci-002':
        mode: completion
        max_input_tokens: 16384
        max_output_tokens: 4096
        input_cost_per_1k: 0.002
        output_cost_per_1k: 0.002
      'ft:babbage-002':
        mode: completion
        max_input_tokens: 16384
        max_output_tokens: 4096
        input_cost_per_1k: 0.0016
        output_cost_per_1k: 0.0016
      'ft:davinci-002':
        mode: completion
        max_input_tokens: 16384
        max_output_tokens: 4096
        input_cost_per_1k: 0.012
        output_cost_per_1k: 0.012
      'gpt-3.5-turbo-instruct':
        mode: completion
        max_input_tokens: 8192
        max_output_tokens: 4096
        input_cost_per_1k: 0.0015
        output_cost_per_1k: 0.002
      'gpt-3.5-turbo-instruct-0914':
        mode: completion
        max_input_tokens: 8192
        max_output_tokens: 4097
        input_cost_per_1k: 0.0015
        output_cost_per_1k: 0.002
  together:
    model_count: 40
    models:
      'BAAI/bge-base-en-v1.5':
        litellm_id: 'together_ai/BAAI/bge-base-en-v1.5'
        mode: embedding
        max_input_tokens: 512
        input_cost_per_1k: 8e-06
      'Qwen/Qwen2.5-72B-Instruct-Turbo':
        litellm_id: 'together_ai/Qwen/Qwen2.5-72B-Instruct-Turbo'
        mode: chat
        capabilities: [function_calling, parallel_function_calling, json_mode]
      'Qwen/Qwen2.5-7B-Instruct-Turbo':
        litellm_id: 'together_ai/Qwen/Qwen2.5-7B-Instruct-Turbo'
        mode: chat
        capabilities: [function_calling, parallel_function_calling, json_mode]
      'Qwen/Qwen3-235B-A22B-Instruct-2507-tput':
        litellm_id: 'together_ai/Qwen/Qwen3-235B-A22B-Instruct-2507-tput'
        mode: chat
        max_input_tokens: 262000
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.006
        capabilities: [function_calling, parallel_function_calling, json_mode]
      'Qwen/Qwen3-235B-A22B-Thinking-2507':
        litellm_id: 'together_ai/Qwen/Qwen3-235B-A22B-Thinking-2507'
        mode: chat
        max_input_tokens: 256000
        input_cost_per_1k: 0.00065
        output_cost_per_1k: 0.003
        capabilities: [function_calling, parallel_function_calling, json_mode]
      'Qwen/Qwen3-235B-A22B-fp8-tput':
        litellm_id: 'together_ai/Qwen/Qwen3-235B-A22B-fp8-tput'
        mode: chat
        max_input_tokens: 40000
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0006
      'Qwen/Qwen3-Coder-480B-A35B-Instruct-FP8':
        litellm_id: 'together_ai/Qwen/Qwen3-Coder-480B-A35B-Instruct-FP8'
        mode: chat
        max_input_tokens: 256000
        input_cost_per_1k: 0.002
        output_cost_per_1k: 0.002
        capabilities: [function_calling, parallel_function_calling, json_mode]
      'Qwen/Qwen3-Next-80B-A3B-Instruct':
        litellm_id: 'together_ai/Qwen/Qwen3-Next-80B-A3B-Instruct'
        mode: chat
        max_input_tokens: 262144
        input_cost_per_1k: 0.00015
        output_cost_per_1k: 0.0015
        capabilities: [function_calling, parallel_function_calling, json_mode]
      'Qwen/Qwen3-Next-80B-A3B-Thinking':
        litellm_id: 'together_ai/Qwen/Qwen3-Next-80B-A3B-Thinking'
        mode: chat
        max_input_tokens: 262144
        input_cost_per_1k: 0.00015
        output_cost_per_1k: 0.0015
        capabilities: [function_calling, parallel_function_calling, json_mode]
      'baai/bge-base-en-v1.5':
        litellm_id: 'together_ai/baai/bge-base-en-v1.5'
        mode: embedding
        max_input_tokens: 512
        input_cost_per_1k: 8e-06
      'deepseek-ai/DeepSeek-R1':
        litellm_id: 'together_ai/deepseek-ai/DeepSeek-R1'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 20480
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.007
        capabilities: [function_calling, parallel_function_calling, json_mode]
      'deepseek-ai/DeepSeek-R1-0528-tput':
        litellm_id: 'together_ai/deepseek-ai/DeepSeek-R1-0528-tput'
        mode: chat
        max_input_tokens: 128000
        input_cost_per_1k: 0.00055
        output_cost_per_1k: 0.00219
        capabilities: [function_calling, parallel_function_calling, json_mode]
      'deepseek-ai/DeepSeek-V3':
        litellm_id: 'together_ai/deepseek-ai/DeepSeek-V3'
        mode: chat
        max_input_tokens: 65536
        max_output_tokens: 8192
        input_cost_per_1k: 0.00125
        output_cost_per_1k: 0.00125
        capabilities: [function_calling, parallel_function_calling, json_mode]
      'deepseek-ai/DeepSeek-V3.1':
        litellm_id: 'together_ai/deepseek-ai/DeepSeek-V3.1'
        mode: chat
        max_input_tokens: 128000
        input_cost_per_1k: 0.0006
        output_cost_per_1k: 0.0017
        capabilities: [function_calling, parallel_function_calling, reasoning]
      'meta-llama/Llama-3.2-3B-Instruct-Turbo':
        litellm_id: 'together_ai/meta-llama/Llama-3.2-3B-Instruct-Turbo'
        mode: chat
        capabilities: [function_calling, parallel_function_calling, json_mode]
      'meta-llama/Llama-3.3-70B-Instruct-Turbo':
        litellm_id: 'together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo'
        mode: chat
        input_cost_per_1k: 0.00088
        output_cost_per_1k: 0.00088
        capabilities: [function_calling, parallel_function_calling, json_mode]
      'meta-llama/Llama-3.3-70B-Instruct-Turbo-Free':
        litellm_id: 'together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo-Free'
        mode: chat
        capabilities: [function_calling, parallel_function_calling, json_mode]
      'meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8':
        litellm_id: 'together_ai/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8'
        mode: chat
        input_cost_per_1k: 0.00027
        output_cost_per_1k: 0.00085
        capabilities: [function_calling, parallel_function_calling, json_mode]
      'meta-llama/Llama-4-Scout-17B-16E-Instruct':
        litellm_id: 'together_ai/meta-llama/Llama-4-Scout-17B-16E-Instruct'
        mode: chat
        input_cost_per_1k: 0.00018
        output_cost_per_1k: 0.00059
        capabilities: [function_calling, parallel_function_calling, json_mode]
      'meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo':
        litellm_id: 'together_ai/meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo'
        mode: chat
        input_cost_per_1k: 0.0035
        output_cost_per_1k: 0.0035
        capabilities: [function_calling, parallel_function_calling, json_mode]
      'meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo':
        litellm_id: 'together_ai/meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo'
        mode: chat
        input_cost_per_1k: 0.00088
        output_cost_per_1k: 0.00088
        capabilities: [function_calling, parallel_function_calling, json_mode]
      'meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo':
        litellm_id: 'together_ai/meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo'
        mode: chat
        input_cost_per_1k: 0.00018
        output_cost_per_1k: 0.00018
        capabilities: [function_calling, parallel_function_calling, json_mode]
      'mistralai/Mistral-7B-Instruct-v0.1':
        litellm_id: 'together_ai/mistralai/Mistral-7B-Instruct-v0.1'
        mode: chat
        capabilities: [function_calling, parallel_function_calling, json_mode]
      'mistralai/Mistral-Small-24B-Instruct-2501':
        litellm_id: 'together_ai/mistralai/Mistral-Small-24B-Instruct-2501'
        mode: chat
        capabilities: [function_calling, parallel_function_calling]
      'mistralai/Mixtral-8x7B-Instruct-v0.1':
        litellm_id: 'together_ai/mistralai/Mixtral-8x7B-Instruct-v0.1'
        mode: chat
        input_cost_per_1k: 0.0006
        output_cost_per_1k: 0.0006
        capabilities: [function_calling, parallel_function_calling, json_mode]
      'moonshotai/Kimi-K2-Instruct':
        litellm_id: 'together_ai/moonshotai/Kimi-K2-Instruct'
        mode: chat
        input_cost_per_1k: 0.001
        output_cost_per_1k: 0.003
        capabilities: [function_calling, parallel_function_calling, json_mode]
      'moonshotai/Kimi-K2-Instruct-0905':
        litellm_id: 'together_ai/moonshotai/Kimi-K2-Instruct-0905'
        mode: chat
        max_input_tokens: 262144
        input_cost_per_1k: 0.001
        output_cost_per_1k: 0.003
        capabilities: [function_calling, parallel_function_calling]
      'openai/gpt-oss-120b':
        litellm_id: 'together_ai/openai/gpt-oss-120b'
        mode: chat
        max_input_tokens: 128000
        input_cost_per_1k: 0.00015
        output_cost_per_1k: 0.0006
        capabilities: [function_calling, parallel_function_calling, json_mode]
      'openai/gpt-oss-20b':
        litellm_id: 'together_ai/openai/gpt-oss-20b'
        mode: chat
        max_input_tokens: 128000
        input_cost_per_1k: 5e-05
        output_cost_per_1k: 0.0002
        capabilities: [function_calling, parallel_function_calling, json_mode]
      'together-ai-21.1b-41b':
        mode: chat
        input_cost_per_1k: 0.0008
        output_cost_per_1k: 0.0008
      'together-ai-4.1b-8b':
        mode: chat
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
      'together-ai-41.1b-80b':
        mode: chat
        input_cost_per_1k: 0.0009
        output_cost_per_1k: 0.0009
      'together-ai-8.1b-21b':
        mode: chat
        max_input_tokens: 1000
        input_cost_per_1k: 0.0003
        output_cost_per_1k: 0.0003
      'together-ai-81.1b-110b':
        mode: chat
        input_cost_per_1k: 0.0018
        output_cost_per_1k: 0.0018
      'together-ai-embedding-151m-to-350m':
        mode: embedding
        input_cost_per_1k: 1.6e-05
      'together-ai-embedding-up-to-150m':
        mode: embedding
        input_cost_per_1k: 8e-06
      'together-ai-up-to-4b':
        mode: chat
        input_cost_per_1k: 0.0001
        output_cost_per_1k: 0.0001
      'togethercomputer/CodeLlama-34b-Instruct':
        litellm_id: 'together_ai/togethercomputer/CodeLlama-34b-Instruct'
        mode: chat
        capabilities: [function_calling, parallel_function_calling]
      'zai-org/GLM-4.5-Air-FP8':
        litellm_id: 'together_ai/zai-org/GLM-4.5-Air-FP8'
        mode: chat
        max_input_tokens: 128000
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0011
        capabilities: [function_calling, parallel_function_calling, json_mode]
      'zai-org/GLM-4.6':
        litellm_id: 'together_ai/zai-org/GLM-4.6'
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 200000
        input_cost_per_1k: 0.0006
        output_cost_per_1k: 0.0022
        capabilities: [function_calling, parallel_function_calling, reasoning]
  v0:
    model_count: 3
    models:
      'v0-1.0-md':
        litellm_id: 'v0/v0-1.0-md'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 128000
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.015
        capabilities: [vision, function_calling, parallel_function_calling, system_messages]
      'v0-1.5-lg':
        litellm_id: 'v0/v0-1.5-lg'
        mode: chat
        max_input_tokens: 512000
        max_output_tokens: 512000
        input_cost_per_1k: 0.015
        output_cost_per_1k: 0.075
        capabilities: [vision, function_calling, parallel_function_calling, system_messages]
      'v0-1.5-md':
        litellm_id: 'v0/v0-1.5-md'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 128000
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.015
        capabilities: [vision, function_calling, parallel_function_calling, system_messages]
  vercel_ai_gateway:
    model_count: 91
    models:
      'alibaba/qwen-3-14b':
        litellm_id: 'vercel_ai_gateway/alibaba/qwen-3-14b'
        mode: chat
        max_input_tokens: 40960
        max_output_tokens: 16384
        input_cost_per_1k: 8e-05
        output_cost_per_1k: 0.00024
      'alibaba/qwen-3-235b':
        litellm_id: 'vercel_ai_gateway/alibaba/qwen-3-235b'
        mode: chat
        max_input_tokens: 40960
        max_output_tokens: 16384
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0006
      'alibaba/qwen-3-30b':
        litellm_id: 'vercel_ai_gateway/alibaba/qwen-3-30b'
        mode: chat
        max_input_tokens: 40960
        max_output_tokens: 16384
        input_cost_per_1k: 0.0001
        output_cost_per_1k: 0.0003
      'alibaba/qwen-3-32b':
        litellm_id: 'vercel_ai_gateway/alibaba/qwen-3-32b'
        mode: chat
        max_input_tokens: 40960
        max_output_tokens: 16384
        input_cost_per_1k: 0.0001
        output_cost_per_1k: 0.0003
      'alibaba/qwen3-coder':
        litellm_id: 'vercel_ai_gateway/alibaba/qwen3-coder'
        mode: chat
        max_input_tokens: 262144
        max_output_tokens: 66536
        input_cost_per_1k: 0.0004
        output_cost_per_1k: 0.0016
      'amazon/nova-lite':
        litellm_id: 'vercel_ai_gateway/amazon/nova-lite'
        mode: chat
        max_input_tokens: 300000
        max_output_tokens: 8192
        input_cost_per_1k: 6e-05
        output_cost_per_1k: 0.00024
      'amazon/nova-micro':
        litellm_id: 'vercel_ai_gateway/amazon/nova-micro'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 8192
        input_cost_per_1k: 3.5e-05
        output_cost_per_1k: 0.00014
      'amazon/nova-pro':
        litellm_id: 'vercel_ai_gateway/amazon/nova-pro'
        mode: chat
        max_input_tokens: 300000
        max_output_tokens: 8192
        input_cost_per_1k: 0.0008
        output_cost_per_1k: 0.0032
      'amazon/titan-embed-text-v2':
        litellm_id: 'vercel_ai_gateway/amazon/titan-embed-text-v2'
        mode: chat
        max_input_tokens: 0
        max_output_tokens: 0
        input_cost_per_1k: 2e-05
      'anthropic/claude-3-haiku':
        litellm_id: 'vercel_ai_gateway/anthropic/claude-3-haiku'
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 4096
        input_cost_per_1k: 0.00025
        output_cost_per_1k: 0.00125
      'anthropic/claude-3-opus':
        litellm_id: 'vercel_ai_gateway/anthropic/claude-3-opus'
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 4096
        input_cost_per_1k: 0.015
        output_cost_per_1k: 0.075
      'anthropic/claude-3.5-haiku':
        litellm_id: 'vercel_ai_gateway/anthropic/claude-3.5-haiku'
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 8192
        input_cost_per_1k: 0.0008
        output_cost_per_1k: 0.004
      'anthropic/claude-3.5-sonnet':
        litellm_id: 'vercel_ai_gateway/anthropic/claude-3.5-sonnet'
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 8192
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.015
      'anthropic/claude-3.7-sonnet':
        litellm_id: 'vercel_ai_gateway/anthropic/claude-3.7-sonnet'
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 64000
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.015
      'anthropic/claude-4-opus':
        litellm_id: 'vercel_ai_gateway/anthropic/claude-4-opus'
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 32000
        input_cost_per_1k: 0.015
        output_cost_per_1k: 0.075
      'anthropic/claude-4-sonnet':
        litellm_id: 'vercel_ai_gateway/anthropic/claude-4-sonnet'
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 64000
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.015
      'cohere/command-a':
        litellm_id: 'vercel_ai_gateway/cohere/command-a'
        mode: chat
        max_input_tokens: 256000
        max_output_tokens: 8000
        input_cost_per_1k: 0.0025
        output_cost_per_1k: 0.01
      'cohere/command-r':
        litellm_id: 'vercel_ai_gateway/cohere/command-r'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 4096
        input_cost_per_1k: 0.00015
        output_cost_per_1k: 0.0006
      'cohere/command-r-plus':
        litellm_id: 'vercel_ai_gateway/cohere/command-r-plus'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 4096
        input_cost_per_1k: 0.0025
        output_cost_per_1k: 0.01
      'cohere/embed-v4.0':
        litellm_id: 'vercel_ai_gateway/cohere/embed-v4.0'
        mode: chat
        max_input_tokens: 0
        max_output_tokens: 0
        input_cost_per_1k: 0.00012
      'deepseek/deepseek-r1':
        litellm_id: 'vercel_ai_gateway/deepseek/deepseek-r1'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 8192
        input_cost_per_1k: 0.00055
        output_cost_per_1k: 0.00219
      'deepseek/deepseek-r1-distill-llama-70b':
        litellm_id: 'vercel_ai_gateway/deepseek/deepseek-r1-distill-llama-70b'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.00075
        output_cost_per_1k: 0.00099
      'deepseek/deepseek-v3':
        litellm_id: 'vercel_ai_gateway/deepseek/deepseek-v3'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 8192
        input_cost_per_1k: 0.0009
        output_cost_per_1k: 0.0009
      'google/gemini-2.0-flash':
        litellm_id: 'vercel_ai_gateway/google/gemini-2.0-flash'
        mode: chat
        max_input_tokens: 1048576
        max_output_tokens: 8192
        input_cost_per_1k: 0.00015
        output_cost_per_1k: 0.0006
      'google/gemini-2.0-flash-lite':
        litellm_id: 'vercel_ai_gateway/google/gemini-2.0-flash-lite'
        mode: chat
        max_input_tokens: 1048576
        max_output_tokens: 8192
        input_cost_per_1k: 7.5e-05
        output_cost_per_1k: 0.0003
      'google/gemini-2.5-flash':
        litellm_id: 'vercel_ai_gateway/google/gemini-2.5-flash'
        mode: chat
        max_input_tokens: 1000000
        max_output_tokens: 65536
        input_cost_per_1k: 0.0003
        output_cost_per_1k: 0.0025
      'google/gemini-2.5-pro':
        litellm_id: 'vercel_ai_gateway/google/gemini-2.5-pro'
        mode: chat
        max_input_tokens: 1048576
        max_output_tokens: 65536
        input_cost_per_1k: 0.0025
        output_cost_per_1k: 0.01
      'google/gemini-embedding-001':
        litellm_id: 'vercel_ai_gateway/google/gemini-embedding-001'
        mode: embedding
        max_input_tokens: 0
        max_output_tokens: 0
        input_cost_per_1k: 0.00015
      'google/gemma-2-9b':
        litellm_id: 'vercel_ai_gateway/google/gemma-2-9b'
        mode: chat
        max_input_tokens: 8192
        max_output_tokens: 8192
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
      'google/text-embedding-005':
        litellm_id: 'vercel_ai_gateway/google/text-embedding-005'
        mode: embedding
        max_input_tokens: 0
        max_output_tokens: 0
        input_cost_per_1k: 2.5e-05
      'google/text-multilingual-embedding-002':
        litellm_id: 'vercel_ai_gateway/google/text-multilingual-embedding-002'
        mode: embedding
        max_input_tokens: 0
        max_output_tokens: 0
        input_cost_per_1k: 2.5e-05
      'inception/mercury-coder-small':
        litellm_id: 'vercel_ai_gateway/inception/mercury-coder-small'
        mode: chat
        max_input_tokens: 32000
        max_output_tokens: 16384
        input_cost_per_1k: 0.00025
        output_cost_per_1k: 0.001
      'meta/llama-3-70b':
        litellm_id: 'vercel_ai_gateway/meta/llama-3-70b'
        mode: chat
        max_input_tokens: 8192
        max_output_tokens: 8192
        input_cost_per_1k: 0.00059
        output_cost_per_1k: 0.00079
      'meta/llama-3-8b':
        litellm_id: 'vercel_ai_gateway/meta/llama-3-8b'
        mode: chat
        max_input_tokens: 8192
        max_output_tokens: 8192
        input_cost_per_1k: 5e-05
        output_cost_per_1k: 8e-05
      'meta/llama-3.1-70b':
        litellm_id: 'vercel_ai_gateway/meta/llama-3.1-70b'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 8192
        input_cost_per_1k: 0.00072
        output_cost_per_1k: 0.00072
      'meta/llama-3.1-8b':
        litellm_id: 'vercel_ai_gateway/meta/llama-3.1-8b'
        mode: chat
        max_input_tokens: 131000
        max_output_tokens: 131072
        input_cost_per_1k: 5e-05
        output_cost_per_1k: 8e-05
      'meta/llama-3.2-11b':
        litellm_id: 'vercel_ai_gateway/meta/llama-3.2-11b'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 8192
        input_cost_per_1k: 0.00016
        output_cost_per_1k: 0.00016
      'meta/llama-3.2-1b':
        litellm_id: 'vercel_ai_gateway/meta/llama-3.2-1b'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 8192
        input_cost_per_1k: 0.0001
        output_cost_per_1k: 0.0001
      'meta/llama-3.2-3b':
        litellm_id: 'vercel_ai_gateway/meta/llama-3.2-3b'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 8192
        input_cost_per_1k: 0.00015
        output_cost_per_1k: 0.00015
      'meta/llama-3.2-90b':
        litellm_id: 'vercel_ai_gateway/meta/llama-3.2-90b'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 8192
        input_cost_per_1k: 0.00072
        output_cost_per_1k: 0.00072
      'meta/llama-3.3-70b':
        litellm_id: 'vercel_ai_gateway/meta/llama-3.3-70b'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 8192
        input_cost_per_1k: 0.00072
        output_cost_per_1k: 0.00072
      'meta/llama-4-maverick':
        litellm_id: 'vercel_ai_gateway/meta/llama-4-maverick'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 8192
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0006
      'meta/llama-4-scout':
        litellm_id: 'vercel_ai_gateway/meta/llama-4-scout'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 8192
        input_cost_per_1k: 0.0001
        output_cost_per_1k: 0.0003
      'mistral/codestral':
        litellm_id: 'vercel_ai_gateway/mistral/codestral'
        mode: chat
        max_input_tokens: 256000
        max_output_tokens: 4000
        input_cost_per_1k: 0.0003
        output_cost_per_1k: 0.0009
      'mistral/codestral-embed':
        litellm_id: 'vercel_ai_gateway/mistral/codestral-embed'
        mode: chat
        max_input_tokens: 0
        max_output_tokens: 0
        input_cost_per_1k: 0.00015
      'mistral/devstral-small':
        litellm_id: 'vercel_ai_gateway/mistral/devstral-small'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 128000
        input_cost_per_1k: 7e-05
        output_cost_per_1k: 0.00028
      'mistral/magistral-medium':
        litellm_id: 'vercel_ai_gateway/mistral/magistral-medium'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 64000
        input_cost_per_1k: 0.002
        output_cost_per_1k: 0.005
      'mistral/magistral-small':
        litellm_id: 'vercel_ai_gateway/mistral/magistral-small'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 64000
        input_cost_per_1k: 0.0005
        output_cost_per_1k: 0.0015
      'mistral/ministral-3b':
        litellm_id: 'vercel_ai_gateway/mistral/ministral-3b'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 4000
        input_cost_per_1k: 4e-05
        output_cost_per_1k: 4e-05
      'mistral/ministral-8b':
        litellm_id: 'vercel_ai_gateway/mistral/ministral-8b'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 4000
        input_cost_per_1k: 0.0001
        output_cost_per_1k: 0.0001
      'mistral/mistral-embed':
        litellm_id: 'vercel_ai_gateway/mistral/mistral-embed'
        mode: chat
        max_input_tokens: 0
        max_output_tokens: 0
        input_cost_per_1k: 0.0001
      'mistral/mistral-large':
        litellm_id: 'vercel_ai_gateway/mistral/mistral-large'
        mode: chat
        max_input_tokens: 32000
        max_output_tokens: 4000
        input_cost_per_1k: 0.002
        output_cost_per_1k: 0.006
      'mistral/mistral-saba-24b':
        litellm_id: 'vercel_ai_gateway/mistral/mistral-saba-24b'
        mode: chat
        max_input_tokens: 32768
        max_output_tokens: 32768
        input_cost_per_1k: 0.00079
        output_cost_per_1k: 0.00079
      'mistral/mistral-small':
        litellm_id: 'vercel_ai_gateway/mistral/mistral-small'
        mode: chat
        max_input_tokens: 32000
        max_output_tokens: 4000
        input_cost_per_1k: 0.0001
        output_cost_per_1k: 0.0003
      'mistral/mixtral-8x22b-instruct':
        litellm_id: 'vercel_ai_gateway/mistral/mixtral-8x22b-instruct'
        mode: chat
        max_input_tokens: 65536
        max_output_tokens: 2048
        input_cost_per_1k: 0.0012
        output_cost_per_1k: 0.0012
      'mistral/pixtral-12b':
        litellm_id: 'vercel_ai_gateway/mistral/pixtral-12b'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 4000
        input_cost_per_1k: 0.00015
        output_cost_per_1k: 0.00015
      'mistral/pixtral-large':
        litellm_id: 'vercel_ai_gateway/mistral/pixtral-large'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 4000
        input_cost_per_1k: 0.002
        output_cost_per_1k: 0.006
      'moonshotai/kimi-k2':
        litellm_id: 'vercel_ai_gateway/moonshotai/kimi-k2'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 16384
        input_cost_per_1k: 0.00055
        output_cost_per_1k: 0.0022
      'morph/morph-v3-fast':
        litellm_id: 'vercel_ai_gateway/morph/morph-v3-fast'
        mode: chat
        max_input_tokens: 32768
        max_output_tokens: 16384
        input_cost_per_1k: 0.0008
        output_cost_per_1k: 0.0012
      'morph/morph-v3-large':
        litellm_id: 'vercel_ai_gateway/morph/morph-v3-large'
        mode: chat
        max_input_tokens: 32768
        max_output_tokens: 16384
        input_cost_per_1k: 0.0009
        output_cost_per_1k: 0.0019
      'openai/gpt-3.5-turbo':
        litellm_id: 'vercel_ai_gateway/openai/gpt-3.5-turbo'
        mode: chat
        max_input_tokens: 16385
        max_output_tokens: 4096
        input_cost_per_1k: 0.0005
        output_cost_per_1k: 0.0015
      'openai/gpt-3.5-turbo-instruct':
        litellm_id: 'vercel_ai_gateway/openai/gpt-3.5-turbo-instruct'
        mode: chat
        max_input_tokens: 8192
        max_output_tokens: 4096
        input_cost_per_1k: 0.0015
        output_cost_per_1k: 0.002
      'openai/gpt-4-turbo':
        litellm_id: 'vercel_ai_gateway/openai/gpt-4-turbo'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 4096
        input_cost_per_1k: 0.01
        output_cost_per_1k: 0.03
      'openai/gpt-4.1':
        litellm_id: 'vercel_ai_gateway/openai/gpt-4.1'
        mode: chat
        max_input_tokens: 1047576
        max_output_tokens: 32768
        input_cost_per_1k: 0.002
        output_cost_per_1k: 0.008
      'openai/gpt-4.1-mini':
        litellm_id: 'vercel_ai_gateway/openai/gpt-4.1-mini'
        mode: chat
        max_input_tokens: 1047576
        max_output_tokens: 32768
        input_cost_per_1k: 0.0004
        output_cost_per_1k: 0.0016
      'openai/gpt-4.1-nano':
        litellm_id: 'vercel_ai_gateway/openai/gpt-4.1-nano'
        mode: chat
        max_input_tokens: 1047576
        max_output_tokens: 32768
        input_cost_per_1k: 0.0001
        output_cost_per_1k: 0.0004
      'openai/gpt-4o':
        litellm_id: 'vercel_ai_gateway/openai/gpt-4o'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 16384
        input_cost_per_1k: 0.0025
        output_cost_per_1k: 0.01
      'openai/gpt-4o-mini':
        litellm_id: 'vercel_ai_gateway/openai/gpt-4o-mini'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 16384
        input_cost_per_1k: 0.00015
        output_cost_per_1k: 0.0006
      'openai/o1':
        litellm_id: 'vercel_ai_gateway/openai/o1'
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 100000
        input_cost_per_1k: 0.015
        output_cost_per_1k: 0.06
      'openai/o3':
        litellm_id: 'vercel_ai_gateway/openai/o3'
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 100000
        input_cost_per_1k: 0.002
        output_cost_per_1k: 0.008
      'openai/o3-mini':
        litellm_id: 'vercel_ai_gateway/openai/o3-mini'
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 100000
        input_cost_per_1k: 0.0011
        output_cost_per_1k: 0.0044
      'openai/o4-mini':
        litellm_id: 'vercel_ai_gateway/openai/o4-mini'
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 100000
        input_cost_per_1k: 0.0011
        output_cost_per_1k: 0.0044
      'openai/text-embedding-3-large':
        litellm_id: 'vercel_ai_gateway/openai/text-embedding-3-large'
        mode: embedding
        max_input_tokens: 0
        max_output_tokens: 0
        input_cost_per_1k: 0.00013
      'openai/text-embedding-3-small':
        litellm_id: 'vercel_ai_gateway/openai/text-embedding-3-small'
        mode: embedding
        max_input_tokens: 0
        max_output_tokens: 0
        input_cost_per_1k: 2e-05
      'openai/text-embedding-ada-002':
        litellm_id: 'vercel_ai_gateway/openai/text-embedding-ada-002'
        mode: embedding
        max_input_tokens: 0
        max_output_tokens: 0
        input_cost_per_1k: 0.0001
      'perplexity/sonar':
        litellm_id: 'vercel_ai_gateway/perplexity/sonar'
        mode: chat
        max_input_tokens: 127000
        max_output_tokens: 8000
        input_cost_per_1k: 0.001
        output_cost_per_1k: 0.001
      'perplexity/sonar-pro':
        litellm_id: 'vercel_ai_gateway/perplexity/sonar-pro'
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 8000
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.015
      'perplexity/sonar-reasoning':
        litellm_id: 'vercel_ai_gateway/perplexity/sonar-reasoning'
        mode: chat
        max_input_tokens: 127000
        max_output_tokens: 8000
        input_cost_per_1k: 0.001
        output_cost_per_1k: 0.005
      'perplexity/sonar-reasoning-pro':
        litellm_id: 'vercel_ai_gateway/perplexity/sonar-reasoning-pro'
        mode: chat
        max_input_tokens: 127000
        max_output_tokens: 8000
        input_cost_per_1k: 0.002
        output_cost_per_1k: 0.008
      'vercel/v0-1.0-md':
        litellm_id: 'vercel_ai_gateway/vercel/v0-1.0-md'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 32000
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.015
      'vercel/v0-1.5-md':
        litellm_id: 'vercel_ai_gateway/vercel/v0-1.5-md'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 32768
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.015
      'xai/grok-2':
        litellm_id: 'vercel_ai_gateway/xai/grok-2'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 4000
        input_cost_per_1k: 0.002
        output_cost_per_1k: 0.01
      'xai/grok-2-vision':
        litellm_id: 'vercel_ai_gateway/xai/grok-2-vision'
        mode: chat
        max_input_tokens: 32768
        max_output_tokens: 32768
        input_cost_per_1k: 0.002
        output_cost_per_1k: 0.01
      'xai/grok-3':
        litellm_id: 'vercel_ai_gateway/xai/grok-3'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.015
      'xai/grok-3-fast':
        litellm_id: 'vercel_ai_gateway/xai/grok-3-fast'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.005
        output_cost_per_1k: 0.025
      'xai/grok-3-mini':
        litellm_id: 'vercel_ai_gateway/xai/grok-3-mini'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.0003
        output_cost_per_1k: 0.0005
      'xai/grok-3-mini-fast':
        litellm_id: 'vercel_ai_gateway/xai/grok-3-mini-fast'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.0006
        output_cost_per_1k: 0.004
      'xai/grok-4':
        litellm_id: 'vercel_ai_gateway/xai/grok-4'
        mode: chat
        max_input_tokens: 256000
        max_output_tokens: 256000
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.015
      'zai/glm-4.5':
        litellm_id: 'vercel_ai_gateway/zai/glm-4.5'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.0006
        output_cost_per_1k: 0.0022
      'zai/glm-4.5-air':
        litellm_id: 'vercel_ai_gateway/zai/glm-4.5-air'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 96000
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0011
      'zai/glm-4.6':
        litellm_id: 'vercel_ai_gateway/zai/glm-4.6'
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 200000
        input_cost_per_1k: 0.00045
        output_cost_per_1k: 0.0018
        capabilities: [function_calling, parallel_function_calling]
  vertex_ai-ai21_models:
    model_count: 5
    models:
      'vertex_ai/jamba-1.5':
        mode: chat
        max_input_tokens: 256000
        max_output_tokens: 256000
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0004
      'vertex_ai/jamba-1.5-large':
        mode: chat
        max_input_tokens: 256000
        max_output_tokens: 256000
        input_cost_per_1k: 0.002
        output_cost_per_1k: 0.008
      'vertex_ai/jamba-1.5-large@001':
        mode: chat
        max_input_tokens: 256000
        max_output_tokens: 256000
        input_cost_per_1k: 0.002
        output_cost_per_1k: 0.008
      'vertex_ai/jamba-1.5-mini':
        mode: chat
        max_input_tokens: 256000
        max_output_tokens: 256000
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0004
      'vertex_ai/jamba-1.5-mini@001':
        mode: chat
        max_input_tokens: 256000
        max_output_tokens: 256000
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0004
  vertex_ai-anthropic_models:
    model_count: 24
    models:
      'vertex_ai/claude-3-5-haiku':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 8192
        input_cost_per_1k: 0.001
        output_cost_per_1k: 0.005
        capabilities: [function_calling]
      'vertex_ai/claude-3-5-haiku@20241022':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 8192
        input_cost_per_1k: 0.001
        output_cost_per_1k: 0.005
        capabilities: [function_calling]
      'vertex_ai/claude-3-5-sonnet':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 8192
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.015
        capabilities: [vision, function_calling]
      'vertex_ai/claude-3-5-sonnet-v2':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 8192
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.015
        capabilities: [vision, function_calling]
      'vertex_ai/claude-3-5-sonnet-v2@20241022':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 8192
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.015
        capabilities: [vision, function_calling]
      'vertex_ai/claude-3-5-sonnet@20240620':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 8192
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.015
        capabilities: [vision, function_calling]
      'vertex_ai/claude-3-7-sonnet@20250219':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 8192
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.015
        capabilities: [vision, function_calling, json_mode, prompt_caching, reasoning]
        deprecated: true
        deprecation_date: '2025-06-01'
      'vertex_ai/claude-3-haiku':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 4096
        input_cost_per_1k: 0.00025
        output_cost_per_1k: 0.00125
        capabilities: [vision, function_calling]
      'vertex_ai/claude-3-haiku@20240307':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 4096
        input_cost_per_1k: 0.00025
        output_cost_per_1k: 0.00125
        capabilities: [vision, function_calling]
      'vertex_ai/claude-3-opus':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 4096
        input_cost_per_1k: 0.015
        output_cost_per_1k: 0.075
        capabilities: [vision, function_calling]
      'vertex_ai/claude-3-opus@20240229':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 4096
        input_cost_per_1k: 0.015
        output_cost_per_1k: 0.075
        capabilities: [vision, function_calling]
      'vertex_ai/claude-3-sonnet':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 4096
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.015
        capabilities: [vision, function_calling]
      'vertex_ai/claude-3-sonnet@20240229':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 4096
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.015
        capabilities: [vision, function_calling]
      'vertex_ai/claude-haiku-4-5@20251001':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 8192
        input_cost_per_1k: 0.001
        output_cost_per_1k: 0.005
        capabilities: [function_calling, json_mode, prompt_caching, reasoning]
      'vertex_ai/claude-opus-4':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 32000
        input_cost_per_1k: 0.015
        output_cost_per_1k: 0.075
        capabilities: [vision, function_calling, json_mode, prompt_caching, reasoning]
      'vertex_ai/claude-opus-4-1':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 32000
        input_cost_per_1k: 0.015
        output_cost_per_1k: 0.075
        capabilities: [vision, function_calling]
      'vertex_ai/claude-opus-4-1@20250805':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 32000
        input_cost_per_1k: 0.015
        output_cost_per_1k: 0.075
        capabilities: [vision, function_calling]
      'vertex_ai/claude-opus-4-5':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 64000
        input_cost_per_1k: 0.005
        output_cost_per_1k: 0.025
        capabilities: [vision, function_calling, json_mode, prompt_caching, reasoning]
      'vertex_ai/claude-opus-4-5@20251101':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 64000
        input_cost_per_1k: 0.005
        output_cost_per_1k: 0.025
        capabilities: [vision, function_calling, json_mode, prompt_caching, reasoning]
      'vertex_ai/claude-opus-4@20250514':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 32000
        input_cost_per_1k: 0.015
        output_cost_per_1k: 0.075
        capabilities: [vision, function_calling, json_mode, prompt_caching, reasoning]
      'vertex_ai/claude-sonnet-4':
        mode: chat
        max_input_tokens: 1000000
        max_output_tokens: 64000
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.015
        capabilities: [vision, function_calling, json_mode, prompt_caching, reasoning]
      'vertex_ai/claude-sonnet-4-5':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 64000
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.015
        capabilities: [vision, function_calling, json_mode, prompt_caching, reasoning]
      'vertex_ai/claude-sonnet-4-5@20250929':
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 64000
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.015
        capabilities: [vision, function_calling, json_mode, prompt_caching, reasoning]
      'vertex_ai/claude-sonnet-4@20250514':
        mode: chat
        max_input_tokens: 1000000
        max_output_tokens: 64000
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.015
        capabilities: [vision, function_calling, json_mode, prompt_caching, reasoning]
  vertex_ai-chat-models:
    model_count: 5
    models:
      'chat-bison':
        mode: chat
        max_input_tokens: 8192
        max_output_tokens: 4096
        input_cost_per_1k: 0.000125
        output_cost_per_1k: 0.000125
      'chat-bison-32k':
        mode: chat
        max_input_tokens: 32000
        max_output_tokens: 8192
        input_cost_per_1k: 0.000125
        output_cost_per_1k: 0.000125
      'chat-bison-32k@002':
        mode: chat
        max_input_tokens: 32000
        max_output_tokens: 8192
        input_cost_per_1k: 0.000125
        output_cost_per_1k: 0.000125
      'chat-bison@001':
        mode: chat
        max_input_tokens: 8192
        max_output_tokens: 4096
        input_cost_per_1k: 0.000125
        output_cost_per_1k: 0.000125
      'chat-bison@002':
        mode: chat
        max_input_tokens: 8192
        max_output_tokens: 4096
        input_cost_per_1k: 0.000125
        output_cost_per_1k: 0.000125
        deprecated: true
        deprecation_date: '2025-04-09'
  vertex_ai-code-chat-models:
    model_count: 6
    models:
      'codechat-bison':
        mode: chat
        max_input_tokens: 6144
        max_output_tokens: 1024
        input_cost_per_1k: 0.000125
        output_cost_per_1k: 0.000125
      'codechat-bison-32k':
        mode: chat
        max_input_tokens: 32000
        max_output_tokens: 8192
        input_cost_per_1k: 0.000125
        output_cost_per_1k: 0.000125
      'codechat-bison-32k@002':
        mode: chat
        max_input_tokens: 32000
        max_output_tokens: 8192
        input_cost_per_1k: 0.000125
        output_cost_per_1k: 0.000125
      'codechat-bison@001':
        mode: chat
        max_input_tokens: 6144
        max_output_tokens: 1024
        input_cost_per_1k: 0.000125
        output_cost_per_1k: 0.000125
      'codechat-bison@002':
        mode: chat
        max_input_tokens: 6144
        max_output_tokens: 1024
        input_cost_per_1k: 0.000125
        output_cost_per_1k: 0.000125
      'codechat-bison@latest':
        mode: chat
        max_input_tokens: 6144
        max_output_tokens: 1024
        input_cost_per_1k: 0.000125
        output_cost_per_1k: 0.000125
  vertex_ai-code-text-models:
    model_count: 9
    models:
      'code-bison':
        mode: chat
        max_input_tokens: 6144
        max_output_tokens: 1024
        input_cost_per_1k: 0.000125
        output_cost_per_1k: 0.000125
      'code-bison-32k@002':
        mode: completion
        max_input_tokens: 6144
        max_output_tokens: 1024
        input_cost_per_1k: 0.000125
        output_cost_per_1k: 0.000125
      'code-bison32k':
        mode: completion
        max_input_tokens: 6144
        max_output_tokens: 1024
        input_cost_per_1k: 0.000125
        output_cost_per_1k: 0.000125
      'code-bison@001':
        mode: completion
        max_input_tokens: 6144
        max_output_tokens: 1024
        input_cost_per_1k: 0.000125
        output_cost_per_1k: 0.000125
      'code-bison@002':
        mode: completion
        max_input_tokens: 6144
        max_output_tokens: 1024
        input_cost_per_1k: 0.000125
        output_cost_per_1k: 0.000125
      'code-gecko':
        mode: completion
        max_input_tokens: 2048
        max_output_tokens: 64
        input_cost_per_1k: 0.000125
        output_cost_per_1k: 0.000125
      'code-gecko-latest':
        mode: completion
        max_input_tokens: 2048
        max_output_tokens: 64
        input_cost_per_1k: 0.000125
        output_cost_per_1k: 0.000125
      'code-gecko@001':
        mode: completion
        max_input_tokens: 2048
        max_output_tokens: 64
        input_cost_per_1k: 0.000125
        output_cost_per_1k: 0.000125
      'code-gecko@002':
        mode: completion
        max_input_tokens: 2048
        max_output_tokens: 64
        input_cost_per_1k: 0.000125
        output_cost_per_1k: 0.000125
  vertex_ai-deepseek_models:
    model_count: 3
    models:
      'vertex_ai/deepseek-ai/deepseek-r1-0528-maas':
        mode: chat
        max_input_tokens: 65336
        max_output_tokens: 8192
        input_cost_per_1k: 0.00135
        output_cost_per_1k: 0.0054
        capabilities: [function_calling, prompt_caching, reasoning]
      'vertex_ai/deepseek-ai/deepseek-v3.1-maas':
        mode: chat
        max_input_tokens: 163840
        max_output_tokens: 32768
        input_cost_per_1k: 0.00135
        output_cost_per_1k: 0.0054
        capabilities: [function_calling, prompt_caching, reasoning]
      'vertex_ai/deepseek-ai/deepseek-v3.2-maas':
        mode: chat
        max_input_tokens: 163840
        max_output_tokens: 32768
        input_cost_per_1k: 0.00056
        output_cost_per_1k: 0.00168
        capabilities: [function_calling, prompt_caching, reasoning]
  vertex_ai-embedding-models:
    model_count: 14
    models:
      'gemini-embedding-001':
        mode: embedding
        max_input_tokens: 2048
        input_cost_per_1k: 0.00015
      'multimodalembedding':
        mode: embedding
        max_input_tokens: 2048
        input_cost_per_1k: 0.0008
      'multimodalembedding@001':
        mode: embedding
        max_input_tokens: 2048
        input_cost_per_1k: 0.0008
      'text-embedding-004':
        mode: embedding
        max_input_tokens: 2048
        input_cost_per_1k: 0.0001
      'text-embedding-005':
        mode: embedding
        max_input_tokens: 2048
        input_cost_per_1k: 0.0001
      'text-embedding-large-exp-03-07':
        mode: embedding
        max_input_tokens: 8192
        input_cost_per_1k: 0.0001
      'text-embedding-preview-0409':
        mode: embedding
        max_input_tokens: 3072
        input_cost_per_1k: 6.25e-06
      'text-multilingual-embedding-002':
        mode: embedding
        max_input_tokens: 2048
        input_cost_per_1k: 0.0001
      'text-multilingual-embedding-preview-0409':
        mode: embedding
        max_input_tokens: 3072
        input_cost_per_1k: 6.25e-06
      'textembedding-gecko':
        mode: embedding
        max_input_tokens: 3072
        input_cost_per_1k: 0.0001
      'textembedding-gecko-multilingual':
        mode: embedding
        max_input_tokens: 3072
        input_cost_per_1k: 0.0001
      'textembedding-gecko-multilingual@001':
        mode: embedding
        max_input_tokens: 3072
        input_cost_per_1k: 0.0001
      'textembedding-gecko@001':
        mode: embedding
        max_input_tokens: 3072
        input_cost_per_1k: 0.0001
      'textembedding-gecko@003':
        mode: embedding
        max_input_tokens: 3072
        input_cost_per_1k: 0.0001
  vertex_ai-image-models:
    model_count: 8
    models:
      'vertex_ai/imagegeneration@006':
        mode: image
      'vertex_ai/imagen-3.0-capability-001':
        mode: image
      'vertex_ai/imagen-3.0-fast-generate-001':
        mode: image
      'vertex_ai/imagen-3.0-generate-001':
        mode: image
      'vertex_ai/imagen-3.0-generate-002':
        mode: image
      'vertex_ai/imagen-4.0-fast-generate-001':
        mode: image
      'vertex_ai/imagen-4.0-generate-001':
        mode: image
      'vertex_ai/imagen-4.0-ultra-generate-001':
        mode: image
  vertex_ai-llama_models:
    model_count: 11
    models:
      'vertex_ai/meta/llama-3.1-405b-instruct-maas':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 2048
        input_cost_per_1k: 0.005
        output_cost_per_1k: 0.016
        capabilities: [vision, system_messages]
      'vertex_ai/meta/llama-3.1-70b-instruct-maas':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 2048
        capabilities: [vision, system_messages]
      'vertex_ai/meta/llama-3.1-8b-instruct-maas':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 2048
        capabilities: [vision, system_messages]
      'vertex_ai/meta/llama-3.2-90b-vision-instruct-maas':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 2048
        capabilities: [vision, system_messages]
      'vertex_ai/meta/llama-4-maverick-17b-128e-instruct-maas':
        mode: chat
        max_input_tokens: 1000000
        max_output_tokens: 1000000
        input_cost_per_1k: 0.00035
        output_cost_per_1k: 0.00115
        capabilities: [function_calling]
      'vertex_ai/meta/llama-4-maverick-17b-16e-instruct-maas':
        mode: chat
        max_input_tokens: 1000000
        max_output_tokens: 1000000
        input_cost_per_1k: 0.00035
        output_cost_per_1k: 0.00115
        capabilities: [function_calling]
      'vertex_ai/meta/llama-4-scout-17b-128e-instruct-maas':
        mode: chat
        max_input_tokens: 10000000
        max_output_tokens: 10000000
        input_cost_per_1k: 0.00025
        output_cost_per_1k: 0.0007
        capabilities: [function_calling]
      'vertex_ai/meta/llama-4-scout-17b-16e-instruct-maas':
        mode: chat
        max_input_tokens: 10000000
        max_output_tokens: 10000000
        input_cost_per_1k: 0.00025
        output_cost_per_1k: 0.0007
        capabilities: [function_calling]
      'vertex_ai/meta/llama3-405b-instruct-maas':
        mode: chat
        max_input_tokens: 32000
        max_output_tokens: 32000
      'vertex_ai/meta/llama3-70b-instruct-maas':
        mode: chat
        max_input_tokens: 32000
        max_output_tokens: 32000
      'vertex_ai/meta/llama3-8b-instruct-maas':
        mode: chat
        max_input_tokens: 32000
        max_output_tokens: 32000
  vertex_ai-minimax_models:
    model_count: 1
    models:
      'vertex_ai/minimaxai/minimax-m2-maas':
        mode: chat
        max_input_tokens: 196608
        max_output_tokens: 196608
        input_cost_per_1k: 0.0003
        output_cost_per_1k: 0.0012
        capabilities: [function_calling]
  vertex_ai-mistral_models:
    model_count: 19
    models:
      'vertex_ai/codestral-2':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 128000
        input_cost_per_1k: 0.0003
        output_cost_per_1k: 0.0009
        capabilities: [function_calling]
      'vertex_ai/codestral-2501':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 128000
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0006
        capabilities: [function_calling]
      'vertex_ai/codestral-2@001':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 128000
        input_cost_per_1k: 0.0003
        output_cost_per_1k: 0.0009
        capabilities: [function_calling]
      'vertex_ai/codestral@2405':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 128000
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0006
        capabilities: [function_calling]
      'vertex_ai/codestral@latest':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 128000
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0006
        capabilities: [function_calling]
      'vertex_ai/mistral-large-2411':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 8191
        input_cost_per_1k: 0.002
        output_cost_per_1k: 0.006
        capabilities: [function_calling]
      'vertex_ai/mistral-large@2407':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 8191
        input_cost_per_1k: 0.002
        output_cost_per_1k: 0.006
        capabilities: [function_calling]
      'vertex_ai/mistral-large@2411-001':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 8191
        input_cost_per_1k: 0.002
        output_cost_per_1k: 0.006
        capabilities: [function_calling]
      'vertex_ai/mistral-large@latest':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 8191
        input_cost_per_1k: 0.002
        output_cost_per_1k: 0.006
        capabilities: [function_calling]
      'vertex_ai/mistral-medium-3':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 8191
        input_cost_per_1k: 0.0004
        output_cost_per_1k: 0.002
        capabilities: [function_calling]
      'vertex_ai/mistral-medium-3@001':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 8191
        input_cost_per_1k: 0.0004
        output_cost_per_1k: 0.002
        capabilities: [function_calling]
      'vertex_ai/mistral-nemo@2407':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 128000
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.003
        capabilities: [function_calling]
      'vertex_ai/mistral-nemo@latest':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 128000
        input_cost_per_1k: 0.00015
        output_cost_per_1k: 0.00015
        capabilities: [function_calling]
      'vertex_ai/mistral-small-2503':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 128000
        input_cost_per_1k: 0.001
        output_cost_per_1k: 0.003
        capabilities: [vision, function_calling]
      'vertex_ai/mistral-small-2503@001':
        mode: chat
        max_input_tokens: 32000
        max_output_tokens: 8191
        input_cost_per_1k: 0.001
        output_cost_per_1k: 0.003
        capabilities: [function_calling]
      'vertex_ai/mistralai/codestral-2':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 128000
        input_cost_per_1k: 0.0003
        output_cost_per_1k: 0.0009
        capabilities: [function_calling]
      'vertex_ai/mistralai/codestral-2@001':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 128000
        input_cost_per_1k: 0.0003
        output_cost_per_1k: 0.0009
        capabilities: [function_calling]
      'vertex_ai/mistralai/mistral-medium-3':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 8191
        input_cost_per_1k: 0.0004
        output_cost_per_1k: 0.002
        capabilities: [function_calling]
      'vertex_ai/mistralai/mistral-medium-3@001':
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 8191
        input_cost_per_1k: 0.0004
        output_cost_per_1k: 0.002
        capabilities: [function_calling]
  vertex_ai-moonshot_models:
    model_count: 1
    models:
      'vertex_ai/moonshotai/kimi-k2-thinking-maas':
        mode: chat
        max_input_tokens: 256000
        max_output_tokens: 256000
        input_cost_per_1k: 0.0006
        output_cost_per_1k: 0.0025
        capabilities: [function_calling, web_search]
  vertex_ai-openai_models:
    model_count: 2
    models:
      'vertex_ai/openai/gpt-oss-120b-maas':
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 32768
        input_cost_per_1k: 0.00015
        output_cost_per_1k: 0.0006
        capabilities: [reasoning]
      'vertex_ai/openai/gpt-oss-20b-maas':
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 32768
        input_cost_per_1k: 7.5e-05
        output_cost_per_1k: 0.0003
        capabilities: [reasoning]
  vertex_ai-qwen_models:
    model_count: 4
    models:
      'vertex_ai/qwen/qwen3-235b-a22b-instruct-2507-maas':
        mode: chat
        max_input_tokens: 262144
        max_output_tokens: 16384
        input_cost_per_1k: 0.00025
        output_cost_per_1k: 0.001
        capabilities: [function_calling]
      'vertex_ai/qwen/qwen3-coder-480b-a35b-instruct-maas':
        mode: chat
        max_input_tokens: 262144
        max_output_tokens: 32768
        input_cost_per_1k: 0.001
        output_cost_per_1k: 0.004
        capabilities: [function_calling]
      'vertex_ai/qwen/qwen3-next-80b-a3b-instruct-maas':
        mode: chat
        max_input_tokens: 262144
        max_output_tokens: 262144
        input_cost_per_1k: 0.00015
        output_cost_per_1k: 0.0012
        capabilities: [function_calling]
      'vertex_ai/qwen/qwen3-next-80b-a3b-thinking-maas':
        mode: chat
        max_input_tokens: 262144
        max_output_tokens: 262144
        input_cost_per_1k: 0.00015
        output_cost_per_1k: 0.0012
        capabilities: [function_calling]
  vertex_ai-text-models:
    model_count: 7
    models:
      'text-bison':
        mode: completion
        max_input_tokens: 8192
        max_output_tokens: 2048
      'text-bison32k':
        mode: completion
        max_input_tokens: 8192
        max_output_tokens: 1024
        input_cost_per_1k: 0.000125
        output_cost_per_1k: 0.000125
      'text-bison32k@002':
        mode: completion
        max_input_tokens: 8192
        max_output_tokens: 1024
        input_cost_per_1k: 0.000125
        output_cost_per_1k: 0.000125
      'text-bison@001':
        mode: completion
        max_input_tokens: 8192
        max_output_tokens: 1024
      'text-bison@002':
        mode: completion
        max_input_tokens: 8192
        max_output_tokens: 1024
      'text-unicorn':
        mode: completion
        max_input_tokens: 8192
        max_output_tokens: 1024
        input_cost_per_1k: 0.01
        output_cost_per_1k: 0.028
      'text-unicorn@001':
        mode: completion
        max_input_tokens: 8192
        max_output_tokens: 1024
        input_cost_per_1k: 0.01
        output_cost_per_1k: 0.028
  vertex_ai-video-models:
    model_count: 9
    models:
      'vertex_ai/veo-2.0-generate-001':
        mode: video_generation
        max_input_tokens: 1024
      'vertex_ai/veo-3.0-fast-generate-001':
        mode: video_generation
        max_input_tokens: 1024
      'vertex_ai/veo-3.0-fast-generate-preview':
        mode: video_generation
        max_input_tokens: 1024
      'vertex_ai/veo-3.0-generate-001':
        mode: video_generation
        max_input_tokens: 1024
      'vertex_ai/veo-3.0-generate-preview':
        mode: video_generation
        max_input_tokens: 1024
      'vertex_ai/veo-3.1-fast-generate-001':
        mode: video_generation
        max_input_tokens: 1024
      'vertex_ai/veo-3.1-fast-generate-preview':
        mode: video_generation
        max_input_tokens: 1024
      'vertex_ai/veo-3.1-generate-001':
        mode: video_generation
        max_input_tokens: 1024
      'vertex_ai/veo-3.1-generate-preview':
        mode: video_generation
        max_input_tokens: 1024
  vertex_ai-vision-models:
    model_count: 3
    models:
      'gemini-1.0-pro-vision':
        mode: chat
        max_input_tokens: 16384
        max_output_tokens: 2048
        input_cost_per_1k: 0.0005
        output_cost_per_1k: 0.0015
        capabilities: [vision, function_calling, parallel_function_calling]
      'gemini-1.0-pro-vision-001':
        mode: chat
        max_input_tokens: 16384
        max_output_tokens: 2048
        input_cost_per_1k: 0.0005
        output_cost_per_1k: 0.0015
        capabilities: [vision, function_calling, parallel_function_calling]
        deprecated: true
        deprecation_date: '2025-04-09'
      'gemini-pro-vision':
        mode: chat
        max_input_tokens: 16384
        max_output_tokens: 2048
        input_cost_per_1k: 0.0005
        output_cost_per_1k: 0.0015
        capabilities: [vision, function_calling, parallel_function_calling]
  volcengine:
    model_count: 5
    models:
      'doubao-embedding':
        mode: embedding
        max_input_tokens: 4096
      'doubao-embedding-large':
        mode: embedding
        max_input_tokens: 4096
      'doubao-embedding-large-text-240915':
        mode: embedding
        max_input_tokens: 4096
      'doubao-embedding-large-text-250515':
        mode: embedding
        max_input_tokens: 4096
      'doubao-embedding-text-240715':
        mode: embedding
        max_input_tokens: 4096
  voyage:
    model_count: 19
    models:
      'rerank-2':
        litellm_id: 'voyage/rerank-2'
        mode: rerank
        max_input_tokens: 16000
        max_output_tokens: 16000
        input_cost_per_1k: 5e-05
      'rerank-2-lite':
        litellm_id: 'voyage/rerank-2-lite'
        mode: rerank
        max_input_tokens: 8000
        max_output_tokens: 8000
        input_cost_per_1k: 2e-05
      'rerank-2.5':
        litellm_id: 'voyage/rerank-2.5'
        mode: rerank
        max_input_tokens: 32000
        max_output_tokens: 32000
        input_cost_per_1k: 5e-05
      'rerank-2.5-lite':
        litellm_id: 'voyage/rerank-2.5-lite'
        mode: rerank
        max_input_tokens: 32000
        max_output_tokens: 32000
        input_cost_per_1k: 2e-05
      'voyage-2':
        litellm_id: 'voyage/voyage-2'
        mode: embedding
        max_input_tokens: 4000
        input_cost_per_1k: 0.0001
      'voyage-3':
        litellm_id: 'voyage/voyage-3'
        mode: embedding
        max_input_tokens: 32000
        input_cost_per_1k: 6e-05
      'voyage-3-large':
        litellm_id: 'voyage/voyage-3-large'
        mode: embedding
        max_input_tokens: 32000
        input_cost_per_1k: 0.00018
      'voyage-3-lite':
        litellm_id: 'voyage/voyage-3-lite'
        mode: embedding
        max_input_tokens: 32000
        input_cost_per_1k: 2e-05
      'voyage-3.5':
        litellm_id: 'voyage/voyage-3.5'
        mode: embedding
        max_input_tokens: 32000
        input_cost_per_1k: 6e-05
      'voyage-3.5-lite':
        litellm_id: 'voyage/voyage-3.5-lite'
        mode: embedding
        max_input_tokens: 32000
        input_cost_per_1k: 2e-05
      'voyage-code-2':
        litellm_id: 'voyage/voyage-code-2'
        mode: embedding
        max_input_tokens: 16000
        input_cost_per_1k: 0.00012
      'voyage-code-3':
        litellm_id: 'voyage/voyage-code-3'
        mode: embedding
        max_input_tokens: 32000
        input_cost_per_1k: 0.00018
      'voyage-context-3':
        litellm_id: 'voyage/voyage-context-3'
        mode: embedding
        max_input_tokens: 120000
        input_cost_per_1k: 0.00018
      'voyage-finance-2':
        litellm_id: 'voyage/voyage-finance-2'
        mode: embedding
        max_input_tokens: 32000
        input_cost_per_1k: 0.00012
      'voyage-large-2':
        litellm_id: 'voyage/voyage-large-2'
        mode: embedding
        max_input_tokens: 16000
        input_cost_per_1k: 0.00012
      'voyage-law-2':
        litellm_id: 'voyage/voyage-law-2'
        mode: embedding
        max_input_tokens: 16000
        input_cost_per_1k: 0.00012
      'voyage-lite-01':
        litellm_id: 'voyage/voyage-lite-01'
        mode: embedding
        max_input_tokens: 4096
        input_cost_per_1k: 0.0001
      'voyage-lite-02-instruct':
        litellm_id: 'voyage/voyage-lite-02-instruct'
        mode: embedding
        max_input_tokens: 4000
        input_cost_per_1k: 0.0001
      'voyage-multimodal-3':
        litellm_id: 'voyage/voyage-multimodal-3'
        mode: embedding
        max_input_tokens: 32000
        input_cost_per_1k: 0.00012
  wandb:
    model_count: 14
    models:
      'Qwen/Qwen3-235B-A22B-Instruct-2507':
        litellm_id: 'wandb/Qwen/Qwen3-235B-A22B-Instruct-2507'
        mode: chat
        max_input_tokens: 262144
        max_output_tokens: 262144
        input_cost_per_1k: 10.0
        output_cost_per_1k: 10.0
      'Qwen/Qwen3-235B-A22B-Thinking-2507':
        litellm_id: 'wandb/Qwen/Qwen3-235B-A22B-Thinking-2507'
        mode: chat
        max_input_tokens: 262144
        max_output_tokens: 262144
        input_cost_per_1k: 10.0
        output_cost_per_1k: 10.0
      'Qwen/Qwen3-Coder-480B-A35B-Instruct':
        litellm_id: 'wandb/Qwen/Qwen3-Coder-480B-A35B-Instruct'
        mode: chat
        max_input_tokens: 262144
        max_output_tokens: 262144
        input_cost_per_1k: 100.0
        output_cost_per_1k: 150.0
      'deepseek-ai/DeepSeek-R1-0528':
        litellm_id: 'wandb/deepseek-ai/DeepSeek-R1-0528'
        mode: chat
        max_input_tokens: 161000
        max_output_tokens: 161000
        input_cost_per_1k: 135.0
        output_cost_per_1k: 540.0
      'deepseek-ai/DeepSeek-V3-0324':
        litellm_id: 'wandb/deepseek-ai/DeepSeek-V3-0324'
        mode: chat
        max_input_tokens: 161000
        max_output_tokens: 161000
        input_cost_per_1k: 114.0
        output_cost_per_1k: 275.0
      'deepseek-ai/DeepSeek-V3.1':
        litellm_id: 'wandb/deepseek-ai/DeepSeek-V3.1'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 128000
        input_cost_per_1k: 55.0
        output_cost_per_1k: 165.0
      'meta-llama/Llama-3.1-8B-Instruct':
        litellm_id: 'wandb/meta-llama/Llama-3.1-8B-Instruct'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 128000
        input_cost_per_1k: 22.0
        output_cost_per_1k: 22.0
      'meta-llama/Llama-3.3-70B-Instruct':
        litellm_id: 'wandb/meta-llama/Llama-3.3-70B-Instruct'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 128000
        input_cost_per_1k: 71.0
        output_cost_per_1k: 71.0
      'meta-llama/Llama-4-Scout-17B-16E-Instruct':
        litellm_id: 'wandb/meta-llama/Llama-4-Scout-17B-16E-Instruct'
        mode: chat
        max_input_tokens: 64000
        max_output_tokens: 64000
        input_cost_per_1k: 17.0
        output_cost_per_1k: 66.0
      'microsoft/Phi-4-mini-instruct':
        litellm_id: 'wandb/microsoft/Phi-4-mini-instruct'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 128000
        input_cost_per_1k: 8.0
        output_cost_per_1k: 35.0
      'moonshotai/Kimi-K2-Instruct':
        litellm_id: 'wandb/moonshotai/Kimi-K2-Instruct'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 128000
        input_cost_per_1k: 0.0006
        output_cost_per_1k: 0.0025
      'openai/gpt-oss-120b':
        litellm_id: 'wandb/openai/gpt-oss-120b'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 15.0
        output_cost_per_1k: 60.0
      'openai/gpt-oss-20b':
        litellm_id: 'wandb/openai/gpt-oss-20b'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 5.0
        output_cost_per_1k: 20.0
      'zai-org/GLM-4.5':
        litellm_id: 'wandb/zai-org/GLM-4.5'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 55.0
        output_cost_per_1k: 200.0
  watsonx:
    model_count: 29
    models:
      'bigscience/mt0-xxl-13b':
        litellm_id: 'watsonx/bigscience/mt0-xxl-13b'
        mode: chat
        max_input_tokens: 8192
        max_output_tokens: 8192
        input_cost_per_1k: 0.5
        output_cost_per_1k: 2.0
      'core42/jais-13b-chat':
        litellm_id: 'watsonx/core42/jais-13b-chat'
        mode: chat
        max_input_tokens: 8192
        max_output_tokens: 8192
        input_cost_per_1k: 0.5
        output_cost_per_1k: 2.0
      'google/flan-t5-xl-3b':
        litellm_id: 'watsonx/google/flan-t5-xl-3b'
        mode: chat
        max_input_tokens: 8192
        max_output_tokens: 8192
        input_cost_per_1k: 0.0006
        output_cost_per_1k: 0.0006
      'ibm/granite-13b-chat-v2':
        litellm_id: 'watsonx/ibm/granite-13b-chat-v2'
        mode: chat
        max_input_tokens: 8192
        max_output_tokens: 8192
        input_cost_per_1k: 0.0006
        output_cost_per_1k: 0.0006
      'ibm/granite-13b-instruct-v2':
        litellm_id: 'watsonx/ibm/granite-13b-instruct-v2'
        mode: chat
        max_input_tokens: 8192
        max_output_tokens: 8192
        input_cost_per_1k: 0.0006
        output_cost_per_1k: 0.0006
      'ibm/granite-3-3-8b-instruct':
        litellm_id: 'watsonx/ibm/granite-3-3-8b-instruct'
        mode: chat
        max_input_tokens: 8192
        max_output_tokens: 8192
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
        capabilities: [function_calling, parallel_function_calling]
      'ibm/granite-3-8b-instruct':
        litellm_id: 'watsonx/ibm/granite-3-8b-instruct'
        mode: chat
        max_input_tokens: 8192
        max_output_tokens: 1024
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
        capabilities: [function_calling, system_messages, json_mode, prompt_caching]
      'ibm/granite-4-h-small':
        litellm_id: 'watsonx/ibm/granite-4-h-small'
        mode: chat
        max_input_tokens: 20480
        max_output_tokens: 20480
        input_cost_per_1k: 6e-05
        output_cost_per_1k: 0.00025
        capabilities: [function_calling, parallel_function_calling]
      'ibm/granite-guardian-3-2-2b':
        litellm_id: 'watsonx/ibm/granite-guardian-3-2-2b'
        mode: chat
        max_input_tokens: 8192
        max_output_tokens: 8192
        input_cost_per_1k: 0.0001
        output_cost_per_1k: 0.0001
      'ibm/granite-guardian-3-3-8b':
        litellm_id: 'watsonx/ibm/granite-guardian-3-3-8b'
        mode: chat
        max_input_tokens: 8192
        max_output_tokens: 8192
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0002
      'ibm/granite-ttm-1024-96-r2':
        litellm_id: 'watsonx/ibm/granite-ttm-1024-96-r2'
        mode: chat
        max_input_tokens: 512
        max_output_tokens: 512
        input_cost_per_1k: 0.00038
        output_cost_per_1k: 0.00038
      'ibm/granite-ttm-1536-96-r2':
        litellm_id: 'watsonx/ibm/granite-ttm-1536-96-r2'
        mode: chat
        max_input_tokens: 512
        max_output_tokens: 512
        input_cost_per_1k: 0.00038
        output_cost_per_1k: 0.00038
      'ibm/granite-ttm-512-96-r2':
        litellm_id: 'watsonx/ibm/granite-ttm-512-96-r2'
        mode: chat
        max_input_tokens: 512
        max_output_tokens: 512
        input_cost_per_1k: 0.00038
        output_cost_per_1k: 0.00038
      'ibm/granite-vision-3-2-2b':
        litellm_id: 'watsonx/ibm/granite-vision-3-2-2b'
        mode: chat
        max_input_tokens: 8192
        max_output_tokens: 8192
        input_cost_per_1k: 0.0001
        output_cost_per_1k: 0.0001
        capabilities: [vision]
      'meta-llama/llama-3-2-11b-vision-instruct':
        litellm_id: 'watsonx/meta-llama/llama-3-2-11b-vision-instruct'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 128000
        input_cost_per_1k: 0.00035
        output_cost_per_1k: 0.00035
        capabilities: [vision, function_calling, parallel_function_calling]
      'meta-llama/llama-3-2-1b-instruct':
        litellm_id: 'watsonx/meta-llama/llama-3-2-1b-instruct'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 128000
        input_cost_per_1k: 0.0001
        output_cost_per_1k: 0.0001
        capabilities: [function_calling, parallel_function_calling]
      'meta-llama/llama-3-2-3b-instruct':
        litellm_id: 'watsonx/meta-llama/llama-3-2-3b-instruct'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 128000
        input_cost_per_1k: 0.00015
        output_cost_per_1k: 0.00015
        capabilities: [function_calling, parallel_function_calling]
      'meta-llama/llama-3-2-90b-vision-instruct':
        litellm_id: 'watsonx/meta-llama/llama-3-2-90b-vision-instruct'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 128000
        input_cost_per_1k: 0.002
        output_cost_per_1k: 0.002
        capabilities: [vision, function_calling, parallel_function_calling]
      'meta-llama/llama-3-3-70b-instruct':
        litellm_id: 'watsonx/meta-llama/llama-3-3-70b-instruct'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 128000
        input_cost_per_1k: 0.00071
        output_cost_per_1k: 0.00071
        capabilities: [function_calling, parallel_function_calling]
      'meta-llama/llama-4-maverick-17b':
        litellm_id: 'watsonx/meta-llama/llama-4-maverick-17b'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 128000
        input_cost_per_1k: 0.00035
        output_cost_per_1k: 0.0014
        capabilities: [function_calling, parallel_function_calling]
      'meta-llama/llama-guard-3-11b-vision':
        litellm_id: 'watsonx/meta-llama/llama-guard-3-11b-vision'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 128000
        input_cost_per_1k: 0.00035
        output_cost_per_1k: 0.00035
        capabilities: [vision]
      'mistralai/mistral-large':
        litellm_id: 'watsonx/mistralai/mistral-large'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 16384
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.01
        capabilities: [function_calling, system_messages, json_mode, prompt_caching]
      'mistralai/mistral-medium-2505':
        litellm_id: 'watsonx/mistralai/mistral-medium-2505'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 128000
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.01
        capabilities: [function_calling, parallel_function_calling]
      'mistralai/mistral-small-2503':
        litellm_id: 'watsonx/mistralai/mistral-small-2503'
        mode: chat
        max_input_tokens: 32000
        max_output_tokens: 32000
        input_cost_per_1k: 0.0001
        output_cost_per_1k: 0.0003
        capabilities: [function_calling, parallel_function_calling]
      'mistralai/mistral-small-3-1-24b-instruct-2503':
        litellm_id: 'watsonx/mistralai/mistral-small-3-1-24b-instruct-2503'
        mode: chat
        max_input_tokens: 32000
        max_output_tokens: 32000
        input_cost_per_1k: 0.0001
        output_cost_per_1k: 0.0003
        capabilities: [function_calling, parallel_function_calling]
      'mistralai/pixtral-12b-2409':
        litellm_id: 'watsonx/mistralai/pixtral-12b-2409'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 128000
        input_cost_per_1k: 0.00035
        output_cost_per_1k: 0.00035
        capabilities: [vision]
      'openai/gpt-oss-120b':
        litellm_id: 'watsonx/openai/gpt-oss-120b'
        mode: chat
        max_input_tokens: 8192
        max_output_tokens: 8192
        input_cost_per_1k: 0.00015
        output_cost_per_1k: 0.0006
      'sdaia/allam-1-13b-instruct':
        litellm_id: 'watsonx/sdaia/allam-1-13b-instruct'
        mode: chat
        max_input_tokens: 8192
        max_output_tokens: 8192
        input_cost_per_1k: 0.0018
        output_cost_per_1k: 0.0018
      'whisper-large-v3-turbo':
        litellm_id: 'watsonx/whisper-large-v3-turbo'
        mode: audio_transcription
  xai:
    model_count: 32
    models:
      'grok-2':
        litellm_id: 'xai/grok-2'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.002
        output_cost_per_1k: 0.01
        capabilities: [function_calling, web_search]
      'grok-2-1212':
        litellm_id: 'xai/grok-2-1212'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.002
        output_cost_per_1k: 0.01
        capabilities: [function_calling, web_search]
      'grok-2-latest':
        litellm_id: 'xai/grok-2-latest'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.002
        output_cost_per_1k: 0.01
        capabilities: [function_calling, web_search]
      'grok-2-vision':
        litellm_id: 'xai/grok-2-vision'
        mode: chat
        max_input_tokens: 32768
        max_output_tokens: 32768
        input_cost_per_1k: 0.002
        output_cost_per_1k: 0.01
        capabilities: [vision, function_calling, web_search]
      'grok-2-vision-1212':
        litellm_id: 'xai/grok-2-vision-1212'
        mode: chat
        max_input_tokens: 32768
        max_output_tokens: 32768
        input_cost_per_1k: 0.002
        output_cost_per_1k: 0.01
        capabilities: [vision, function_calling, web_search]
      'grok-2-vision-latest':
        litellm_id: 'xai/grok-2-vision-latest'
        mode: chat
        max_input_tokens: 32768
        max_output_tokens: 32768
        input_cost_per_1k: 0.002
        output_cost_per_1k: 0.01
        capabilities: [vision, function_calling, web_search]
      'grok-3':
        litellm_id: 'xai/grok-3'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.015
        capabilities: [function_calling, web_search]
      'grok-3-beta':
        litellm_id: 'xai/grok-3-beta'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.015
        capabilities: [function_calling, web_search]
      'grok-3-fast-beta':
        litellm_id: 'xai/grok-3-fast-beta'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.005
        output_cost_per_1k: 0.025
        capabilities: [function_calling, web_search]
      'grok-3-fast-latest':
        litellm_id: 'xai/grok-3-fast-latest'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.005
        output_cost_per_1k: 0.025
        capabilities: [function_calling, web_search]
      'grok-3-latest':
        litellm_id: 'xai/grok-3-latest'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.015
        capabilities: [function_calling, web_search]
      'grok-3-mini':
        litellm_id: 'xai/grok-3-mini'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.0003
        output_cost_per_1k: 0.0005
        capabilities: [function_calling, reasoning, web_search]
      'grok-3-mini-beta':
        litellm_id: 'xai/grok-3-mini-beta'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.0003
        output_cost_per_1k: 0.0005
        capabilities: [function_calling, reasoning, web_search]
      'grok-3-mini-fast':
        litellm_id: 'xai/grok-3-mini-fast'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.0006
        output_cost_per_1k: 0.004
        capabilities: [function_calling, reasoning, web_search]
      'grok-3-mini-fast-beta':
        litellm_id: 'xai/grok-3-mini-fast-beta'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.0006
        output_cost_per_1k: 0.004
        capabilities: [function_calling, reasoning, web_search]
      'grok-3-mini-fast-latest':
        litellm_id: 'xai/grok-3-mini-fast-latest'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.0006
        output_cost_per_1k: 0.004
        capabilities: [function_calling, reasoning, web_search]
      'grok-3-mini-latest':
        litellm_id: 'xai/grok-3-mini-latest'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.0003
        output_cost_per_1k: 0.0005
        capabilities: [function_calling, reasoning, web_search]
      'grok-4':
        litellm_id: 'xai/grok-4'
        mode: chat
        max_input_tokens: 256000
        max_output_tokens: 256000
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.015
        capabilities: [function_calling, web_search]
      'grok-4-0709':
        litellm_id: 'xai/grok-4-0709'
        mode: chat
        max_input_tokens: 256000
        max_output_tokens: 256000
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.015
        capabilities: [function_calling, web_search]
      'grok-4-1-fast':
        litellm_id: 'xai/grok-4-1-fast'
        mode: chat
        max_input_tokens: 2000000.0
        max_output_tokens: 2000000.0
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0005
        capabilities: [vision, function_calling, json_mode, reasoning, web_search, audio_input]
      'grok-4-1-fast-non-reasoning':
        litellm_id: 'xai/grok-4-1-fast-non-reasoning'
        mode: chat
        max_input_tokens: 2000000.0
        max_output_tokens: 2000000.0
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0005
        capabilities: [vision, function_calling, json_mode, web_search, audio_input]
      'grok-4-1-fast-non-reasoning-latest':
        litellm_id: 'xai/grok-4-1-fast-non-reasoning-latest'
        mode: chat
        max_input_tokens: 2000000.0
        max_output_tokens: 2000000.0
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0005
        capabilities: [vision, function_calling, json_mode, web_search, audio_input]
      'grok-4-1-fast-reasoning':
        litellm_id: 'xai/grok-4-1-fast-reasoning'
        mode: chat
        max_input_tokens: 2000000.0
        max_output_tokens: 2000000.0
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0005
        capabilities: [vision, function_calling, json_mode, reasoning, web_search, audio_input]
      'grok-4-1-fast-reasoning-latest':
        litellm_id: 'xai/grok-4-1-fast-reasoning-latest'
        mode: chat
        max_input_tokens: 2000000.0
        max_output_tokens: 2000000.0
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0005
        capabilities: [vision, function_calling, json_mode, reasoning, web_search, audio_input]
      'grok-4-fast-non-reasoning':
        litellm_id: 'xai/grok-4-fast-non-reasoning'
        mode: chat
        max_input_tokens: 2000000.0
        max_output_tokens: 2000000.0
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0005
        capabilities: [function_calling, web_search]
      'grok-4-fast-reasoning':
        litellm_id: 'xai/grok-4-fast-reasoning'
        mode: chat
        max_input_tokens: 2000000.0
        max_output_tokens: 2000000.0
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0005
        capabilities: [function_calling, web_search]
      'grok-4-latest':
        litellm_id: 'xai/grok-4-latest'
        mode: chat
        max_input_tokens: 256000
        max_output_tokens: 256000
        input_cost_per_1k: 0.003
        output_cost_per_1k: 0.015
        capabilities: [function_calling, web_search]
      'grok-beta':
        litellm_id: 'xai/grok-beta'
        mode: chat
        max_input_tokens: 131072
        max_output_tokens: 131072
        input_cost_per_1k: 0.005
        output_cost_per_1k: 0.015
        capabilities: [vision, function_calling, web_search]
      'grok-code-fast':
        litellm_id: 'xai/grok-code-fast'
        mode: chat
        max_input_tokens: 256000
        max_output_tokens: 256000
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0015
        capabilities: [function_calling, reasoning]
      'grok-code-fast-1':
        litellm_id: 'xai/grok-code-fast-1'
        mode: chat
        max_input_tokens: 256000
        max_output_tokens: 256000
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0015
        capabilities: [function_calling, reasoning]
      'grok-code-fast-1-0825':
        litellm_id: 'xai/grok-code-fast-1-0825'
        mode: chat
        max_input_tokens: 256000
        max_output_tokens: 256000
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0015
        capabilities: [function_calling, reasoning]
      'grok-vision-beta':
        litellm_id: 'xai/grok-vision-beta'
        mode: chat
        max_input_tokens: 8192
        max_output_tokens: 8192
        input_cost_per_1k: 0.005
        output_cost_per_1k: 0.015
        capabilities: [vision, function_calling, web_search]
  zai:
    model_count: 8
    models:
      'glm-4-32b-0414-128k':
        litellm_id: 'zai/glm-4-32b-0414-128k'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 32000
        input_cost_per_1k: 0.0001
        output_cost_per_1k: 0.0001
        capabilities: [function_calling]
      'glm-4.5':
        litellm_id: 'zai/glm-4.5'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 32000
        input_cost_per_1k: 0.0006
        output_cost_per_1k: 0.0022
        capabilities: [function_calling]
      'glm-4.5-air':
        litellm_id: 'zai/glm-4.5-air'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 32000
        input_cost_per_1k: 0.0002
        output_cost_per_1k: 0.0011
        capabilities: [function_calling]
      'glm-4.5-airx':
        litellm_id: 'zai/glm-4.5-airx'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 32000
        input_cost_per_1k: 0.0011
        output_cost_per_1k: 0.0045
        capabilities: [function_calling]
      'glm-4.5-flash':
        litellm_id: 'zai/glm-4.5-flash'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 32000
        capabilities: [function_calling]
      'glm-4.5-x':
        litellm_id: 'zai/glm-4.5-x'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 32000
        input_cost_per_1k: 0.0022
        output_cost_per_1k: 0.0089
        capabilities: [function_calling]
      'glm-4.5v':
        litellm_id: 'zai/glm-4.5v'
        mode: chat
        max_input_tokens: 128000
        max_output_tokens: 32000
        input_cost_per_1k: 0.0006
        output_cost_per_1k: 0.0018
        capabilities: [vision, function_calling]
      'glm-4.6':
        litellm_id: 'zai/glm-4.6'
        mode: chat
        max_input_tokens: 200000
        max_output_tokens: 128000
        input_cost_per_1k: 0.0006
        output_cost_per_1k: 0.0022
        capabilities: [function_calling]
